{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "> \"NLP\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: 이강철\n",
    "- categories: [python]\n",
    "- hide :false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1`. 차원의 저주(Curse of Dimensionality) : 수학적 공간 차원(=변수 개수)이 늘어나면서, 문제 계산법이 지수적으로 커지는 상황"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 $x=[1,2,3,4,5],\\, y= [0,0,0,0,0]\\to (X,Y)$ 을 표현한다고 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 1차원 상에서 표현되는 정보를 2차원 상에서 표현하게되어 설명 공간이 $5^2 =25$가 된 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 경우를 차원의 저주라고 하며 이는 모델링 과정에서 저장 공간과 처리 시간이 불필요하게 증가됨을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 문제점은 위와 같은 $(X,Y)$이산형 확률분포에서 결합분포를 구할 때 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 이산형 변수들이 다양한 값$(0,1,2\\dots 145748)$을 가질 경우 같은 길이의 두 문자열에서 거리를 측정하는 척도인 **\"해밍 거리\"**의 값이 거의 최댓값에 이르게 된다. [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "ax1, ax2 =axes\n",
    "\n",
    "x = [1,2,3,4,5]\n",
    "y = [0,0,0,0,0]\n",
    "ax1.plot(x,y)\n",
    "ax1.set_title(\"1-dimensional\")\n",
    "ax1.axis(\"off\")\n",
    "ax2.plot(x,y)\n",
    "ax2.set_title(\"2-dimensional\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 문제점을 해결하기 위해 NLP 분야에서는 단어를 저차원에 표현하기 위한 **\"워드 임베딩(Word Embedding)\"**을 제안하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 통계적인 방법이 단어의 출현 빈도에 집중 한다면 워드 임베딩은 서로 유사한 단어들 간 유사성을 포착하는데 집중한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 가정 : 유사한 의미를 가진 단어는 유사한 문맥안에서 발견된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 가정의 해석 : 유사한 의미를 가진 단어들은 유사한 단어 벡터를 가진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 이점 : 이웃된 단어들의 단어 벡터들을 학습하여 단어간 유사성을 도출해낼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> example 1 : 'man' + 'royal' = 'king'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이러한 임베딩 기법 중 하나인 **\"Word2Vec\"** 기법을 소개한다. [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 워드 \"**Word2Vec**\" 중 가장 대표적인 방법으로 **\"CBOW\"**, **\"skip-gram\"** 이 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주변 단어를 이용하여 중심단어를 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주어진 문맥에서 window size $k$ 를 적용해 target word 양옆에 $k$개의 단어들을 이용하여 조건부 확률을 계산한다. (편의상 k=1 이라고 설정) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `프라닭` 이라는 단어를 예측한다고 가정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`문장1`: 금요일 밤에 `프라닭`은 못참지\n",
    "\n",
    "`문장2`: 불금인데 교촌치킨 에 맥주?\n",
    "\n",
    "`단어` : [ \"금요일\", \"밤\", \"프라닭\",\"불금\", \"교촌치킨\", \"맥주\" ] $\\to Word \\in R^{6 \\times 6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문장의 개수는 $j=2$, 단어의 개수는 총 $i=6$, 축소할 임베딩 차원의 개수는 $n=3$ 으로 설정하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 차원축소를 위해 생성되는 임베딩(=가중치) 행렬 $W \\in R^{6\\times 3}$ 으로 파이토치 기준 $N(0,1)$에서 생성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 목적 1 : $Word \\in R^{6 \\times 6} \\to W \\in R^{6\\times 3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  목적 2 : 단어간 의미적 유사성을 포착하기 위한 임베딩 행렬 갱신 $W^{t} \\to W^{t+1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1`. one-hot vector $Word \\in R^{6 \\times 6}$ 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>금요일</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>밤</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>프라닭</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>불금</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>교촌치킨</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>맥주</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5\n",
       "금요일   1  0  0  0  0  0\n",
       "밤     0  1  0  0  0  0\n",
       "프라닭   0  0  1  0  0  0\n",
       "불금    0  0  0  1  0  0\n",
       "교촌치킨  0  0  0  0  1  0\n",
       "맥주    0  0  0  0  0  1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "index = [ \"금요일\", \"밤\", \"프라닭\",\"불금\", \"교촌치킨\", \"맥주\" ]\n",
    "\n",
    "word1 = [1,0,0,0,0,0]\n",
    "word2 = [0,1,0,0,0,0]\n",
    "word3 = [0,0,1,0,0,0]\n",
    "word4 = [0,0,0,1,0,0]\n",
    "word5 = [0,0,0,0,1,0]\n",
    "word6 = [0,0,0,0,0,1]\n",
    "\n",
    "one_hot = pd.DataFrame([word1,word2,word3,word4,word5,word6],index=index)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2`. 임베딩(가중치) 행렬 생성 $W \\in R^{6\\times 3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W1</th>\n",
       "      <th>W2</th>\n",
       "      <th>W3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>금요일</th>\n",
       "      <td>-0.946677</td>\n",
       "      <td>-0.964799</td>\n",
       "      <td>-2.236172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>밤</th>\n",
       "      <td>1.481341</td>\n",
       "      <td>0.678401</td>\n",
       "      <td>-1.239748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>프라닭</th>\n",
       "      <td>-0.855941</td>\n",
       "      <td>0.556102</td>\n",
       "      <td>0.330505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>불금</th>\n",
       "      <td>0.316146</td>\n",
       "      <td>-1.791996</td>\n",
       "      <td>-0.307091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>교촌치킨</th>\n",
       "      <td>1.289018</td>\n",
       "      <td>-1.415381</td>\n",
       "      <td>0.418707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>맥주</th>\n",
       "      <td>1.106920</td>\n",
       "      <td>0.051748</td>\n",
       "      <td>0.478479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            W1        W2        W3\n",
       "금요일  -0.946677 -0.964799 -2.236172\n",
       "밤     1.481341  0.678401 -1.239748\n",
       "프라닭  -0.855941  0.556102  0.330505\n",
       "불금    0.316146 -1.791996 -0.307091\n",
       "교촌치킨  1.289018 -1.415381  0.418707\n",
       "맥주    1.106920  0.051748  0.478479"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "W = np.random.normal(loc = 0, scale=1,size=18).reshape(6,3)\n",
    "W = pd.DataFrame(W,index=index, columns = [\"W1\",\"W2\",\"W3\"])\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3`. $\\widehat W_{프라닭} = \\frac {W_{밤} + W_{불금}} {2}   = [0.89,-0.55,0.77]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8987433793584608, -0.5567973759616802, -0.7734197905021276]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "W_1 = list((W.loc[\"밤\"] + W.loc[\"불금\"])/2)\n",
    "W_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4`.  $ Z = \\widehat W_{프라닭} \\times  W^T = $ [ 1.42,  1.91, -1.33,  1.52,  1.62,  0.6 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.42,  1.91, -1.33,  1.52,  1.62,  0.6 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "z = np.dot(np.array(W_1),W.T.to_numpy())\n",
    "z.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5`. $\\hat y$ 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat y=softmax(Z) = [0.18,\\,\\,0.30,\\,\\,0.01,\\,\\,0.20,\\,\\,0.22,\\,\\,0.08]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>금요일</th>\n",
       "      <td>0.182270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>밤</th>\n",
       "      <td>0.299486</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>프라닭</th>\n",
       "      <td>0.011647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>불금</th>\n",
       "      <td>0.202155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>교촌치킨</th>\n",
       "      <td>0.224158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>맥주</th>\n",
       "      <td>0.080284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_hat  y\n",
       "금요일   0.182270  0\n",
       "밤     0.299486  0\n",
       "프라닭   0.011647  1\n",
       "불금    0.202155  0\n",
       "교촌치킨  0.224158  0\n",
       "맥주    0.080284  0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "from scipy.special import softmax\n",
    "y=[0,0,1,0,0,0]\n",
    "so = pd.DataFrame({\"y_hat\": softmax(z), \"y\" : y},index=index)\n",
    "so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  가중치  행렬 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 과정을 모든 단어에 대해 수행하여 크로스 엔트로피 함수를 적용한 $loss$를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1`.   $loss=-\\sum_{i=1}^6 y_i\\log  p_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2`.$loss$를 최소화 하는 최적의 파라미터 $\\theta$를 구함 $\\to\\frac {\\partial loss}{\\partial p} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3`. example $W_{밤},W_{불금}$ 업데이트($\\alpha $ :  learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_{밤}^{t+1} = W_{밤}^t + \\left(\\,\\alpha\\,\\times \\theta\\,\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_{불금}^{t+1} = W_{불금}^t + \\left(\\,\\alpha\\,\\times \\theta\\,\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 중심단어 벡터 $W_c$가 있고, 주변 단어 벡터 $W_o$가 있다고 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $t+1$ 시점에서 $t$ 시점의 결과를 반영하여 단어벡터 $W_{o}$를 갱신한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $W^{t+1}_{o}  =W^{t}_{o} + \\alpha \\times [l(\\theta_{c})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 타겟단어 예측시 사용되는 수식 [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_O|w_I) = \\frac{\\exp\\,({v^{\\prime}_{w_O}}^Tv_{w_{I}})} {\\sum_{w=1}^W \\exp\\,({v^{\\prime}_w}^Tv_{w_{I}}) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I \\,\\,:\\,\\,Input,\\,\\,O\\,\\,:\\,\\, Output$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W : number\\,\\,\\, of\\,\\,\\, Word$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Skip-gram의 경우 CBOW와 달리 중심단어를 가지고 주변단어를 예측하는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 따라서 CBOW의 `3`번째 단계 window-size내의 주변 단어들의 합을 평균 내는 과정이 생략된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이러한 부분을 빼면 CBOW와 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Skip-gram과 CBOW의 경우 아래와 같은 수식을 최대화 하는 것을 목표로 한다. (베르누이분포의 MLE 를 생각해보장) [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 베르누이분포의 MLE와 크로스 엔트로피 손실함수의 최소값 파라미터를 구하는 것이 동치라고 생각해보자.(이 부분 다시 증명)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac 1T \\sum_{t=1}^T \\sum_{-c\\leq j\\leq c, j\\neq 0} \\log p\\left(w_{t+j}|w_t\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $T :$ number of traning word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $c$ : the size of training context (=window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(w_O|w_I) = \\frac{\\exp\\,({v^{\\prime}_{w_O}}^Tv_{w_{I}})} {\\sum_{w=1}^W \\exp\\,({v^{\\prime}_w}^Tv_{w_{I}}) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위와 같이 Word2Vec은 출력층이 내놓는 스코어 값에 소프트맥스 함수를 적용해 확률값으로 변환한 후 이를 정답과 비교해 **\"역전파(backpropagation)\"** 하는 구조이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그런데 소프트맥스를 적용하려면 분모에 해당하는 값, 즉 중심(=input) 단어와 나머지 모든 단어의 내적을 한 뒤 이를 다시 $\\exp$ 취해줘야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보통 전체 단어가 10만개 이상 주어지므로 계산량이 어마어마 해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Negative Sampling은 소프트맥스 확률을 구할 때 전체 단어를 구하지 않고, 일부 단어만 뽑아서 계산을 하자는 아이디어다.[3][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용자가 지정한 윈도우 사이즈 내에 등장하지 않는 단어(Negative saple)을 5~20개 정도 뽑고, 이를 우리가 예측하고자 하는 타겟 단어와 합쳐 전체 단어처럼 소프트맥스 확률을 구하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 보통 5~20 ($k$) 개 정도 뽑는데, 이는 만약 윈도우 사이즈가 5일 경우 최대 25개의 단어를 대상으로만 소프트맥스 확률을 계산하고, 파라미터 업데이트도 25개 대상으로만 이뤄진다는 이야기이다.[3][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(w_O|w_I) = \\frac{\\exp\\,({v^{\\prime}_{w_O}}^Tv_{w_{I}})} {\\sum_{i=1}^k \\exp\\,({v^{\\prime}_w}^Tv_{w_{I}}) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 특정 단어가 negative sampliing 될 확률 $P_n(w_i) $은 다음과 같다.(논문에선 정의한 수식은 첫번째인데....더 찾아보자) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_n(w_i)= \\frac {U(w_i)^{3/4}}{Z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이건 구글링한 수식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P_n(w_i)=\\frac {f(w_i)^{3/4}}{\\sum_{j=0}^n f(w_j)^{3/4}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  $$f(w_i) =  (해당\\,\\,단어\\,\\,빈도\\,\\, /\\,\\,전체\\,\\, 단어\\,\\, 수)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고문헌\n",
    "\n",
    "skip-gram : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/\n",
    "\n",
    " [1] : [Bengio, Yoshua, et al. \"A neural probabilistic language model.\" Journal of machine learning research 3.Feb (2003): 1137-1155.](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Ftmp%2Fbengio03a.pdf&clen=140095&chunk=true)\n",
    "\n",
    "\n",
    "[2] : [Young, Tom, et al. \"Recent trends in deep learning based natural language processing.\" ieee Computational intelligenCe magazine 13.3 (2018): 55-75.](https://ieeexplore.ieee.org/abstract/document/8416973)\n",
    "\n",
    "\n",
    "[3] : [Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781 (2013).](https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "[4] : [Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems. 2013.](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2013%2Ffile%2F9aa42b31882ec039965f3c4923ce901b-Paper.pdf&clen=111997)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
