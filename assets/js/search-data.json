{
  
    
        "post0": {
            "title": "KoBert(데이터 증강 이후)",
            "content": "데이터 증강했더니 결과가 말도 안되게 좋게 나옴..... | . 문서 1개당 데이터 증강 기법을 이용하여 10개씩 증가 시킴 | . GPU &#50857;&#47049;&#54869;&#51064; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . cd /content/drive/MyDrive/Colab Notebooks . /content/drive/MyDrive/Colab Notebooks . `런타임 초기화 하고 아래코드 돌려서 gpu용량 확인하고 코드돌리자.... . import torch import gc gc.collect() torch.cuda.empty_cache() . !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi !pip install gputil !pip install psutil !pip install humanize import psutil import humanize import os import GPUtil as GPU GPUs = GPU.getGPUs() # XXX: only one GPU on Colab and isn’t guaranteed gpu = GPUs[0] def printm(): process = psutil.Process(os.getpid()) print(&quot;Gen RAM Free: &quot; + humanize.naturalsize(psutil.virtual_memory().available), &quot; | Proc size: &quot; + humanize.naturalsize(process.memory_info().rss)) print(&quot;GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB&quot;.format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal)) printm() . Collecting gputil Downloading GPUtil-1.4.0.tar.gz (5.5 kB) Building wheels for collected packages: gputil Building wheel for gputil (setup.py) ... done Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=271d573a9c70e2a88e7a3a2a58c0ef0d94f02fdb2ad0c98e3da4655a55c83c6f Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c Successfully built gputil Installing collected packages: gputil Successfully installed gputil-1.4.0 Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8) Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1) Gen RAM Free: 25.7 GB | Proc size: 1.2 GB GPU RAM Free: 16280MB | Used: 0MB | Util 0% | Total 16280MB . Colab &#54872;&#44221;&#49444;&#51221; . !pip install gluonnlp pandas tqdm !pip install mxnet !pip install sentencepiece==0.1.91 !pip install transformers==4.8.2 !pip install torch . github&#50640;&#49436; KoBERT &#54028;&#51068;&#51012; &#47196;&#46300; &#48143; KoBERT&#47784;&#45944; &#48520;&#47084;&#50724;&#44592; . !pip install &#39;git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&amp;subdirectory=kobert_hf&#39; . Collecting kobert_tokenizer Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-4r3ewpxg/kobert-tokenizer_c82b16b44def42eb8d0e87007fa3446b Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-4r3ewpxg/kobert-tokenizer_c82b16b44def42eb8d0e87007fa3446b Building wheels for collected packages: kobert-tokenizer Building wheel for kobert-tokenizer (setup.py) ... done Created wheel for kobert-tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4649 sha256=c152b7704cfcbfa48a71c2292cd07b7ee81f68d742d4fd17cf3ce6cf956a8569 Stored in directory: /tmp/pip-ephem-wheel-cache-5jjwadk9/wheels/10/b4/d9/cb627bbfaefa266657b0b4e8127f7bf96d27376fa1a23897b4 Successfully built kobert-tokenizer Installing collected packages: kobert-tokenizer Successfully installed kobert-tokenizer-0.1 . !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master . Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-wf20wsut Running command git clone -q &#39;https://****@github.com/SKTBrain/KoBERT.git&#39; /tmp/pip-req-build-wf20wsut Collecting boto3 Downloading boto3-1.21.14-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 2.7 MB/s Collecting gluonnlp&gt;=0.6.0 Downloading gluonnlp-0.10.0.tar.gz (344 kB) |████████████████████████████████| 344 kB 19.5 MB/s Collecting mxnet&gt;=1.4.0 Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB) |████████████████████████████████| 47.3 MB 1.3 MB/s Collecting onnxruntime==1.8.0 Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB) |████████████████████████████████| 4.5 MB 57.6 MB/s Collecting sentencepiece&gt;=0.1.6 Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB) |████████████████████████████████| 1.2 MB 51.9 MB/s Requirement already satisfied: torch&gt;=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111) Collecting transformers&gt;=4.8.1 Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB) |████████████████████████████████| 3.8 MB 76.8 MB/s Requirement already satisfied: numpy&gt;=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (1.21.5) Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (2.0) Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (3.17.3) Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (0.29.28) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (21.3) Collecting graphviz&lt;0.9.0,&gt;=0.8.1 Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB) Requirement already satisfied: requests&lt;3,&gt;=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2.23.0) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2021.10.8) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.7.0-&gt;kobert==0.2.3) (3.10.0.2) Collecting pyyaml&gt;=5.1 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |████████████████████████████████| 596 kB 61.5 MB/s Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (4.63.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (2019.12.20) Collecting huggingface-hub&lt;1.0,&gt;=0.1.0 Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB) |████████████████████████████████| 67 kB 7.0 MB/s Collecting tokenizers!=0.11.3,&gt;=0.11.1 Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB) |████████████████████████████████| 6.5 MB 49.4 MB/s Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (4.11.2) Collecting sacremoses Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 44.5 MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (3.6.0) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (3.0.7) Collecting botocore&lt;1.25.0,&gt;=1.24.14 Downloading botocore-1.24.14-py3-none-any.whl (8.6 MB) |████████████████████████████████| 8.6 MB 47.3 MB/s Collecting s3transfer&lt;0.6.0,&gt;=0.5.0 Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 8.0 MB/s Collecting jmespath&lt;1.0.0,&gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore&lt;1.25.0,&gt;=1.24.14-&gt;boto3-&gt;kobert==0.2.3) (2.8.2) Collecting urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB) |████████████████████████████████| 127 kB 85.9 MB/s Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.25.0,&gt;=1.24.14-&gt;boto3-&gt;kobert==0.2.3) (1.15.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (3.7.0) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (1.1.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (7.1.2) Building wheels for collected packages: kobert, gluonnlp Building wheel for kobert (setup.py) ... done Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15674 sha256=cfdc156fbb3359ef69bf2322cf258f4c9e2a7d6d6d1d9f9f38bd0680210da889 Stored in directory: /tmp/pip-ephem-wheel-cache-og_2ib6e/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0 Building wheel for gluonnlp (setup.py) ... done Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595733 sha256=1721ccb325c78ee23df62639bcde95b8bafe1fcee5f3796d599a24d58fcfa30d Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00 Successfully built kobert gluonnlp Installing collected packages: urllib3, jmespath, pyyaml, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, graphviz, transformers, sentencepiece, onnxruntime, mxnet, gluonnlp, boto3, kobert Attempting uninstall: urllib3 Found existing installation: urllib3 1.24.3 Uninstalling urllib3-1.24.3: Successfully uninstalled urllib3-1.24.3 Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: graphviz Found existing installation: graphviz 0.10.1 Uninstalling graphviz-0.10.1: Successfully uninstalled graphviz-0.10.1 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. Successfully installed boto3-1.21.14 botocore-1.24.14 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.4.0 jmespath-0.10.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 s3transfer-0.5.2 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.17.0 urllib3-1.25.11 . from kobert import get_pytorch_kobert_model from kobert_tokenizer import KoBERTTokenizer tokenizer = KoBERTTokenizer.from_pretrained(&#39;skt/kobert-base-v1&#39;) bertmodel, vocab = get_pytorch_kobert_model() . The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. The tokenizer class you load from this checkpoint is &#39;XLNetTokenizer&#39;. The class this function is called from is &#39;KoBERTTokenizer&#39;. . using cached model. /content/drive/MyDrive/Colab Notebooks/.cache/kobert_v1.zip using cached model. /content/drive/MyDrive/Colab Notebooks/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece . &#54596;&#50836;&#54620; &#46972;&#51060;&#48652;&#47084;&#47532; &#48520;&#47084;&#50724;&#44592; . import torch from torch import nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import Dataset, DataLoader import gluonnlp as nlp import numpy as np from tqdm import tqdm, tqdm_notebook import pandas as pd #transformers from transformers import AdamW from transformers.optimization import get_cosine_schedule_with_warmup from transformers import BertModel #GPU 사용 시 device = torch.device(&quot;cuda:0&quot;) . &#45936;&#51060;&#53552;&#49483; &#48520;&#47084;&#50724;&#44592; . import pandas as pd data = pd.read_csv(&#39;kobert입력데이터2.csv&#39;) . data.head() . document topic text . 0 1 | 15 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 1 1 | 15 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 2 1 | 15 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 3 1 | 15 | 존경 지지 주택 전시관 계약 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 4 1 | 15 | 존경 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 업체 관계 사... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; data[&quot;topic&quot;] = data[&quot;topic&quot;]-1 . data.head() . document topic text . 0 1 | 14 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 1 1 | 14 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 2 1 | 14 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 3 1 | 14 | 존경 지지 주택 전시관 계약 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 4 1 | 14 | 존경 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 업체 관계 사... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; data_list = [] for ques, label in zip(data[&#39;text&#39;], data[&#39;topic&#39;]) : data = [] data.append(ques) data.append(str(label)) data_list.append(data) . &#51077;&#47141; &#45936;&#51060;&#53552;&#49483;&#51012; &#53664;&#53360;&#54868;&#54616;&#44592; . class BERTDataset(Dataset): def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len, pad, pair): transform = nlp.data.BERTSentenceTransform( bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair) self.sentences = [transform([i[sent_idx]]) for i in dataset] self.labels = [np.int32(i[label_idx]) for i in dataset] def __getitem__(self, i): return (self.sentences[i] + (self.labels[i], )) def __len__(self): return (len(self.labels)) . &#54028;&#46972;&#48120;&#53552; &#49483;&#54021; . max_len = 200 batch_size = 32 warmup_ratio = 0.1 num_epochs = 10 max_grad_norm = 1 log_interval = 200 learning_rate = 4e-5 . Train data &amp; Test data . from sklearn.model_selection import train_test_split dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=34) . tok=tokenizer.tokenize data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False) data_test = BERTDataset(dataset_test,0, 1, tok, vocab, max_len, True, False) . &#53664;&#53360;&#54868; &#48143; &#54056;&#46377;, &#51221;&#49688; &#51064;&#53076;&#46377; &#51652;&#54665; . train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4) test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4) . KoBERT &#47784;&#45944; &#44396;&#54788; . class BERTClassifier(nn.Module): def __init__(self, bert, hidden_size = 768, num_classes=23, ##클래스 수 조정## dr_rate=None, params=None): super(BERTClassifier, self).__init__() self.bert = bert self.dr_rate = dr_rate self.classifier = nn.Linear(hidden_size , num_classes) if dr_rate: self.dropout = nn.Dropout(p=dr_rate) def gen_attention_mask(self, token_ids, valid_length): attention_mask = torch.zeros_like(token_ids) for i, v in enumerate(valid_length): attention_mask[i][:v] = 1 return attention_mask.float() def forward(self, token_ids, valid_length, segment_ids): attention_mask = self.gen_attention_mask(token_ids, valid_length) _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False) if self.dr_rate: out = self.dropout(pooler) return self.classifier(out) . model = BERTClassifier(bertmodel, dr_rate=0.3).to(device) #optimizer와 schedule 설정 no_decay = [&#39;bias&#39;, &#39;LayerNorm.weight&#39;] optimizer_grouped_parameters = [ {&#39;params&#39;: [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 1e-6}, {&#39;params&#39;: [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 0.0} ] optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate) loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 대표적인 loss func t_total = len(train_dataloader) * num_epochs warmup_step = int(t_total * warmup_ratio) scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total) #정확도 측정을 위한 함수 정의 def calc_accuracy(X,Y): max_vals, max_indices = torch.max(X, 1) train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0] return train_acc train_dataloader . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, . &lt;torch.utils.data.dataloader.DataLoader at 0x7f06d7b3ab50&gt; . train_history=[] test_history=[] loss_history=[] for e in range(num_epochs): train_acc = 0.0 test_acc = 0.0 model.train() for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)): optimizer.zero_grad() token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length= valid_length label = label.long().to(device) out = model(token_ids, valid_length, segment_ids) #print(label.shape,out.shape) loss = loss_fn(out, label) loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) optimizer.step() scheduler.step() # Update learning rate schedule train_acc += calc_accuracy(out, label) if batch_id % log_interval == 0: print(&quot;epoch {} batch id {} loss {} train acc {}&quot;.format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1))) train_history.append(train_acc / (batch_id+1)) loss_history.append(loss.data.cpu().numpy()) print(&quot;epoch {} train acc {}&quot;.format(e+1, train_acc / (batch_id+1))) #train_history.append(train_acc / (batch_id+1)) model.eval() for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)): token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length= valid_length label = label.long().to(device) out = model(token_ids, valid_length, segment_ids) test_acc += calc_accuracy(out, label) print(&quot;epoch {} test acc {}&quot;.format(e+1, test_acc / (batch_id+1))) test_history.append(test_acc / (batch_id+1)) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` . epoch 1 batch id 1 loss 3.1852335929870605 train acc 0.0625 epoch 1 batch id 201 loss 3.1079132556915283 train acc 0.055970149253731345 epoch 1 batch id 401 loss 2.9384312629699707 train acc 0.08354114713216958 epoch 1 batch id 601 loss 2.4579482078552246 train acc 0.19098377703826955 epoch 1 batch id 801 loss 1.7107940912246704 train acc 0.2938514357053683 epoch 1 batch id 1001 loss 1.6819950342178345 train acc 0.36975524475524474 epoch 1 batch id 1201 loss 1.475642442703247 train acc 0.4274302664446295 epoch 1 batch id 1401 loss 1.1943906545639038 train acc 0.47309957173447537 epoch 1 batch id 1601 loss 0.37908223271369934 train acc 0.5094081823860087 epoch 1 batch id 1801 loss 1.336313247680664 train acc 0.5388499444752916 epoch 1 batch id 2001 loss 0.5357224345207214 train acc 0.5646864067966016 epoch 1 batch id 2201 loss 0.5976346135139465 train acc 0.586182417083144 epoch 1 batch id 2401 loss 0.544891893863678 train acc 0.6052035610162433 epoch 1 batch id 2601 loss 0.5338609218597412 train acc 0.6225730488273741 epoch 1 batch id 2801 loss 0.8217948079109192 train acc 0.6376963584434131 epoch 1 batch id 3001 loss 0.40613868832588196 train acc 0.6512308397200933 epoch 1 batch id 3201 loss 0.5672867894172668 train acc 0.6639136207435177 epoch 1 batch id 3401 loss 0.1540270745754242 train acc 0.6753436489267862 epoch 1 batch id 3601 loss 0.4213937819004059 train acc 0.6859205776173285 epoch 1 batch id 3801 loss 0.6628513336181641 train acc 0.6954502104709287 epoch 1 train acc 0.7042229307326832 . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` . epoch 1 test acc 0.8896319444444445 epoch 2 batch id 1 loss 0.3010570704936981 train acc 0.90625 epoch 2 batch id 201 loss 1.015760064125061 train acc 0.8748445273631841 epoch 2 batch id 401 loss 0.3370214104652405 train acc 0.8772599750623441 epoch 2 batch id 601 loss 0.46612173318862915 train acc 0.8816035773710482 epoch 2 batch id 801 loss 0.23539766669273376 train acc 0.8852215980024969 epoch 2 batch id 1001 loss 0.5730848908424377 train acc 0.8883616383616384 epoch 2 batch id 1201 loss 0.09476225078105927 train acc 0.8913405495420483 epoch 2 batch id 1401 loss 0.5718106627464294 train acc 0.8944503925767309 epoch 2 batch id 1601 loss 0.19091130793094635 train acc 0.8969394128669581 epoch 2 batch id 1801 loss 0.20529909431934357 train acc 0.8996564408661855 epoch 2 batch id 2001 loss 0.22421641647815704 train acc 0.9017053973013494 epoch 2 batch id 2201 loss 0.22272928059101105 train acc 0.9030838255338483 epoch 2 batch id 2401 loss 0.042043376713991165 train acc 0.9044538733860892 epoch 2 batch id 2601 loss 0.12918879091739655 train acc 0.9065023068050749 epoch 2 batch id 2801 loss 0.3433103561401367 train acc 0.9082582113530882 epoch 2 batch id 3001 loss 0.10605460405349731 train acc 0.9098737920693102 epoch 2 batch id 3201 loss 0.38171711564064026 train acc 0.9113948766010622 epoch 2 batch id 3401 loss 0.06485921889543533 train acc 0.9130494707438989 epoch 2 batch id 3601 loss 0.10051734745502472 train acc 0.91439009997223 epoch 2 batch id 3801 loss 0.291980117559433 train acc 0.9153594448829255 epoch 2 train acc 0.9167682545636409 epoch 2 test acc 0.93728125 epoch 3 batch id 1 loss 0.02345673181116581 train acc 1.0 epoch 3 batch id 201 loss 0.6477029919624329 train acc 0.9347014925373134 epoch 3 batch id 401 loss 0.25639456510543823 train acc 0.9367986284289277 epoch 3 batch id 601 loss 0.2181258201599121 train acc 0.9372920133111481 epoch 3 batch id 801 loss 0.08926738798618317 train acc 0.9382802746566792 epoch 3 batch id 1001 loss 0.1647966057062149 train acc 0.9378121878121878 epoch 3 batch id 1201 loss 0.05174892395734787 train acc 0.9384367194004996 epoch 3 batch id 1401 loss 0.2204378992319107 train acc 0.9395521056388294 epoch 3 batch id 1601 loss 0.03071366436779499 train acc 0.9405840099937539 epoch 3 batch id 1801 loss 0.12029184401035309 train acc 0.9409876457523598 epoch 3 batch id 2001 loss 0.25424206256866455 train acc 0.9415292353823088 epoch 3 batch id 2201 loss 0.05183256417512894 train acc 0.9416316447069514 epoch 3 batch id 2401 loss 0.2688687741756439 train acc 0.9421074552269888 epoch 3 batch id 2601 loss 0.049613308161497116 train acc 0.9428344867358708 epoch 3 batch id 2801 loss 0.19678881764411926 train acc 0.9433684398429133 epoch 3 batch id 3001 loss 0.01577559858560562 train acc 0.9439145284905032 epoch 3 batch id 3201 loss 0.33903968334198 train acc 0.944304514214308 epoch 3 batch id 3401 loss 0.07305717468261719 train acc 0.944951852396354 epoch 3 batch id 3601 loss 0.16152040660381317 train acc 0.9454144682032769 epoch 3 batch id 3801 loss 0.1871003359556198 train acc 0.945614640883978 epoch 3 train acc 0.9460958989747437 epoch 3 test acc 0.9495 epoch 4 batch id 1 loss 0.012217085808515549 train acc 1.0 epoch 4 batch id 201 loss 0.6240464448928833 train acc 0.9513370646766169 epoch 4 batch id 401 loss 0.15754097700119019 train acc 0.9538653366583542 epoch 4 batch id 601 loss 0.08080334216356277 train acc 0.9545029118136439 epoch 4 batch id 801 loss 0.07366951555013657 train acc 0.9544709737827716 epoch 4 batch id 1001 loss 0.14693213999271393 train acc 0.9536401098901099 epoch 4 batch id 1201 loss 0.17272573709487915 train acc 0.9538665695253955 epoch 4 batch id 1401 loss 0.21869274973869324 train acc 0.9544075660242684 epoch 4 batch id 1601 loss 0.05171825364232063 train acc 0.9544034978138664 epoch 4 batch id 1801 loss 0.017643172293901443 train acc 0.9544523875624653 epoch 4 batch id 2001 loss 0.19172336161136627 train acc 0.9544134182908546 epoch 4 batch id 2201 loss 0.10434684157371521 train acc 0.9543815311222171 epoch 4 batch id 2401 loss 0.14382174611091614 train acc 0.9545371720116618 epoch 4 batch id 2601 loss 0.02976571023464203 train acc 0.9551134179161861 epoch 4 batch id 2801 loss 0.34157559275627136 train acc 0.9551276329882185 epoch 4 batch id 3001 loss 0.01952483132481575 train acc 0.9552336721092969 epoch 4 batch id 3201 loss 0.21966442465782166 train acc 0.9554826616682287 epoch 4 batch id 3401 loss 0.08189360052347183 train acc 0.9556656130549839 epoch 4 batch id 3601 loss 0.016298091039061546 train acc 0.9558976673146348 epoch 4 batch id 3801 loss 0.06911414116621017 train acc 0.9558997632202052 epoch 4 train acc 0.9561531007751938 epoch 4 test acc 0.95140625 epoch 5 batch id 1 loss 0.09324368834495544 train acc 0.96875 epoch 5 batch id 201 loss 0.6039071679115295 train acc 0.9524253731343284 epoch 5 batch id 401 loss 0.20248308777809143 train acc 0.9558135910224439 epoch 5 batch id 601 loss 0.022437140345573425 train acc 0.9564787853577371 epoch 5 batch id 801 loss 0.008236521854996681 train acc 0.9570458801498127 epoch 5 batch id 1001 loss 0.3836018741130829 train acc 0.9569805194805194 epoch 5 batch id 1201 loss 0.03141740337014198 train acc 0.9577175270607827 epoch 5 batch id 1401 loss 0.24425312876701355 train acc 0.9579764453961456 epoch 5 batch id 1601 loss 0.01882309466600418 train acc 0.9584049031855091 epoch 5 batch id 1801 loss 0.009548060595989227 train acc 0.9588770127706829 epoch 5 batch id 2001 loss 0.2631719708442688 train acc 0.958895552223888 epoch 5 batch id 2201 loss 0.06503468751907349 train acc 0.958924920490686 epoch 5 batch id 2401 loss 0.1919662356376648 train acc 0.959222719700125 epoch 5 batch id 2601 loss 0.04719703271985054 train acc 0.9595227797001153 epoch 5 batch id 2801 loss 0.3168156147003174 train acc 0.9595345412352732 epoch 5 batch id 3001 loss 0.02686820924282074 train acc 0.959638453848717 epoch 5 batch id 3201 loss 0.14111323654651642 train acc 0.9599832083723836 epoch 5 batch id 3401 loss 0.0349947065114975 train acc 0.9602506615701264 epoch 5 batch id 3601 loss 0.08212032914161682 train acc 0.9603321993890586 epoch 5 batch id 3801 loss 0.18731896579265594 train acc 0.9603558274138385 epoch 5 train acc 0.9605057514378594 epoch 5 test acc 0.954 epoch 6 batch id 1 loss 0.019299624487757683 train acc 1.0 epoch 6 batch id 201 loss 0.5856429934501648 train acc 0.9597325870646766 epoch 6 batch id 401 loss 0.192503422498703 train acc 0.9620480049875312 epoch 6 batch id 601 loss 0.06098446995019913 train acc 0.961730449251248 epoch 6 batch id 801 loss 0.016413744539022446 train acc 0.9617275280898876 epoch 6 batch id 1001 loss 0.1038106381893158 train acc 0.9609453046953047 epoch 6 batch id 1201 loss 0.05387420952320099 train acc 0.9610741049125728 epoch 6 batch id 1401 loss 0.06666409224271774 train acc 0.9613445753033547 epoch 6 batch id 1601 loss 0.02861827239394188 train acc 0.9616060274828232 epoch 6 batch id 1801 loss 0.015487557277083397 train acc 0.9615491393670184 epoch 6 batch id 2001 loss 0.2132091075181961 train acc 0.9614567716141929 epoch 6 batch id 2201 loss 0.0814809575676918 train acc 0.9613669922762381 epoch 6 batch id 2401 loss 0.0691489726305008 train acc 0.9615004164931279 epoch 6 batch id 2601 loss 0.06692101806402206 train acc 0.9619737600922722 epoch 6 batch id 2801 loss 0.258147656917572 train acc 0.9622679400214209 epoch 6 batch id 3001 loss 0.027032947167754173 train acc 0.9624812562479174 epoch 6 batch id 3201 loss 0.2069619596004486 train acc 0.9627655420181194 epoch 6 batch id 3401 loss 0.006746581289917231 train acc 0.962970449867686 epoch 6 batch id 3601 loss 0.052793052047491074 train acc 0.963152596500972 epoch 6 batch id 3801 loss 0.05918806418776512 train acc 0.9630442646671928 epoch 6 train acc 0.9632251812953239 epoch 6 test acc 0.95334375 epoch 7 batch id 1 loss 0.022131115198135376 train acc 1.0 epoch 7 batch id 201 loss 0.22887606918811798 train acc 0.960976368159204 epoch 7 batch id 401 loss 0.10670600086450577 train acc 0.9637624688279302 epoch 7 batch id 601 loss 0.027210498228669167 train acc 0.963914309484193 epoch 7 batch id 801 loss 0.021994328126311302 train acc 0.9643024344569289 epoch 7 batch id 1001 loss 0.10893231630325317 train acc 0.9639735264735265 epoch 7 batch id 1201 loss 0.018332282081246376 train acc 0.9641444629475437 epoch 7 batch id 1401 loss 0.06376929581165314 train acc 0.9639320128479657 epoch 7 batch id 1601 loss 0.019474318251013756 train acc 0.9642996564647096 epoch 7 batch id 1801 loss 0.01565304398536682 train acc 0.9643600777345919 epoch 7 batch id 2001 loss 0.09814544022083282 train acc 0.9640179910044977 epoch 7 batch id 2201 loss 0.06151555851101875 train acc 0.9640930258973194 epoch 7 batch id 2401 loss 0.13854467868804932 train acc 0.964246668054977 epoch 7 batch id 2601 loss 0.03242442011833191 train acc 0.9647611495578624 epoch 7 batch id 2801 loss 0.1967732310295105 train acc 0.9649009282399144 epoch 7 batch id 3001 loss 0.019724251702427864 train acc 0.9650012495834722 epoch 7 batch id 3201 loss 0.16426371037960052 train acc 0.9651866604186192 epoch 7 batch id 3401 loss 0.0070608267560601234 train acc 0.9654513378418113 epoch 7 batch id 3601 loss 0.032421212643384933 train acc 0.9656432241044154 epoch 7 batch id 3801 loss 0.059247471392154694 train acc 0.9655929360694554 epoch 7 train acc 0.9658664666166542 epoch 7 test acc 0.9529375 epoch 8 batch id 1 loss 0.01750648394227028 train acc 1.0 epoch 8 batch id 201 loss 0.2634584605693817 train acc 0.9642412935323383 epoch 8 batch id 401 loss 0.05795007571578026 train acc 0.9666458852867831 epoch 8 batch id 601 loss 0.04528792202472687 train acc 0.9671900998336106 epoch 8 batch id 801 loss 0.0184800885617733 train acc 0.9673455056179775 epoch 8 batch id 1001 loss 0.11017853021621704 train acc 0.9668768731268731 epoch 8 batch id 1201 loss 0.02038302645087242 train acc 0.9667464612822648 epoch 8 batch id 1401 loss 0.06230441853404045 train acc 0.9668094218415417 epoch 8 batch id 1601 loss 0.016102619469165802 train acc 0.966798094940662 epoch 8 batch id 1801 loss 0.017888877540826797 train acc 0.9672577734591893 epoch 8 batch id 2001 loss 0.1735694259405136 train acc 0.9671726636681659 epoch 8 batch id 2201 loss 0.06498179584741592 train acc 0.9669894366197183 epoch 8 batch id 2401 loss 0.04991516098380089 train acc 0.9671621199500208 epoch 8 batch id 2601 loss 0.014021644368767738 train acc 0.9675725682429834 epoch 8 batch id 2801 loss 0.09371583163738251 train acc 0.9678909318100678 epoch 8 batch id 3001 loss 0.026674829423427582 train acc 0.96796901032989 epoch 8 batch id 3201 loss 0.13769741356372833 train acc 0.9681349578256795 epoch 8 batch id 3401 loss 0.017529677599668503 train acc 0.9683181417230227 epoch 8 batch id 3601 loss 0.019356414675712585 train acc 0.9685590808108858 epoch 8 batch id 3801 loss 0.02890157327055931 train acc 0.968355367008682 epoch 8 train acc 0.9685155663915979 epoch 8 test acc 0.9530625 epoch 9 batch id 1 loss 0.03973601013422012 train acc 1.0 epoch 9 batch id 201 loss 0.2246280312538147 train acc 0.964863184079602 epoch 9 batch id 401 loss 0.08065763860940933 train acc 0.9692955112219451 epoch 9 batch id 601 loss 0.046916451305150986 train acc 0.9702579034941764 epoch 9 batch id 801 loss 0.01988433115184307 train acc 0.9701935081148564 epoch 9 batch id 1001 loss 0.09331707656383514 train acc 0.9699050949050949 epoch 9 batch id 1201 loss 0.019136041402816772 train acc 0.9695826394671108 epoch 9 batch id 1401 loss 0.04694148525595665 train acc 0.9699098857958601 epoch 9 batch id 1601 loss 0.01455066166818142 train acc 0.9699406620861961 epoch 9 batch id 1801 loss 0.007262763567268848 train acc 0.969981954469739 epoch 9 batch id 2001 loss 0.10219796746969223 train acc 0.9698432033983009 epoch 9 batch id 2201 loss 0.05588527396321297 train acc 0.9694883007723762 epoch 9 batch id 2401 loss 0.044618528336286545 train acc 0.9695829862557268 epoch 9 batch id 2601 loss 0.009160742163658142 train acc 0.9696991541714726 epoch 9 batch id 2801 loss 0.08222953975200653 train acc 0.9698656729739379 epoch 9 batch id 3001 loss 0.010992823168635368 train acc 0.9700412362545818 epoch 9 batch id 3201 loss 0.10338685661554337 train acc 0.970204623555139 epoch 9 batch id 3401 loss 0.001963959774002433 train acc 0.9703212290502793 epoch 9 batch id 3601 loss 0.0523468442261219 train acc 0.9704335601221883 epoch 9 batch id 3801 loss 0.018299525603652 train acc 0.9703778610891871 epoch 9 train acc 0.9705863965991498 epoch 9 test acc 0.9509375 epoch 10 batch id 1 loss 0.0445123054087162 train acc 0.96875 epoch 10 batch id 201 loss 0.1403944492340088 train acc 0.9709266169154229 epoch 10 batch id 401 loss 0.0452195405960083 train acc 0.971399625935162 epoch 10 batch id 601 loss 0.04688456282019615 train acc 0.9723897670549085 epoch 10 batch id 801 loss 0.025614922866225243 train acc 0.9720271535580525 epoch 10 batch id 1001 loss 0.07424446940422058 train acc 0.9716221278721279 epoch 10 batch id 1201 loss 0.01959993503987789 train acc 0.9716382181515404 epoch 10 batch id 1401 loss 0.06428100913763046 train acc 0.9718281584582441 epoch 10 batch id 1601 loss 0.04180331155657768 train acc 0.9719901623985009 epoch 10 batch id 1801 loss 0.011722804978489876 train acc 0.9721335369239311 epoch 10 batch id 2001 loss 0.13746821880340576 train acc 0.9721233133433284 epoch 10 batch id 2201 loss 0.05055586248636246 train acc 0.9719161744661517 epoch 10 batch id 2401 loss 0.036505863070487976 train acc 0.9720168679716785 epoch 10 batch id 2601 loss 0.007652627769857645 train acc 0.9721741637831603 epoch 10 batch id 2801 loss 0.08486463129520416 train acc 0.9723870938950375 epoch 10 batch id 3001 loss 0.019888095557689667 train acc 0.9726237087637454 epoch 10 batch id 3201 loss 0.10162699222564697 train acc 0.972801468291159 epoch 10 batch id 3401 loss 0.0029431877192109823 train acc 0.9728756248162305 epoch 10 batch id 3601 loss 0.03670577332377434 train acc 0.9729241877256317 epoch 10 batch id 3801 loss 0.04396698251366615 train acc 0.9728525388055774 epoch 10 train acc 0.9731417229307326 epoch 10 test acc 0.95165625 . import pandas as pd . len(train_history) . 200 . len(loss_history) . 200 . test_history . [0.8896319444444445, 0.93728125, 0.9495, 0.95140625, 0.954, 0.95334375, 0.9529375, 0.9530625, 0.9509375, 0.95165625] . total_train = pd.DataFrame() . total_train[&quot;train_history&quot;] = train_history total_train[&quot;loss_history&quot;] = loss_history . total_test = pd.DataFrame() . total_test[&quot;test_history&quot;] = test_history . total_train.to_csv(&quot;데이터증강 train.csv&quot;) . total_test.to_csv(&quot;데이터증강 test.csv&quot;) . .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2022/03/09/Kobert_%EB%AA%A8%EB%8D%B8_%ED%95%99%EC%8A%B5(%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A6%9D%EA%B0%95).html",
            "relUrl": "/python/2022/03/09/Kobert_%EB%AA%A8%EB%8D%B8_%ED%95%99%EC%8A%B5(%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A6%9D%EA%B0%95).html",
            "date": " • Mar 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "KoBert",
            "content": "GPU &#50857;&#47049;&#54869;&#51064; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . cd /content/drive/MyDrive/Colab Notebooks . /content/drive/MyDrive/Colab Notebooks . `런타임 초기화 하고 아래코드 돌려서 gpu용량 확인하고 코드돌리자.... . import torch import gc gc.collect() torch.cuda.empty_cache() . !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi !pip install gputil !pip install psutil !pip install humanize import psutil import humanize import os import GPUtil as GPU GPUs = GPU.getGPUs() # XXX: only one GPU on Colab and isn’t guaranteed gpu = GPUs[0] def printm(): process = psutil.Process(os.getpid()) print(&quot;Gen RAM Free: &quot; + humanize.naturalsize(psutil.virtual_memory().available), &quot; | Proc size: &quot; + humanize.naturalsize(process.memory_info().rss)) print(&quot;GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB&quot;.format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal)) printm() . Collecting gputil Downloading GPUtil-1.4.0.tar.gz (5.5 kB) Building wheels for collected packages: gputil Building wheel for gputil (setup.py) ... done Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=6345eee9a52764257055115c4c1bdb855bb6a977fb99d26598ba03998f44e708 Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c Successfully built gputil Installing collected packages: gputil Successfully installed gputil-1.4.0 Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8) Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1) Gen RAM Free: 25.6 GB | Proc size: 1.2 GB GPU RAM Free: 16280MB | Used: 0MB | Util 0% | Total 16280MB . Colab &#54872;&#44221;&#49444;&#51221; . !pip install gluonnlp pandas tqdm !pip install mxnet !pip install sentencepiece==0.1.91 !pip install transformers==4.8.2 !pip install torch . github&#50640;&#49436; KoBERT &#54028;&#51068;&#51012; &#47196;&#46300; &#48143; KoBERT&#47784;&#45944; &#48520;&#47084;&#50724;&#44592; . !pip install &#39;git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&amp;subdirectory=kobert_hf&#39; . Collecting kobert_tokenizer Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-pottzbzv/kobert-tokenizer_27f27d2645ba4a29802a219fa8635a27 Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-pottzbzv/kobert-tokenizer_27f27d2645ba4a29802a219fa8635a27 Building wheels for collected packages: kobert-tokenizer Building wheel for kobert-tokenizer (setup.py) ... done Created wheel for kobert-tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4649 sha256=5f5f6b0cf121b76b5513ae2e422e20a16c13aee407bc53785faa89c2d1fa0efc Stored in directory: /tmp/pip-ephem-wheel-cache-fzvxpa0y/wheels/10/b4/d9/cb627bbfaefa266657b0b4e8127f7bf96d27376fa1a23897b4 Successfully built kobert-tokenizer Installing collected packages: kobert-tokenizer Successfully installed kobert-tokenizer-0.1 . !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master . Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-cy1tez6g Running command git clone -q &#39;https://****@github.com/SKTBrain/KoBERT.git&#39; /tmp/pip-req-build-cy1tez6g Collecting boto3 Downloading boto3-1.21.13-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 5.1 MB/s Collecting gluonnlp&gt;=0.6.0 Downloading gluonnlp-0.10.0.tar.gz (344 kB) |████████████████████████████████| 344 kB 44.6 MB/s Collecting mxnet&gt;=1.4.0 Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB) |████████████████████████████████| 47.3 MB 1.3 MB/s Collecting onnxruntime==1.8.0 Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB) |████████████████████████████████| 4.5 MB 52.9 MB/s Collecting sentencepiece&gt;=0.1.6 Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB) |████████████████████████████████| 1.2 MB 49.9 MB/s Requirement already satisfied: torch&gt;=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111) Collecting transformers&gt;=4.8.1 Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB) |████████████████████████████████| 3.8 MB 39.5 MB/s Requirement already satisfied: numpy&gt;=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (1.21.5) Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (3.17.3) Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (2.0) Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (0.29.28) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (21.3) Requirement already satisfied: requests&lt;3,&gt;=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2.23.0) Collecting graphviz&lt;0.9.0,&gt;=0.8.1 Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2021.10.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2.10) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.7.0-&gt;kobert==0.2.3) (3.10.0.2) Collecting huggingface-hub&lt;1.0,&gt;=0.1.0 Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB) |████████████████████████████████| 67 kB 6.6 MB/s Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (4.11.2) Collecting tokenizers!=0.11.3,&gt;=0.11.1 Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB) |████████████████████████████████| 6.5 MB 37.1 MB/s Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (4.63.0) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (2019.12.20) Collecting sacremoses Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 58.7 MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (3.6.0) Collecting pyyaml&gt;=5.1 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |████████████████████████████████| 596 kB 71.8 MB/s Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (3.0.7) Collecting s3transfer&lt;0.6.0,&gt;=0.5.0 Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 10.1 MB/s Collecting botocore&lt;1.25.0,&gt;=1.24.13 Downloading botocore-1.24.13-py3-none-any.whl (8.6 MB) |████████████████████████████████| 8.6 MB 63.0 MB/s Collecting jmespath&lt;1.0.0,&gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Collecting urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB) |████████████████████████████████| 127 kB 78.9 MB/s Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore&lt;1.25.0,&gt;=1.24.13-&gt;boto3-&gt;kobert==0.2.3) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.25.0,&gt;=1.24.13-&gt;boto3-&gt;kobert==0.2.3) (1.15.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (3.7.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (1.1.0) Building wheels for collected packages: kobert, gluonnlp Building wheel for kobert (setup.py) ... done Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15674 sha256=0fa8593f467634b736bdd27687c88c601d5e2d2f08e980b9e90518416f7d4f76 Stored in directory: /tmp/pip-ephem-wheel-cache-84977rnc/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0 Building wheel for gluonnlp (setup.py) ... done Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595734 sha256=ea51b29dcb3812cda6ff01d38cc522183d192dffbc57392701598ea60357ca65 Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00 Successfully built kobert gluonnlp Installing collected packages: urllib3, jmespath, pyyaml, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, graphviz, transformers, sentencepiece, onnxruntime, mxnet, gluonnlp, boto3, kobert Attempting uninstall: urllib3 Found existing installation: urllib3 1.24.3 Uninstalling urllib3-1.24.3: Successfully uninstalled urllib3-1.24.3 Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: graphviz Found existing installation: graphviz 0.10.1 Uninstalling graphviz-0.10.1: Successfully uninstalled graphviz-0.10.1 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. Successfully installed boto3-1.21.13 botocore-1.24.13 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.4.0 jmespath-0.10.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 s3transfer-0.5.2 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.17.0 urllib3-1.25.11 . from kobert import get_pytorch_kobert_model from kobert_tokenizer import KoBERTTokenizer tokenizer = KoBERTTokenizer.from_pretrained(&#39;skt/kobert-base-v1&#39;) bertmodel, vocab = get_pytorch_kobert_model() . The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. The tokenizer class you load from this checkpoint is &#39;XLNetTokenizer&#39;. The class this function is called from is &#39;KoBERTTokenizer&#39;. . using cached model. /content/drive/MyDrive/Colab Notebooks/.cache/kobert_v1.zip using cached model. /content/drive/MyDrive/Colab Notebooks/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece . &#54596;&#50836;&#54620; &#46972;&#51060;&#48652;&#47084;&#47532; &#48520;&#47084;&#50724;&#44592; . import torch from torch import nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import Dataset, DataLoader import gluonnlp as nlp import numpy as np from tqdm import tqdm, tqdm_notebook import pandas as pd #transformers from transformers import AdamW from transformers.optimization import get_cosine_schedule_with_warmup from transformers import BertModel #GPU 사용 시 device = torch.device(&quot;cuda:0&quot;) . &#45936;&#51060;&#53552;&#49483; &#48520;&#47084;&#50724;&#44592; . import pandas as pd data = pd.read_csv(&#39;kobert입력데이터.csv&#39;) . data.head() . document topic clean_txt1 . 0 1 | 15 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 1 2 | 5 | 올해 여자 작년 치매 고생 엄마 하늘 식구 엄마 우울증 무기력 상태 풀칠 강아지 연... | . 2 3 | 20 | 학생 고등학교 직전 학년 고등학교 선행 학습 학원 치열 주위 고등학교 국어 수학 과... | . 3 4 | 12 | 종국 동포 중소기업 금속 작업 교정 정리 표현 생략 어려움 세월 고국 관심 선거 고... | . 4 5 | 7 | 수고 위정자 헌법 수호 안전 위협 이슬람 영향 민족 지혜 문화 정책 내년 이슬람 관... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; data[&quot;topic&quot;] = data[&quot;topic&quot;]-1 . data.head() . document topic clean_txt1 . 0 1 | 14 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . 1 2 | 4 | 올해 여자 작년 치매 고생 엄마 하늘 식구 엄마 우울증 무기력 상태 풀칠 강아지 연... | . 2 3 | 19 | 학생 고등학교 직전 학년 고등학교 선행 학습 학원 치열 주위 고등학교 국어 수학 과... | . 3 4 | 11 | 종국 동포 중소기업 금속 작업 교정 정리 표현 생략 어려움 세월 고국 관심 선거 고... | . 4 5 | 6 | 수고 위정자 헌법 수호 안전 위협 이슬람 영향 민족 지혜 문화 정책 내년 이슬람 관... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; data_list = [] for ques, label in zip(data[&#39;clean_txt1&#39;], data[&#39;topic&#39;]) : data = [] data.append(ques) data.append(str(label)) data_list.append(data) . &#51077;&#47141; &#45936;&#51060;&#53552;&#49483;&#51012; &#53664;&#53360;&#54868;&#54616;&#44592; . class BERTDataset(Dataset): def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len, pad, pair): transform = nlp.data.BERTSentenceTransform( bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair) self.sentences = [transform([i[sent_idx]]) for i in dataset] self.labels = [np.int32(i[label_idx]) for i in dataset] def __getitem__(self, i): return (self.sentences[i] + (self.labels[i], )) def __len__(self): return (len(self.labels)) . &#54028;&#46972;&#48120;&#53552; &#49483;&#54021; . max_len = 200 batch_size = 32 warmup_ratio = 0.1 num_epochs = 5 max_grad_norm = 1 log_interval = 200 learning_rate = 4e-5 . Train data &amp; Test data . from sklearn.model_selection import train_test_split dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=34) . tok=tokenizer.tokenize data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False) data_test = BERTDataset(dataset_test,0, 1, tok, vocab, max_len, True, False) . &#53664;&#53360;&#54868; &#48143; &#54056;&#46377;, &#51221;&#49688; &#51064;&#53076;&#46377; &#51652;&#54665; . train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4) test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4) . KoBERT &#47784;&#45944; &#44396;&#54788; . class BERTClassifier(nn.Module): def __init__(self, bert, hidden_size = 768, num_classes=23, ##클래스 수 조정## dr_rate=None, params=None): super(BERTClassifier, self).__init__() self.bert = bert self.dr_rate = dr_rate self.classifier = nn.Linear(hidden_size , num_classes) if dr_rate: self.dropout = nn.Dropout(p=dr_rate) def gen_attention_mask(self, token_ids, valid_length): attention_mask = torch.zeros_like(token_ids) for i, v in enumerate(valid_length): attention_mask[i][:v] = 1 return attention_mask.float() def forward(self, token_ids, valid_length, segment_ids): attention_mask = self.gen_attention_mask(token_ids, valid_length) _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False) if self.dr_rate: out = self.dropout(pooler) return self.classifier(out) . model = BERTClassifier(bertmodel, dr_rate=0.3).to(device) #optimizer와 schedule 설정 no_decay = [&#39;bias&#39;, &#39;LayerNorm.weight&#39;] optimizer_grouped_parameters = [ {&#39;params&#39;: [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 1e-6}, {&#39;params&#39;: [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 0.0} ] optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate) loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 대표적인 loss func t_total = len(train_dataloader) * num_epochs warmup_step = int(t_total * warmup_ratio) scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total) #정확도 측정을 위한 함수 정의 def calc_accuracy(X,Y): max_vals, max_indices = torch.max(X, 1) train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0] return train_acc train_dataloader . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, . &lt;torch.utils.data.dataloader.DataLoader at 0x7f2b85501a90&gt; . train_history=[] test_history=[] loss_history=[] for e in range(num_epochs): train_acc = 0.0 test_acc = 0.0 model.train() for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)): optimizer.zero_grad() token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length= valid_length label = label.long().to(device) out = model(token_ids, valid_length, segment_ids) #print(label.shape,out.shape) loss = loss_fn(out, label) loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) optimizer.step() scheduler.step() # Update learning rate schedule train_acc += calc_accuracy(out, label) if batch_id % log_interval == 0: print(&quot;epoch {} batch id {} loss {} train acc {}&quot;.format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1))) train_history.append(train_acc / (batch_id+1)) loss_history.append(loss.data.cpu().numpy()) print(&quot;epoch {} train acc {}&quot;.format(e+1, train_acc / (batch_id+1))) #train_history.append(train_acc / (batch_id+1)) model.eval() for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)): token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length= valid_length label = label.long().to(device) out = model(token_ids, valid_length, segment_ids) test_acc += calc_accuracy(out, label) print(&quot;epoch {} test acc {}&quot;.format(e+1, test_acc / (batch_id+1))) test_history.append(test_acc / (batch_id+1)) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` . epoch 1 batch id 1 loss 3.181016445159912 train acc 0.125 epoch 1 batch id 201 loss 2.133565664291382 train acc 0.3125 epoch 1 train acc 0.49095552884615384 . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` . epoch 1 test acc 0.7092842741935483 epoch 2 batch id 1 loss 1.167819619178772 train acc 0.6875 epoch 2 batch id 201 loss 1.361846685409546 train acc 0.7293221393034826 epoch 2 train acc 0.7577524038461538 epoch 2 test acc 0.7161592741935483 epoch 3 batch id 1 loss 0.8119286894798279 train acc 0.75 epoch 3 batch id 201 loss 0.8991244435310364 train acc 0.8129664179104478 epoch 3 train acc 0.833233173076923 epoch 3 test acc 0.725554435483871 epoch 4 batch id 1 loss 0.6335163116455078 train acc 0.8125 epoch 4 batch id 201 loss 0.7015726566314697 train acc 0.882773631840796 epoch 4 train acc 0.8944170673076922 epoch 4 test acc 0.7405745967741936 epoch 5 batch id 1 loss 0.3983944058418274 train acc 0.875 epoch 5 batch id 201 loss 0.5450688004493713 train acc 0.9169776119402985 epoch 5 train acc 0.9228725961538461 epoch 5 test acc 0.743366935483871 . test_history . [0.7092842741935483, 0.7161592741935483, 0.725554435483871, 0.7405745967741936, 0.743366935483871] . loss_history . [array(3.1810164, dtype=float32), array(2.1335657, dtype=float32), array(1.1678196, dtype=float32), array(1.3618467, dtype=float32), array(0.8119287, dtype=float32), array(0.89912444, dtype=float32), array(0.6335163, dtype=float32), array(0.70157266, dtype=float32), array(0.3983944, dtype=float32), array(0.5450688, dtype=float32)] .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2022/03/07/Kobert-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5.html",
            "relUrl": "/python/2022/03/07/Kobert-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Word Embedding",
            "content": "WordEmbedding . 1. 차원의 저주(Curse of Dimensionality) : 수학적 공간 차원(=변수 개수)이 늘어나면서, 문제 계산법이 지수적으로 커지는 상황 . 만약 $x=[1,2,3,4,5], , y= [0,0,0,0,0] to (X,Y)$ 을 표현한다고 하자 . 아래와 같이 1차원 상에서 표현되는 정보를 2차원 상에서 표현하게되어 설명 공간이 $5^2 =25$가 된 것이다. . 이러한 경우를 차원의 저주라고 하며 이는 모델링 과정에서 저장 공간과 처리 시간이 불필요하게 증가됨을 의미한다. . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . . fig, axes = plt.subplots(1,2, figsize=(10,5)) ax1, ax2 =axes x = [1,2,3,4,5] y = [0,0,0,0,0] ax1.plot(x,y) ax1.set_title(&quot;1-dimensional&quot;) ax1.axis(&quot;off&quot;) ax2.plot(x,y) ax2.set_title(&quot;2-dimensional&quot;) fig.tight_layout() . . 이러한 문제점을 해결하기 위해 NLP 분야에서는 단어를 저차원에 표현하기 위한 &quot;워드 임베딩(Word Embedding)&quot;을 제안하였다. . 기존의 통계적인 방법이 단어의 출현 빈도에 집중 한다면 워드 임베딩은 서로 유사한 단어들 간 유사성을 포착하는데 집중한다. . - 가정 : 유사한 의미를 가진 단어는 유사한 문맥안에서 발견된다. . - 가정의 해석 : 유사한 의미를 가진 단어들은 유사한 단어 벡터를 가진다. . - 이점 : 이웃된 단어들의 단어 벡터들을 학습하여 단어간 유사성을 도출해낼 수 있다. . example 1 :&#39;man&#39; + &#39;royal&#39; = &#39;king&#39; . Word2Vec . 워드 &quot;Word2Vec&quot; 중 가장 대표적인 방법으로 &quot;CBOW&quot;, &quot;skip-gram&quot; 이 존재한다. | . CBOW (Continuous Bag of Words) . 주변 단어를 이용하여 중심단어를 예측한다. | . 주어진 단어집합에서 window size $k$ 를 지정해 target word 주변 $k$개의 단어들을 이용하여 조건부 확률을 계산한다. | . 프라닭 이라는 단어를 예측한다고 가정한다. | . 문장1: 금요일 밤에 프라닭은 못참지 . 문장2: 불금인데 교촌치킨 에 맥주? . 단어 : [ &quot;금요일&quot;, &quot;밤&quot;, &quot;프라닭&quot;,&quot;불금&quot;, &quot;교촌치킨&quot;, &quot;맥주&quot; ] $ to Word in R^{6 times 6}$ . 문장의 개수는 $j=2$, 단어의 개수는 총 $i=6$, 축소할 임베딩 차원의 개수는 $n=3$ 으로 설정하자. | . 차원축소를 위해 생성되는 임베딩(=가중치) 행렬 $W in R^{6 times 3}$ 으로 파이토치 기준 $N(0,1)$에서 생성된다. | . 목적 1 : $Word in R^{6 times 6} to W in R^{6 times 3}$ | . 목적 2 : 단어간 의미적 유사성을 포착하기 위한 임베딩 행렬 갱신 $W^{t} to W^{t+1} $ | . 1. one-hot vector $Word in R^{6 times 6}$ 생성 . import numpy as np import pandas as pd index = [ &quot;금요일&quot;, &quot;밤&quot;, &quot;프라닭&quot;,&quot;불금&quot;, &quot;교촌치킨&quot;, &quot;맥주&quot; ] word1 = [1,0,0,0,0,0] word2 = [0,1,0,0,0,0] word3 = [0,0,1,0,0,0] word4 = [0,0,0,1,0,0] word5 = [0,0,0,0,1,0] word6 = [0,0,0,0,0,1] one_hot = pd.DataFrame([word1,word2,word3,word4,word5,word6],index=index) one_hot . . 0 1 2 3 4 5 . 금요일 1 | 0 | 0 | 0 | 0 | 0 | . 밤 0 | 1 | 0 | 0 | 0 | 0 | . 프라닭 0 | 0 | 1 | 0 | 0 | 0 | . 불금 0 | 0 | 0 | 1 | 0 | 0 | . 교촌치킨 0 | 0 | 0 | 0 | 1 | 0 | . 맥주 0 | 0 | 0 | 0 | 0 | 1 | . 2. 임베딩(가중치) 행렬 생성 $W in R^{6 times 3}$ . W = np.random.normal(loc = 0, scale=1,size=18).reshape(6,3) W = pd.DataFrame(W,index=index, columns = [&quot;W1&quot;,&quot;W2&quot;,&quot;W3&quot;]) W . . W1 W2 W3 . 금요일 -0.946677 | -0.964799 | -2.236172 | . 밤 1.481341 | 0.678401 | -1.239748 | . 프라닭 -0.855941 | 0.556102 | 0.330505 | . 불금 0.316146 | -1.791996 | -0.307091 | . 교촌치킨 1.289018 | -1.415381 | 0.418707 | . 맥주 1.106920 | 0.051748 | 0.478479 | . 3. $ widehat W_{프라닭} = frac {W_{밤} + W_{불금}} {2} = [0.89,-0.55,0.77]$ . W_1 = list((W.loc[&quot;밤&quot;] + W.loc[&quot;불금&quot;])/2) W_1 . . [0.8987433793584608, -0.5567973759616802, -0.7734197905021276] . 4. $ Z = widehat W_{프라닭} times W^T = $ [ 1.42, 1.91, -1.33, 1.52, 1.62, 0.6 ] . z = np.dot(np.array(W_1),W.T.to_numpy()) z.round(2) . . array([ 1.42, 1.91, -1.33, 1.52, 1.62, 0.6 ]) . 5. $ hat y$ 계산 . $ hat y=softmax(Z) = [0.18, , ,0.30, , ,0.01, , ,0.20, , ,0.22, , ,0.08]$ . from scipy.special import softmax y=[0,0,1,0,0,0] so = pd.DataFrame({&quot;y_hat&quot;: softmax(z), &quot;y&quot; : y},index=index) so . . y_hat y . 금요일 0.182270 | 0 | . 밤 0.299486 | 0 | . 프라닭 0.011647 | 1 | . 불금 0.202155 | 0 | . 교촌치킨 0.224158 | 0 | . 맥주 0.080284 | 0 | . &#44032;&#51473;&#52824; &#54665;&#47148; &#44081;&#49888; . 위와 같은 과정을 모든 단어에 대해 수행하여 크로스 엔트로피 함수를 적용한 $loss$를 계산한다. . 1. $loss=- sum_{i=1}^6 y_i log p_i$ . 2.$loss$를 최소화 하는 최적의 파라미터 $ theta$를 구함 $ to theta= frac { partial loss}{ partial p} $ . 3. example $W_{밤},W_{불금}$ 업데이트($ alpha $ : learning rate) . $W_{밤}^{t+1} = W_{밤}^t + left( , alpha , times theta , right)$ . $W_{불금}^{t+1} = W_{불금}^t + left( , alpha , times theta , right)$ . Summary . 중심단어 벡터 $W_c$가 있고, 주변 단어 벡터 $W_o$가 있다고 하자. | . $t+1$ 시점에서 $t$ 시점의 결과를 반영하여 단어벡터 $W_{o}$를 갱신한다. | . $W^{t+1}_{o} =W^{t}_{o} + alpha times [l( theta_{c})]$ | . 타겟단어 예측시 사용되는 수식 [4] | . $$P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{w=1}^W exp ,({v^{ prime}_w}^Tv_{w_{I}}) } $$ . $$I , ,: , ,Input, , ,O , ,: , , Output$$ . $$W : number , , , of , , , Word$$ . Skip-gram . Skip-gram의 경우 CBOW와 달리 중심단어를 가지고 주변단어를 예측하는 과정이다. | . 따라서 CBOW의 3번째 단계 window-size내의 주변 단어들의 합을 평균 내는 과정이 생략된다. | .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2022/02/22/%EC%97%B0%EA%B5%AC%EB%B0%9C%ED%91%9C.html",
            "relUrl": "/python/2022/02/22/%EC%97%B0%EA%B5%AC%EB%B0%9C%ED%91%9C.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Word Embedding",
            "content": "WordEmbedding . 1. 차원의 저주(Curse of Dimensionality) : 수학적 공간 차원(=변수 개수)이 늘어나면서, 문제 계산법이 지수적으로 커지는 상황 . 만약 $x=[1,2,3,4,5], , y= [0,0,0,0,0] to (X,Y)$ 을 표현한다고 하자 . 아래와 같이 1차원 상에서 표현되는 정보를 2차원 상에서 표현하게되어 설명 공간이 $5^2 =25$가 된 것이다. . 이러한 경우를 차원의 저주라고 하며 이는 모델링 과정에서 저장 공간과 처리 시간이 불필요하게 증가됨을 의미한다. . 이러한 문제점은 위와 같은 $(X,Y)$이산형 확률분포에서 결합분포를 구할 때 발생한다. . 또한 이산형 변수들이 다양한 값$(0,1,2 dots 145748)$을 가질 경우 같은 길이의 두 문자열에서 거리를 측정하는 척도인 &quot;해밍 거리&quot;의 값이 거의 최댓값에 이르게 된다. [1] . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . . fig, axes = plt.subplots(1,2, figsize=(10,5)) ax1, ax2 =axes x = [1,2,3,4,5] y = [0,0,0,0,0] ax1.plot(x,y) ax1.set_title(&quot;1-dimensional&quot;) ax1.axis(&quot;off&quot;) ax2.plot(x,y) ax2.set_title(&quot;2-dimensional&quot;) fig.tight_layout() . . 이러한 문제점을 해결하기 위해 NLP 분야에서는 단어를 저차원에 표현하기 위한 &quot;워드 임베딩(Word Embedding)&quot;을 제안하였다. . 기존의 통계적인 방법이 단어의 출현 빈도에 집중 한다면 워드 임베딩은 서로 유사한 단어들 간 유사성을 포착하는데 집중한다. . - 가정 : 유사한 의미를 가진 단어는 유사한 문맥안에서 발견된다. . - 가정의 해석 : 유사한 의미를 가진 단어들은 유사한 단어 벡터를 가진다. . - 이점 : 이웃된 단어들의 단어 벡터들을 학습하여 단어간 유사성을 도출해낼 수 있다. . example 1 :&#39;man&#39; + &#39;royal&#39; = &#39;king&#39; . 이제 이러한 임베딩 기법 중 하나인 &quot;Word2Vec&quot; 기법을 소개한다. [2] . Word2Vec . 워드 &quot;Word2Vec&quot; 중 가장 대표적인 방법으로 &quot;CBOW&quot;, &quot;skip-gram&quot; 이 존재한다. | . CBOW . 주변 단어를 이용하여 중심단어를 예측한다. | . 주어진 문맥에서 window size $k$ 를 적용해 target word 양옆에 $k$개의 단어들을 이용하여 조건부 확률을 계산한다. (편의상 k=1 이라고 설정) | . 프라닭 이라는 단어를 예측한다고 가정한다. | . 문장1: 금요일 밤에 프라닭은 못참지 . 문장2: 불금인데 교촌치킨 에 맥주? . 단어 : [ &quot;금요일&quot;, &quot;밤&quot;, &quot;프라닭&quot;,&quot;불금&quot;, &quot;교촌치킨&quot;, &quot;맥주&quot; ] $ to Word in R^{6 times 6}$ . 문장의 개수는 $j=2$, 단어의 개수는 총 $i=6$, 축소할 임베딩 차원의 개수는 $n=3$ 으로 설정하자. | . 차원축소를 위해 생성되는 임베딩(=가중치) 행렬 $W in R^{6 times 3}$ 으로 파이토치 기준 $N(0,1)$에서 생성된다. | . 목적 1 : $Word in R^{6 times 6} to W in R^{6 times 3}$ | . 목적 2 : 단어간 의미적 유사성을 포착하기 위한 임베딩 행렬 갱신 $W^{t} to W^{t+1} $ | . 1. one-hot vector $Word in R^{6 times 6}$ 생성 . import numpy as np import pandas as pd index = [ &quot;금요일&quot;, &quot;밤&quot;, &quot;프라닭&quot;,&quot;불금&quot;, &quot;교촌치킨&quot;, &quot;맥주&quot; ] word1 = [1,0,0,0,0,0] word2 = [0,1,0,0,0,0] word3 = [0,0,1,0,0,0] word4 = [0,0,0,1,0,0] word5 = [0,0,0,0,1,0] word6 = [0,0,0,0,0,1] one_hot = pd.DataFrame([word1,word2,word3,word4,word5,word6],index=index) one_hot . . 0 1 2 3 4 5 . 금요일 1 | 0 | 0 | 0 | 0 | 0 | . 밤 0 | 1 | 0 | 0 | 0 | 0 | . 프라닭 0 | 0 | 1 | 0 | 0 | 0 | . 불금 0 | 0 | 0 | 1 | 0 | 0 | . 교촌치킨 0 | 0 | 0 | 0 | 1 | 0 | . 맥주 0 | 0 | 0 | 0 | 0 | 1 | . 2. 임베딩(가중치) 행렬 생성 $W in R^{6 times 3}$ . W = np.random.normal(loc = 0, scale=1,size=18).reshape(6,3) W = pd.DataFrame(W,index=index, columns = [&quot;W1&quot;,&quot;W2&quot;,&quot;W3&quot;]) W . . W1 W2 W3 . 금요일 -0.946677 | -0.964799 | -2.236172 | . 밤 1.481341 | 0.678401 | -1.239748 | . 프라닭 -0.855941 | 0.556102 | 0.330505 | . 불금 0.316146 | -1.791996 | -0.307091 | . 교촌치킨 1.289018 | -1.415381 | 0.418707 | . 맥주 1.106920 | 0.051748 | 0.478479 | . 3. $ widehat W_{프라닭} = frac {W_{밤} + W_{불금}} {2} = [0.89,-0.55,0.77]$ . W_1 = list((W.loc[&quot;밤&quot;] + W.loc[&quot;불금&quot;])/2) W_1 . . [0.8987433793584608, -0.5567973759616802, -0.7734197905021276] . 4. $ Z = widehat W_{프라닭} times W^T = $ [ 1.42, 1.91, -1.33, 1.52, 1.62, 0.6 ] . z = np.dot(np.array(W_1),W.T.to_numpy()) z.round(2) . . array([ 1.42, 1.91, -1.33, 1.52, 1.62, 0.6 ]) . 5. $ hat y$ 계산 . $ hat y=softmax(Z) = [0.18, , ,0.30, , ,0.01, , ,0.20, , ,0.22, , ,0.08]$ . from scipy.special import softmax y=[0,0,1,0,0,0] so = pd.DataFrame({&quot;y_hat&quot;: softmax(z), &quot;y&quot; : y},index=index) so . . y_hat y . 금요일 0.182270 | 0 | . 밤 0.299486 | 0 | . 프라닭 0.011647 | 1 | . 불금 0.202155 | 0 | . 교촌치킨 0.224158 | 0 | . 맥주 0.080284 | 0 | . &#44032;&#51473;&#52824; &#54665;&#47148; &#44081;&#49888; . 위와 같은 과정을 모든 단어에 대해 수행하여 크로스 엔트로피 함수를 적용한 $loss$를 계산한다. . 1. $loss=- sum_{i=1}^6 y_i log p_i$ . 2.$loss$를 최소화 하는 최적의 파라미터 $ theta$를 구함 $ to frac { partial loss}{ partial p} $ . 3. example $W_{밤},W_{불금}$ 업데이트($ alpha $ : learning rate) . $W_{밤}^{t+1} = W_{밤}^t + left( , alpha , times theta , right)$ . $W_{불금}^{t+1} = W_{불금}^t + left( , alpha , times theta , right)$ . Summary . 중심단어 벡터 $W_c$가 있고, 주변 단어 벡터 $W_o$가 있다고 하자. | . $t+1$ 시점에서 $t$ 시점의 결과를 반영하여 단어벡터 $W_{o}$를 갱신한다. | . $W^{t+1}_{o} =W^{t}_{o} + alpha times [l( theta_{c})]$ | . 타겟단어 예측시 사용되는 수식 [4] | . $$P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{w=1}^W exp ,({v^{ prime}_w}^Tv_{w_{I}}) } $$ . $$I , ,: , ,Input, , ,O , ,: , , Output$$ . $$W : number , , , of , , , Word$$ . Skip-gram . Skip-gram의 경우 CBOW와 달리 중심단어를 가지고 주변단어를 예측하는 과정이다. | . 따라서 CBOW의 3번째 단계 window-size내의 주변 단어들의 합을 평균 내는 과정이 생략된다. | . 이러한 부분을 빼면 CBOW와 동일하다. | . Skip-gram과 CBOW의 경우 아래와 같은 수식을 최대화 하는 것을 목표로 한다. (베르누이분포의 MLE 를 생각해보장) [4] | . 베르누이분포의 MLE와 크로스 엔트로피 손실함수의 최소값 파라미터를 구하는 것이 동치라고 생각해보자.(이 부분 다시 증명) | . $$ frac 1T sum_{t=1}^T sum_{-c leq j leq c, j neq 0} y_{t+j} log p left(w_{t+j}|w_t right), quad[4]$$ . $$T : mathrm{ number , , of , , traning , , word}, quad c : mathrm{the , , size , , of , , training , , context , , (=window)} $$ . 사실상 $y_{t+j}$ 는 예측하고자 하는 target단어가 아니면 0, 맞으면 1이므로 위식은 다시 아래와 같이 바꿔 쓸 수 있다. | . $$ frac 1T sum_{t=1}^T sum_{-c leq j leq c, j neq 0} log p left(w_{t+j}|w_t right), quad[4]$$ . Negative Sampling . $$ P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{w=1}^W exp ,({v^{ prime}_w}^Tv_{w_{I}}) }$$ . 위와 같이 Word2Vec은 출력층이 내놓는 스코어 값에 소프트맥스 함수를 적용해 확률값으로 변환한 후 이를 정답과 비교해 &quot;역전파(backpropagation)&quot; 하는 구조이다. | . 그런데 소프트맥스를 적용하려면 분모에 해당하는 값, 즉 중심(=input) 단어와 나머지 모든 단어의 내적을 한 뒤 이를 다시 $ exp$ 취해줘야한다. | . 보통 전체 단어가 10만개 이상 주어지므로 계산량이 어마어마 해진다. | . Negative Sampling은 소프트맥스 확률을 구할 때 전체 단어를 구하지 않고, 일부 단어만 뽑아서 계산을 하자는 아이디어다.[3][4] | . 사용자가 지정한 윈도우 사이즈 내에 등장하지 않는 단어(Negative saple)을 5~20개 정도 뽑고, 이를 우리가 예측하고자 하는 타겟 단어와 합쳐 전체 단어처럼 소프트맥스 확률을 구하는 것이다. | . 보통 5~20 ($k$) 개 정도 뽑는데, 이는 만약 윈도우 사이즈가 5일 경우 최대 25개의 단어를 대상으로만 소프트맥스 확률을 계산하고, 파라미터 업데이트도 25개 대상으로만 이뤄진다는 이야기이다.[3][4] | . $$ P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{i=1}^k exp ,({v^{ prime}_w}^Tv_{w_{I}}) }$$ . 특정 단어가 negative sampliing 될 확률 $P_n(w_i) $은 다음과 같다. $P_n(w_i)$는 free parameter로 선행연구에 의하면 단일 단어인 $ mathrm{unigram-distribution}$의 경우 $f(w_i)^{3/4}$일 때 뛰어난 성능을 보였다고 한다.[4] | . $$ P_n(w_i)= frac {f(w_i)^{3/4}}{ sum_{j=0}^n f(w_j)^{3/4}}$$ . $$f(w_i) = (해당 , ,단어 , ,빈도 , , / , ,전체 , , 단어 , , 수)$$ . 참고문헌 . skip-gram : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/ . [1] : Bengio, Yoshua, et al. &quot;A neural probabilistic language model.&quot; Journal of machine learning research 3.Feb (2003): 1137-1155. . [2] : Young, Tom, et al. &quot;Recent trends in deep learning based natural language processing.&quot; ieee Computational intelligenCe magazine 13.3 (2018): 55-75. . [3] : Mikolov, Tomas, et al. &quot;Efficient estimation of word representations in vector space.&quot; arXiv preprint arXiv:1301.3781 (2013). . [4] : Mikolov, Tomas, et al. &quot;Distributed representations of words and phrases and their compositionality.&quot; Advances in neural information processing systems. 2013. .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2022/01/21/Word2Vec.html",
            "relUrl": "/python/2022/01/21/Word2Vec.html",
            "date": " • Jan 21, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Chapter 2. NLP 기술 빠르게 훑어보기",
            "content": "&#47568;&#47945;&#52824;, &#53664;&#53360;, &#53440;&#51077; . &#47568;&#47945;&#52824; . 고전이나 현대의 모든 NLP 작업에서 쓰이는 text data 샘플 : metadata + text | 위 같은 샘플들이 모인 데이터 셋을 말뭉치라고 표현한다, | . | . &#53664;&#53360;&#54868; (Tokenization) . 토큰 : 문법적으로 더 이상 나눌 수 없는 언어 요소 | . 텍스트를 토큰으로 나누는 과정 | . &#53076;&#46300; 2-1 . 아래의 코드는 텍스트 처리 분야에 널리 사용되는 패키지인 spacy, NLTK 의 예시이다. | . import spacy . nlp = spacy.load(&quot;en_core_web_sm&quot;) text = &quot;Mary, don&#39;t slap the green witch&quot; print( [str(token) for token in nlp(text.lower())] ) . [&#39;mary&#39;, &#39;,&#39;, &#39;do&#39;, &#34;n&#39;t&#34;, &#39;slap&#39;, &#39;the&#39;, &#39;green&#39;, &#39;witch&#39;] . from nltk.tokenize import TweetTokenizer . tweet = u&quot;Snow White and the Seven Degrees #MakeaMoviecold@midnight:-)&quot; . tokenizer = TweetTokenizer() . spacy 와 nltk 비교 | . print( tokenizer.tokenize(tweet.lower())) . [&#39;snow&#39;, &#39;white&#39;, &#39;and&#39;, &#39;the&#39;, &#39;seven&#39;, &#39;degrees&#39;, &#39;#makeamoviecold&#39;, &#39;@midnight&#39;, &#39;:-)&#39;] . print( [str(token) for token in nlp(tweet.lower())] ) . [&#39;snow&#39;, &#39;white&#39;, &#39;and&#39;, &#39;the&#39;, &#39;seven&#39;, &#39;degrees&#39;, &#39;#&#39;, &#39;makeamoviecold@midnight:-&#39;, &#39;)&#39;] . &#53440;&#51077; . 말뭉치에 등장하는 고유한 토큰 | . 말뭉치에 있는 모든 타입의 집합이 어휘 사전 또는 어휘(lexicon)이다. | . $ divideontimes$ 특성 공학 : 언어학을 이해하고 NLP 문제 해결에 적용하는 과정 . N-gram . 텍스트에 있는 고정 길이 (n)의 연속된 토큰 시퀀스이다. | . &#53076;&#46300; 2-2 . def n_gram(text,n) : return [ text [i:i+n] for i in range(len(text)-n+1)] . cleaned = [&quot;marry&quot;, &quot;,&quot;,&quot;n&#39;t&quot;, &quot;slap&quot;, &quot;green&quot;, &quot;witch&quot;,&quot;,&quot;] . print(n_gram(cleaned,3)) . [[&#39;marry&#39;, &#39;,&#39;, &#34;n&#39;t&#34;], [&#39;,&#39;, &#34;n&#39;t&#34;, &#39;slap&#39;], [&#34;n&#39;t&#34;, &#39;slap&#39;, &#39;green&#39;], [&#39;slap&#39;, &#39;green&#39;, &#39;witch&#39;], [&#39;green&#39;, &#39;witch&#39;, &#39;,&#39;]] . 부분 단어 자체가 유용한 정보를 전달한다면 문자 $n-gram$을 생성할 수 있음 | . example :methanol의 methan 탄소 화합물, 접미사 -ol 은 알코올 종류를 나타낸다. 이 같은 경우 2-gram 으로 생각할 수 있지만 유기 화합물 이름을 구분하는 작업에서는 토큰 하나로 취급할 수 있다. . &#54364;&#51228;&#50612;&#50752; &#50612;&#44036; . &#54364;&#51228;&#50612; . 표제어 : 단어의 기본형 또는 사전에 등재된 단어 fly : flow, flew, flies, flown, flowing 의 표제어 | 토큰을 표제어로 바꾸어 벡터 표현의 차원을 줄이는 방법도 종종 도움이 된다. | . | . spacy는 사전에 정의된 WordNet 사전을 이용해 표제어를 추출한다. | . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) doc = nlp(&quot;he was running late&quot;) for token in doc : print (&quot;{} --&gt; {}&quot;.format(token, token.lemma_)) . he --&gt; -PRON- was --&gt; be running --&gt; run late --&gt; late . &#50612;&#44036;(Stemming) . 어형변화의 기초가 되는 부분 | . Porter와 Snowball어간 추출기가 유명하다. | . &#53076;&#46300; 2-3 . import nltk from nltk.stem.porter import * . stemmer = PorterStemmer() tokens = [&#39;compute&#39;, &#39;computer&#39;, &#39;computed&#39;, &#39;computing&#39;] for token in tokens: print(token + &#39; --&gt; &#39; + stemmer.stem(token)) . compute --&gt; comput computer --&gt; comput computed --&gt; comput computing --&gt; comput . &#54408;&#49324; &#53468;&#44613; . &#53076;&#46300; 2-4 . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) doc = nlp(u&quot;Mary, don&#39;t slap the green witch&quot;) for token in doc : print (&quot;{} --&gt; {}&quot;.format(token, token.pos_)) . Mary --&gt; PROPN , --&gt; PUNCT do --&gt; VERB n&#39;t --&gt; ADV slap --&gt; VERB the --&gt; DET green --&gt; ADJ witch --&gt; NOUN . &#52397;&#53356;&#45208;&#45572;&#44592; = &#48512;&#48516; &#44396;&#47928; &#48516;&#49437; . Chunking, shallow parsing | . 청크 : 하나의 의미가 있는 말 덩어리 (여러 토큰들이 모여 청크가 될 수 있다고 생각하자) | . 연속된 여러 토큰으로 구분되는 텍스트 구에 레이블을 할당하는 작업 | . 다음은 명사구(NP) 부분 구문 분석결과이다. | . &#53076;&#46300; 2-5 . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) doc = nlp(u&quot;Marry slapped the green witch.&quot;) for chunk in doc.noun_chunks : print (&quot;{} --&gt; {}&quot;.format(chunk, chunk.label_)) . Marry --&gt; NP the green witch --&gt; NP .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2021/01/04/Chapter-2.-NLP-%EA%B8%B0%EC%88%A0-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%ED%9B%91%EC%96%B4%EB%B3%B4%EA%B8%B0.html",
            "relUrl": "/python/2021/01/04/Chapter-2.-NLP-%EA%B8%B0%EC%88%A0-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%ED%9B%91%EC%96%B4%EB%B3%B4%EA%B8%B0.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Chapter 1. Introduction",
            "content": "NLP . Natrual Language Processing | . 언어학 지식에 상관없이 텍스트를 &quot;이해&quot; 하는 통계적인 방법을 사용해 실전 문제를 해결하는 일련의 기술 | . 텍스트 마이닝과의 차이 비교가 현재는 거의 무의미 하지만 아래와 같은 관심 중점에 차이가있다 | . 1 텍스트 마이닝 : 텍스트 데이터로 부터 유의미한 정보를 추출 . 2 NLP : 기계가 자연어를 이해할 수 있고 처리할 수 있도록 하는일을 의미한다. . 여기서 이해라는 것은 주로 텍스트를 계산 가능한 &quot;표현(representation)&quot; 으로 변환함으로써 이루어짐 | . 표현 : 벡터, 텐서, 그래프, 트리 같이 이산적이거나 연속적으로 조합한 구조이다. | . 이 책에서는 딥러닝과 NLP을 배운다. | . Deep learning . 개념 : 계산 그래프와 수치 최적화 기술을 사용해 데이터에서 표현을 효과적으로 학습하는 기술 | . Supervised Learning . 목적 : 주어진 데이터 셋에서 손실 함수를 최소화하는 파라미터 값을 고르는 것 | . 손실함수 : $L( hat y, ,y)$ | . &#44221;&#49324;&#54616;&#44053;&#48277; . Gradient descent : 식의 근을 찾는 일반적인 방법 | . &#51204;&#53685;&#51201;&#51064; &#44221;&#49324; &#54616;&#44053;&#48277; . = batct gradient descent | . 오차를 구할 때 전체 데이터 셋을 고려 $ to$ 한 번에 에포크에서 모든 매개변수에 업데이트를 단 한번 수행 | . 파라미터의 초깃값을 추측한 다음 목적 함수(= 손실 함수)의 값이 수용할만한 임계점(수렴 조건) 아래로 내려갈 때 까지 파라미터를 반복해서 업데이트를 한다. | . 데이터 셋이 클 경우 메모리 제약 $ to$ 계산 비용이 높아 매우 느림 | . &#54869;&#47456;&#51201; &#44221;&#49324;&#54616;&#44053;&#48277; . Stochastic gradient descent(SGD) | . 데이터 포인트를 하나 또는 일부 랜덤하게 선택하여 그래디언트를 계산한다. | . 데이터 포인트를 하나 사용하는 방법은 순수 SGD, 두 개 이상 사용하는 방법은 미니배치 SGD 라고 부른다. | . 순수 SGD : 업데이트에 잡음이 많아 수렴이 매우 느려 실전에서는 거의 사용하지 않음 | . &#50669;&#51204;&#54028; . Backpropagation | . 파라미터를 반복적으로 업데이트 하는 과정 | . 단계 = 정방향 계산 (forward pass) + 역방향 계산 (backward pass) | . 정방향 계산 : 현재 파라미터값으로 입력을 받아 평가하여 손실함수를 계산 | . 역방향 계산 : 손실의 그래이디언트를 사용하여 파라미터를 업데이트 함. | . &#50896;-&#54635; &#54364;&#54788; . &#49324;&#51060;&#53431;&#47088;&#51012; &#49324;&#50857;&#54616;&#50668; &#50896;-&#54635; &#48289;&#53552; &#46608;&#45716; &#51060;&#51652; &#54364;&#54788; &#47564;&#46308;&#44592; . from sklearn.feature_extraction.text import CountVectorizer import seaborn as sns import matplotlib.pyplot as plt . corpus = [&quot;Time flies like an arrow.&quot;,&quot;Fruit flies like a banana&quot;] . one_hot_vectorizer = CountVectorizer(binary=True) one_hot = one_hot_vectorizer.fit_transform(corpus).toarray() . vocab = one_hot_vectorizer.get_feature_names() vocab . [&#39;an&#39;, &#39;arrow&#39;, &#39;banana&#39;, &#39;flies&#39;, &#39;fruit&#39;, &#39;like&#39;, &#39;time&#39;] . sns.heatmap(one_hot, annot = True, cbar = False, xticklabels = vocab, yticklabels = [&quot;Sentence1&quot;, &quot;Setence2&quot;]) . &lt;AxesSubplot:&gt; . TF-IDF &#54364;&#54788; . $$TF(w) = Frequency , , of , ,word$$ . $$IDF(w) = log frac {N}{n_w}$$ . $$N : total , document quad n_w : number ,of , documnet , that , include ,word $$ . $TF$ : $i$번째 문서에서 $j$ 번째 단어의 출현 횟수 | . $IDF$ : 역문서 빈도, 단어 $w$가 출현한 역문서 빈도 | . 사이킷런의 TfidfVectorizer 클래스에서는 단어를 모두 가상의 문서가 있는 것처럼 로그 안의 분모와 분자에 1을 더해서 분모가 0이 되는 상활을 방지함 또한 마지막에 1을 더해 모든 문서에 포함된 단어가 있을 때 IDF 가 0이 되지 않도록함 | . | . $$IDF(w) = log left( frac {N+1}{n_w+1} right ) + 1$$ . 만약 특허와 관련된 문서 묶음이 있다고 가정해보자. 그러면 문서 대부분에 [&quot;Claim&quot;, &quot;System&quot;, &quot;method&quot;] 와 같은 단어가 여러번 반복해서 나온다. | . 이런 흔한 단어는 사실 특정 특허와 관련한 어떤 정보도 담겨있지 않는다. | . 반대로 &quot;tetrafluoroethylene&quot;(테트라플루오로에틸랜) 과 같이 희귀한 단어는 자주 나오지 않지만 특허 문서의 특징을 잘나타냄 | . $TF(w) times IDF(w)$이다. 즉 모든 문서에 등장하는 매우 흔한 단어는 TF-IDF 점수가 0이된다. | . 이처럼 아무런 정보없이 무분별하게 여러번 등장하는 단어들에 패널티를 주기위한 기법이다. | . &#49324;&#51060;&#53431;&#47088;&#51012; &#49324;&#50857;&#54644; TF-IDF &#54364;&#54788; &#47564;&#46308;&#44592; . from sklearn.feature_extraction.text import TfidfVectorizer import seaborn as sns . tfidf_vectorizer = TfidfVectorizer() tfidf =tfidf_vectorizer.fit_transform(corpus).toarray() tfidf . array([[0.49922133, 0.49922133, 0. , 0.35520009, 0. , 0.35520009, 0.49922133], [0. , 0. , 0.57615236, 0.40993715, 0.57615236, 0.40993715, 0. ]]) . sns.heatmap(tfidf,annot=True,xticklabels=vocab,cbar=False, yticklabels=[&quot;Sentence 1&quot;, &quot;Sentence 2&quot;]) . &lt;AxesSubplot:&gt; . &#54028;&#51060;&#53664;&#52824; &#44592;&#52488; . 오픈 소스로 씨아노, 카페, 텐서플로와 달리 파이토치는 테이프 기반 자동 미분 방식을 구현한다. | . 위 방식은 계산 그래프를 동적으로 정의하고 실행할 수 있다. 또한 디버깅이 아주 편리하며 복잡한 모델을 손쉽게 만들게 해줌 | . 텐서 : 다차원 데이터를 담은 수학객체 | . &#53584;&#49436; &#47564;&#46308;&#44592; . 먼저 헬퍼 함수 describe(x)를 정의하여 텐서 타입, 차원, 값 같은 텐서의 속성을 출력 | . def describe(x) : print(&quot;타입 : {}&quot;.format(x.type())) print(&quot;크기 : {}&quot;.format(x.shape)) print(&quot;값 : n{} n n n================================================ n n&quot;.format(x)) . import torch . 차원을 지정 후 텐서를 랜덤하게 초기화하는 방법 | . describe(torch.Tensor(2,3)) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0.2582, 0.4032, 0.1510], [0.8588, 0.2966, 0.9982]]) ================================================ . &#44512;&#46321;&#48516;&#54252;&#50752; &#54364;&#51456;&#51221;&#44508;&#48516;&#54252; . describe(torch.rand(2,3)) ## 균등 분포 describe(torch.randn(2,3)) ## 표준 정규 분포 . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0.6322, 0.4141, 0.1253], [0.5149, 0.2065, 0.5259]]) ================================================ 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[-0.5282, -0.1808, 1.0561], [ 0.6534, -0.0688, 1.0356]]) ================================================ . &#46041;&#51068;&#54620; &#49828;&#52860;&#46972;&#44050;&#51004;&#47196; &#52292;&#50868; &#53584;&#49436;&#47484; &#47564;&#46308;&#44592; . 내장함수로 0또는 1로 채운 텐서 생성 | . describe(torch.zeros(2,3)) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0., 0., 0.], [0., 0., 0.]]) ================================================ . x = torch.ones(2,3) describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 1., 1.], [1., 1., 1.]]) ================================================ . &#51064;-&#54540;&#47112;&#51060;&#49828; &#47700;&#49436;&#46300; . 인-플레이스 메서드 : 데이터를 직접 변경하는 것 | . 아래와 같이 fill_() 메서드와 비슷하게 랜덤 샘플링에 유용한 인-플레이스 메서드가 잇다. | . x.fill_(5) . tensor([[5., 5., 5.], [5., 5., 5.]]) . describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[5., 5., 5.], [5., 5., 5.]]) ================================================ . 새로운 객체를 생성하지 않고 바로 값을 변환하는데 객체 지향적인 관점에서 별로 좋지는 않은 것 같다. | . y = torch.Tensor(3,2) . y . tensor([[4.4155e-05, 2.1259e+20], [8.1714e+20, 6.7875e-07], [2.5668e-09, 4.1537e-08]]) . y.uniform_() . tensor([[0.7345, 0.1133], [0.0873, 0.7492], [0.3604, 0.2273]]) . y . tensor([[0.7345, 0.1133], [0.0873, 0.7492], [0.3604, 0.2273]]) . y.normal_() . tensor([[ 0.3838, -0.6120], [-1.3810, 0.8169], [-0.6775, -1.2050]]) . y . tensor([[ 0.3838, -0.6120], [-1.3810, 0.8169], [-0.6775, -1.2050]]) . &#47532;&#49828;&#53944;&#47196; &#53584;&#49436;&#47484; &#47564;&#46308;&#44256; &#52488;&#44592;&#54868; . x = torch.Tensor([[1,2,3], [4,5,6]]) . describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 2., 3.], [4., 5., 6.]]) ================================================ . 위 처럼 값을 리스트나. 넘파이 배열로 전달하여 텐서를 생성하고 초기화가 가능하다. | . 또한 언제든지 파이토치 텐서를 넘파이 배열로 바꿀 수 있음 | . 단 넘파이 배열을 사용하면 텐서 타입이, FloatTensor가 아니라 DoubleTensor가 된다. | . import numpy as np . npy = np.random.rand(2,3) . describe(torch.from_numpy(npy)) . 타입 : torch.DoubleTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0.9967, 0.8784, 0.1933], [0.6447, 0.7438, 0.1425]], dtype=torch.float64) ================================================ . 넘파이 배열과 파이토치텐서 사이를 변환하는 기능은 넘파이 포맷의 수치데이터를사용하는 래거시(legacy) 라이브러리를 사용할 때 중요하다. | . &#53440;&#51077;&#51012; &#52488;&#44592;&#54868; &#54616;&#50668; &#53584;&#49436; &#49373;&#49457; . x = torch.FloatTensor([[1,2,3], [4,5,6]]) describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 2., 3.], [4., 5., 6.]]) ================================================ . x = x.long() describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1, 2, 3], [4, 5, 6]]) ================================================ . x = torch.tensor([[1,2,3], [4,5,6]], dtype=torch.int64) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1, 2, 3], [4, 5, 6]]) ================================================ . x = x.float() describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 2., 3.], [4., 5., 6.]]) ================================================ . &#53584;&#49436; &#50672;&#49328; . &#45927;&#49480; . import torch . x = torch.randn(2,3) describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[ 0.0715, -0.2960, -0.5417], [ 0.3304, 1.5357, -1.1040]]) ================================================ . describe(torch.add(x,x)) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[ 0.1431, -0.5919, -1.0834], [ 0.6609, 3.0715, -2.2080]]) ================================================ . x = torch.arange(6) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([6]) 값 : tensor([0, 1, 2, 3, 4, 5]) ================================================ . view() 메서드는 동일한 데이터를 공유하는 새로운 텐서를 만든다. data_ptr() 매서드를 사용하면 원본 텐서와 뷰 텐서가 같은 저장 위치를 가리키고 있다는 것을 확인할 수 있다. | . x1 = x.view(2,3) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5]]) ================================================ . x.data_ptr() . 2251512245888 . x1.data_ptr() . 2251512245888 . 0 으로 지정시 colsum 반환, 1로 지정시 rowsum을 반환 | . describe(torch.sum(x,dim=0)) . 타입 : torch.LongTensor 크기 : torch.Size([3]) 값 : tensor([3, 5, 7]) ================================================ . describe(torch.sum(x,dim=1)) . 타입 : torch.LongTensor 크기 : torch.Size([2]) 값 : tensor([ 3, 12]) ================================================ . transpose함수는 두 번째와 세 번째 매개변수로 전달된 차원을 전치한 텐서를 만든다. | . x . tensor([[0, 1, 2], [3, 4, 5]]) . describe(torch.transpose(x,0,1)) . 타입 : torch.LongTensor 크기 : torch.Size([3, 2]) 값 : tensor([[0, 3], [1, 4], [2, 5]]) ================================================ . &#51064;&#45937;&#49905;, &#49836;&#46972;&#51060;&#49905;, &#50672;&#44208; . &#53584;&#49436; &#49836;&#46972;&#51060;&#49905;&#44284; &#51064;&#45937;&#49905; . import torch x = torch.arange(6).view(2,3) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5]]) ================================================ . describe(x[:1,:2]) . 타입 : torch.LongTensor 크기 : torch.Size([1, 2]) 값 : tensor([[0, 1]]) ================================================ . describe(x[0,1]) . 타입 : torch.LongTensor 크기 : torch.Size([]) 값 : 1 ================================================ . &#48373;&#51105;&#54620; &#51064;&#45937;&#49905;, &#50672;&#49549;&#51201;&#51060;&#51648; &#50506;&#51008; &#53584;&#49436; &#51064;&#45937;&#49828; &#52280;&#51312;&#54616;&#44592; . x . tensor([[0, 1, 2], [3, 4, 5]]) . 아래 코드를 해석하면 컬럼차원에서 0,2 번째 컬럼을 반환한다는 뜻이다. | . indices = torch.LongTensor([0,2]) describe(torch.index_select(x,dim=1,index = indices)) . 타입 : torch.LongTensor 크기 : torch.Size([2, 2]) 값 : tensor([[0, 2], [3, 5]]) ================================================ . indices = torch.LongTensor([0,0]) describe(torch.index_select(x,dim=0,index = indices)) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [0, 1, 2]]) ================================================ . 인덱스가 LongTensor 라는 점에 주목하자. 파이토치 함수를 사용할 때 필수 조건이다. | . x . tensor([[0, 1, 2], [3, 4, 5]]) . row_indices = torch.arange(2).long() ## 로우나 컬럼이나 같은 표현이다. col_indices = torch.LongTensor([0,1]) describe(x[row_indices,col_indices]) . 타입 : torch.LongTensor 크기 : torch.Size([2]) 값 : tensor([0, 4]) ================================================ . &#53584;&#49436; &#50672;&#44208; . import torch . x = torch.arange(6).view(2,3) . describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5]]) ================================================ . cat, dim=0 $ to$ r의 cbind | . describe(torch.cat([x,x],dim=0)) ## . 타입 : torch.LongTensor 크기 : torch.Size([4, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5], [0, 1, 2], [3, 4, 5]]) ================================================ . describe(torch.cat([x,x],dim=1)) ## . 타입 : torch.LongTensor 크기 : torch.Size([2, 6]) 값 : tensor([[0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5]]) ================================================ . describe(torch.stack([x,x])) ## . 타입 : torch.LongTensor 크기 : torch.Size([2, 2, 3]) 값 : tensor([[[0, 1, 2], [3, 4, 5]], [[0, 1, 2], [3, 4, 5]]]) ================================================ . &#53584;&#49436;&#51032; &#49440;&#54805; &#45824;&#49688; &#44228;&#49328; : &#54665;&#47148; &#44273;&#49480; . import torch . x1 = torch.arange(6.).view(2,3) describe(x1) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0., 1., 2.], [3., 4., 5.]]) ================================================ . x2 = torch.ones(3,2) x2 . tensor([[1., 1.], [1., 1.], [1., 1.]]) . x2 [:,1] +=1 . describe(x2) . 타입 : torch.FloatTensor 크기 : torch.Size([3, 2]) 값 : tensor([[1., 2.], [1., 2.], [1., 2.]]) ================================================ . 행렬 곱셈 | . torch.mm(x1,x2) . tensor([[ 3., 6.], [12., 24.]]) . 역행렬 | . torch.inverse(torch.rand(2,2)) . tensor([[ 1.1645, -2.5808], [-0.0280, 3.1795]]) . 대각합 | . torch.trace(torch.rand(2,2)) . tensor(0.8843) . &#53584;&#49436;&#50752; &#44228;&#49328;&#44536;&#47000;&#54532; . 그래이디언트 연산을 할 수 있는 텐서 만들기 | . requires_grad = True는 그레이디언트 기반 학습에 필요한 손실함수와 텐서의 그레이디언트를 기록하는 부가 연산을 활성화 시킨다. . import torch . x = torch.ones(2,2, requires_grad=True) . describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 2]) 값 : tensor([[1., 1.], [1., 1.]], requires_grad=True) ================================================ . print(x.grad is None) . True . $$ y = (x+2)(x+5) +3$$ . y = (x+2)*(x+5) + 3 describe(y) print(x.grad is None) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 2]) 값 : tensor([[21., 21.], [21., 21.]], grad_fn=&lt;AddBackward0&gt;) ================================================ True . z = y.mean() describe(z) z.backward() print(x.grad is None) . 타입 : torch.FloatTensor 크기 : torch.Size([]) 값 : 21.0 ================================================ False . 파이토치에서 계산 그래프에 있는 노드에 대한 그레이디언트를 .grad 속성으로 참조할 수 있다. 옵티마이저는 .grad 속성을 사용해서 파라미터 값을 업데이트한다. | . &#50672;&#49845;&#47928;&#51228; . 1&#48264; . 2D 텐서를 만들고 차원 0 위치에 크기가 1인 차원을 추가하시오. . Solution . a = torch.rand(3,3) a . tensor([[0.2966, 0.4459, 0.0016], [0.2637, 0.7056, 0.2209], [0.2585, 0.7810, 0.2748]]) . a.unsqueeze(0) . tensor([[[0.2966, 0.4459, 0.0016], [0.2637, 0.7056, 0.2209], [0.2585, 0.7810, 0.2748]]]) . 2&#48264; . 이전 텐서에 추가한 차원을 삭제하기 . Solution . a.squeeze(0) . tensor([[0.2966, 0.4459, 0.0016], [0.2637, 0.7056, 0.2209], [0.2585, 0.7810, 0.2748]]) . 3&#48264; . 범위가 [3,7) 이고 크기가 $5 times 3$인 램덤판 텐서를 만들기 | . Solution . 3+torch.rand(5,3)*(7-3) . tensor([[4.8969, 5.3626, 4.2073], [5.1916, 6.7691, 3.1864], [6.1773, 6.1206, 5.6650], [4.0778, 6.3007, 6.8788], [5.8507, 4.0870, 5.9019]]) . 4&#48264; . Solution . 정규분포를 사용해서 텐서를 만들기 . a= torch.rand(3,3) a.normal_() . tensor([[ 0.3830, -0.2555, -0.2903], [ 1.0973, 0.3475, -0.5337], [-0.2465, 0.9446, -0.4641]]) . 5&#48264; . 텐서 torch.Tensor([1,1,1,0,1]) 에서 0이 아닌 원소의 인덱스를 추출하세요. . Solution . a = torch.Tensor([1,1,1,0,1]) torch.nonzero(a,as_tuple=True) . (tensor([0, 1, 2, 4]),) . as_tuple : True 로 지정하지 않으면 2차원 텐서를 변환한다. | . 6&#48264; . 크기가 (3,1)인 랜덤한 텐서를 만들고 네 벌을 복사해 쌓으세요 . Solution . a = torch.rand(3,1) a . tensor([[0.1492], [0.1221], [0.5768]]) . a.expand(3,4) . tensor([[0.1492, 0.1492, 0.1492, 0.1492], [0.1221, 0.1221, 0.1221, 0.1221], [0.5768, 0.5768, 0.5768, 0.5768]]) . 7&#48264; . 2차원 행렬 개 ( a = torch.rand(3,4,5), b = torch.rand(3,5,4) ) 의 배치 행렬 곱셈을 계산하세요. . Solution . a = torch.rand(3,4,5) b = torch.rand(3,5,4) . a . tensor([[[0.8432, 0.3043, 0.3210, 0.9431, 0.2673], [0.7349, 0.5522, 0.5258, 0.0104, 0.3234], [0.4519, 0.3940, 0.0187, 0.3106, 0.2780], [0.4799, 0.3480, 0.6322, 0.1634, 0.7024]], [[0.7353, 0.2029, 0.0166, 0.0391, 0.7469], [0.0473, 0.0727, 0.1912, 0.2763, 0.8238], [0.8702, 0.6047, 0.2439, 0.3102, 0.2967], [0.5944, 0.9755, 0.8162, 0.3612, 0.4574]], [[0.7215, 0.0704, 0.7937, 0.7839, 0.7022], [0.3435, 0.1463, 0.8671, 0.2648, 0.7312], [0.7399, 0.3415, 0.9808, 0.7982, 0.8345], [0.1444, 0.1740, 0.1711, 0.0673, 0.3027]]]) . b . tensor([[[0.2951, 0.1355, 0.5337, 0.1931], [0.1613, 0.7947, 0.6819, 0.7209], [0.1560, 0.1124, 0.6236, 0.7558], [0.6739, 0.1252, 0.2276, 0.1586], [0.2627, 0.1673, 0.3396, 0.2907]], [[0.6307, 0.3764, 0.1830, 0.0020], [0.3913, 0.3344, 0.9843, 0.3801], [0.7788, 0.5737, 0.5411, 0.5289], [0.7318, 0.4864, 0.8927, 0.6358], [0.8435, 0.0820, 0.3751, 0.3977]], [[0.5797, 0.1567, 0.3288, 0.7492], [0.0626, 0.7531, 0.8028, 0.4698], [0.3778, 0.8844, 0.9128, 0.1916], [0.9723, 0.2381, 0.1718, 0.3828], [0.0870, 0.5339, 0.6326, 0.3959]]]) . torch.bmm(a,b) . tensor([[[1.0538, 0.5550, 1.1631, 0.8521], [0.4799, 0.6529, 1.2088, 1.0330], [0.4821, 0.4619, 0.6866, 0.5155], [0.5910, 0.5506, 1.1633, 1.0515]], [[1.2148, 0.4345, 0.6584, 0.4093], [1.1042, 0.3537, 0.7393, 0.6321], [1.4528, 0.8450, 1.2747, 0.6757], [2.0424, 1.2314, 2.0047, 1.2151]], [[1.5457, 1.4296, 1.5972, 1.3038], [0.8569, 1.3843, 1.5300, 0.8831], [1.6695, 1.8762, 2.0778, 1.5386], [0.2510, 0.4826, 0.5464, 0.3683]]]) .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2021/01/03/Chapter-1.Introduction.html",
            "relUrl": "/python/2021/01/03/Chapter-1.Introduction.html",
            "date": " • Jan 3, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "github . github . soundcloud . C.I.C .",
          "url": "https://gangcheol.github.io/nlp-with-pytroch/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gangcheol.github.io/nlp-with-pytroch/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}