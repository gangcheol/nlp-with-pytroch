{
  
    
        "post0": {
            "title": "KoBert(데이터 5배수 증가)",
            "content": "&#51652;&#54665;&#49345;&#54889; . 데이터 증강을 하지 않고 모델을 적합한 결과 test auccary가 87% 가량 밖에 나오지가 않음 따라서 데이터 증강 기법을 이용하여 데이터를 증강시킨 후 모델을 훈련시켰다. | . 데이터 증강을 각각 5 배수와 10배수를 하여 모델을 적합한 결과 10 배수는 너무 과적합이고 5배수가 가장 적당한 것 같다. | . GPU &#50857;&#47049;&#54869;&#51064; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . cd /content/drive/MyDrive/Colab Notebooks . /content/drive/MyDrive/Colab Notebooks . `런타임 초기화 하고 아래코드 돌려서 gpu용량 확인하고 코드돌리자.... . import torch import gc gc.collect() torch.cuda.empty_cache() . !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi !pip install gputil !pip install psutil !pip install humanize import psutil import humanize import os import GPUtil as GPU GPUs = GPU.getGPUs() # XXX: only one GPU on Colab and isn’t guaranteed gpu = GPUs[0] def printm(): process = psutil.Process(os.getpid()) print(&quot;Gen RAM Free: &quot; + humanize.naturalsize(psutil.virtual_memory().available), &quot; | Proc size: &quot; + humanize.naturalsize(process.memory_info().rss)) print(&quot;GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB&quot;.format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal)) printm() . Collecting gputil Downloading GPUtil-1.4.0.tar.gz (5.5 kB) Building wheels for collected packages: gputil Building wheel for gputil (setup.py) ... done Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=265a58fab9769879aad95af44bc121053df9ba43ea3457390f08907ba63dc90c Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c Successfully built gputil Installing collected packages: gputil Successfully installed gputil-1.4.0 Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8) Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1) Gen RAM Free: 25.7 GB | Proc size: 1.2 GB GPU RAM Free: 16280MB | Used: 0MB | Util 0% | Total 16280MB . Colab &#54872;&#44221;&#49444;&#51221; . !pip install gluonnlp pandas tqdm !pip install mxnet !pip install sentencepiece==0.1.91 !pip install transformers==4.8.2 !pip install torch . github&#50640;&#49436; KoBERT &#54028;&#51068;&#51012; &#47196;&#46300; &#48143; KoBERT&#47784;&#45944; &#48520;&#47084;&#50724;&#44592; . !pip install &#39;git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&amp;subdirectory=kobert_hf&#39; . Collecting kobert_tokenizer Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-zbmx6a28/kobert-tokenizer_1b2b0dbf3d81413aab84e885b2c156b0 Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-zbmx6a28/kobert-tokenizer_1b2b0dbf3d81413aab84e885b2c156b0 Building wheels for collected packages: kobert-tokenizer Building wheel for kobert-tokenizer (setup.py) ... done Created wheel for kobert-tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4649 sha256=8a7cffaa4f8945bb0ada70ab31238fa1f42c00fc3b3eaa7848e4dcd72f840d49 Stored in directory: /tmp/pip-ephem-wheel-cache-fwcu0bxa/wheels/10/b4/d9/cb627bbfaefa266657b0b4e8127f7bf96d27376fa1a23897b4 Successfully built kobert-tokenizer Installing collected packages: kobert-tokenizer Successfully installed kobert-tokenizer-0.1 . !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master . Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-zgcsi_uh Running command git clone -q &#39;https://****@github.com/SKTBrain/KoBERT.git&#39; /tmp/pip-req-build-zgcsi_uh Collecting boto3 Downloading boto3-1.21.15-py3-none-any.whl (132 kB) |████████████████████████████████| 132 kB 3.2 MB/s Collecting gluonnlp&gt;=0.6.0 Downloading gluonnlp-0.10.0.tar.gz (344 kB) |████████████████████████████████| 344 kB 24.7 MB/s Collecting mxnet&gt;=1.4.0 Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB) |████████████████████████████████| 47.3 MB 1.4 MB/s Collecting onnxruntime==1.8.0 Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB) |████████████████████████████████| 4.5 MB 34.9 MB/s Collecting sentencepiece&gt;=0.1.6 Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB) |████████████████████████████████| 1.2 MB 41.2 MB/s Requirement already satisfied: torch&gt;=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111) Collecting transformers&gt;=4.8.1 Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB) |████████████████████████████████| 3.8 MB 29.3 MB/s Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (2.0) Requirement already satisfied: numpy&gt;=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (1.21.5) Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0-&gt;kobert==0.2.3) (3.17.3) Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (0.29.28) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (21.3) Requirement already satisfied: requests&lt;3,&gt;=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2.23.0) Collecting graphviz&lt;0.9.0,&gt;=0.8.1 Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2021.10.8) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&gt;=1.4.0-&gt;kobert==0.2.3) (3.0.4) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.7.0-&gt;kobert==0.2.3) (3.10.0.2) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (3.6.0) Collecting sacremoses Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 48.6 MB/s Collecting pyyaml&gt;=5.1 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |████████████████████████████████| 596 kB 49.6 MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (2019.12.20) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (4.63.0) Collecting tokenizers!=0.11.3,&gt;=0.11.1 Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB) |████████████████████████████████| 6.5 MB 35.3 MB/s Collecting huggingface-hub&lt;1.0,&gt;=0.1.0 Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB) |████████████████████████████████| 67 kB 4.2 MB/s Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers&gt;=4.8.1-&gt;kobert==0.2.3) (4.11.2) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;gluonnlp&gt;=0.6.0-&gt;kobert==0.2.3) (3.0.7) Collecting jmespath&lt;1.0.0,&gt;=0.7.1 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Collecting s3transfer&lt;0.6.0,&gt;=0.5.0 Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 5.6 MB/s Collecting botocore&lt;1.25.0,&gt;=1.24.15 Downloading botocore-1.24.15-py3-none-any.whl (8.6 MB) |████████████████████████████████| 8.6 MB 39.4 MB/s Collecting urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB) |████████████████████████████████| 127 kB 46.1 MB/s Requirement already satisfied: python-dateutil&lt;3.0.0,&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore&lt;1.25.0,&gt;=1.24.15-&gt;boto3-&gt;kobert==0.2.3) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore&lt;1.25.0,&gt;=1.24.15-&gt;boto3-&gt;kobert==0.2.3) (1.15.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (3.7.0) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (1.1.0) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&gt;=4.8.1-&gt;kobert==0.2.3) (7.1.2) Building wheels for collected packages: kobert, gluonnlp Building wheel for kobert (setup.py) ... done Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15674 sha256=bdfc51e1c51af6aa53b2c1c03500fa599ff62e946d52c22f5b4915c71e013476 Stored in directory: /tmp/pip-ephem-wheel-cache-m0k4hbbz/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0 Building wheel for gluonnlp (setup.py) ... done Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595734 sha256=e7c624a30d2156ceeb1d7e9e8eb4ba30fbee3957f61846811c3025b36556875e Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00 Successfully built kobert gluonnlp Installing collected packages: urllib3, jmespath, pyyaml, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, graphviz, transformers, sentencepiece, onnxruntime, mxnet, gluonnlp, boto3, kobert Attempting uninstall: urllib3 Found existing installation: urllib3 1.24.3 Uninstalling urllib3-1.24.3: Successfully uninstalled urllib3-1.24.3 Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: graphviz Found existing installation: graphviz 0.10.1 Uninstalling graphviz-0.10.1: Successfully uninstalled graphviz-0.10.1 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. Successfully installed boto3-1.21.15 botocore-1.24.15 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.4.0 jmespath-0.10.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 s3transfer-0.5.2 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.17.0 urllib3-1.25.11 . from kobert import get_pytorch_kobert_model from kobert_tokenizer import KoBERTTokenizer tokenizer = KoBERTTokenizer.from_pretrained(&#39;skt/kobert-base-v1&#39;) bertmodel, vocab = get_pytorch_kobert_model() . The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. The tokenizer class you load from this checkpoint is &#39;XLNetTokenizer&#39;. The class this function is called from is &#39;KoBERTTokenizer&#39;. . using cached model. /content/drive/MyDrive/Colab Notebooks/.cache/kobert_v1.zip using cached model. /content/drive/MyDrive/Colab Notebooks/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece . &#54596;&#50836;&#54620; &#46972;&#51060;&#48652;&#47084;&#47532; &#48520;&#47084;&#50724;&#44592; . import torch from torch import nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import Dataset, DataLoader import gluonnlp as nlp import numpy as np from tqdm import tqdm, tqdm_notebook import pandas as pd #transformers from transformers import AdamW from transformers.optimization import get_cosine_schedule_with_warmup from transformers import BertModel #GPU 사용 시 device = torch.device(&quot;cuda:0&quot;) . &#45936;&#51060;&#53552;&#49483; &#48520;&#47084;&#50724;&#44592; . import pandas as pd data = pd.read_csv(&#39;kobert입력데이터(n=5).csv&#39;) . . data.head() . . document topic text . 0 1 | 15 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 강제 철거 업체 계... | . 1 1 | 15 | 존경 전시관 입점 업체 임차인 전시관 업체 심정 호소 강제 업체 계약 관계 통보 피... | . 2 1 | 15 | 존경 감사 주택 전체 입점 업체 임차인 주택 전시관 남용 심정 소속 강제 임대료 감... | . 3 1 | 15 | 존경 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 계약 관... | . 4 1 | 15 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; data[&quot;topic&quot;] = data[&quot;topic&quot;]-1 . . data.head() . . document topic text . 0 1 | 14 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 강제 철거 업체 계... | . 1 1 | 14 | 존경 전시관 입점 업체 임차인 전시관 업체 심정 호소 강제 업체 계약 관계 통보 피... | . 2 1 | 14 | 존경 감사 주택 전체 입점 업체 임차인 주택 전시관 남용 심정 소속 강제 임대료 감... | . 3 1 | 14 | 존경 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 계약 관... | . 4 1 | 14 | 존경 지지 주택 전시관 입점 업체 임차인 주택 전시관 업체 심정 호소 강제 철거 업... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; data_list = [] for ques, label in zip(data[&#39;text&#39;], data[&#39;topic&#39;]) : data = [] data.append(ques) data.append(str(label)) data_list.append(data) . . &#51077;&#47141; &#45936;&#51060;&#53552;&#49483;&#51012; &#53664;&#53360;&#54868;&#54616;&#44592; . class BERTDataset(Dataset): def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len, pad, pair): transform = nlp.data.BERTSentenceTransform( bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair) self.sentences = [transform([i[sent_idx]]) for i in dataset] self.labels = [np.int32(i[label_idx]) for i in dataset] def __getitem__(self, i): return (self.sentences[i] + (self.labels[i], )) def __len__(self): return (len(self.labels)) . &#54028;&#46972;&#48120;&#53552; &#49483;&#54021; . max_len = 200 batch_size = 32 warmup_ratio = 0.1 num_epochs = 10 max_grad_norm = 1 log_interval = 200 learning_rate = 4e-5 . Train data &amp; Test data . from sklearn.model_selection import train_test_split dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=34) . tok=tokenizer.tokenize data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False) data_test = BERTDataset(dataset_test,0, 1, tok, vocab, max_len, True, False) . &#53664;&#53360;&#54868; &#48143; &#54056;&#46377;, &#51221;&#49688; &#51064;&#53076;&#46377; &#51652;&#54665; . train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4) test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4) . KoBERT &#47784;&#45944; &#44396;&#54788; . class BERTClassifier(nn.Module): def __init__(self, bert, hidden_size = 768, num_classes=23, ##클래스 수 조정## dr_rate=None, params=None): super(BERTClassifier, self).__init__() self.bert = bert self.dr_rate = dr_rate self.classifier = nn.Linear(hidden_size , num_classes) if dr_rate: self.dropout = nn.Dropout(p=dr_rate) def gen_attention_mask(self, token_ids, valid_length): attention_mask = torch.zeros_like(token_ids) for i, v in enumerate(valid_length): attention_mask[i][:v] = 1 return attention_mask.float() def forward(self, token_ids, valid_length, segment_ids): attention_mask = self.gen_attention_mask(token_ids, valid_length) _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False) if self.dr_rate: out = self.dropout(pooler) return self.classifier(out) . model = BERTClassifier(bertmodel, dr_rate=0.3).to(device) #optimizer와 schedule 설정 no_decay = [&#39;bias&#39;, &#39;LayerNorm.weight&#39;] optimizer_grouped_parameters = [ {&#39;params&#39;: [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 1e-6}, {&#39;params&#39;: [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 0.0} ] optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate) loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 대표적인 loss func t_total = len(train_dataloader) * num_epochs warmup_step = int(t_total * warmup_ratio) scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total) #정확도 측정을 위한 함수 정의 def calc_accuracy(X,Y): max_vals, max_indices = torch.max(X, 1) train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0] return train_acc train_dataloader . /usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning FutureWarning, . &lt;torch.utils.data.dataloader.DataLoader at 0x7f57d0fef850&gt; . train_history=[] test_history=[] loss_history=[] for e in range(num_epochs): train_acc = 0.0 test_acc = 0.0 model.train() for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)): optimizer.zero_grad() token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length= valid_length label = label.long().to(device) out = model(token_ids, valid_length, segment_ids) #print(label.shape,out.shape) loss = loss_fn(out, label) loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) optimizer.step() scheduler.step() # Update learning rate schedule train_acc += calc_accuracy(out, label) if batch_id % log_interval == 0: print(&quot;epoch {} batch id {} loss {} train acc {}&quot;.format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1))) train_history.append(train_acc / (batch_id+1)) loss_history.append(loss.data.cpu().numpy()) print(&quot;epoch {} train acc {}&quot;.format(e+1, train_acc / (batch_id+1))) #train_history.append(train_acc / (batch_id+1)) model.eval() for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)): token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length= valid_length label = label.long().to(device) out = model(token_ids, valid_length, segment_ids) test_acc += calc_accuracy(out, label) print(&quot;epoch {} test acc {}&quot;.format(e+1, test_acc / (batch_id+1))) test_history.append(test_acc / (batch_id+1)) . . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` . epoch 1 batch id 1 loss 3.2182071208953857 train acc 0.0 epoch 1 batch id 201 loss 3.0333094596862793 train acc 0.052705223880597014 epoch 1 batch id 401 loss 2.5394673347473145 train acc 0.11050498753117206 epoch 1 batch id 601 loss 2.003541946411133 train acc 0.2413685524126456 epoch 1 batch id 801 loss 1.3171145915985107 train acc 0.3445302746566791 epoch 1 batch id 1001 loss 1.361488938331604 train acc 0.41561563436563437 epoch 1 batch id 1201 loss 0.974885880947113 train acc 0.4704933388842631 epoch 1 batch id 1401 loss 0.9846394658088684 train acc 0.5114204139900071 epoch 1 batch id 1601 loss 0.3865899443626404 train acc 0.5452841973766396 epoch 1 batch id 1801 loss 0.7350060939788818 train acc 0.5720433092726264 epoch 1 train acc 0.59415625 . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` . epoch 1 test acc 0.8024825 epoch 2 batch id 1 loss 0.7587311267852783 train acc 0.84375 epoch 2 batch id 201 loss 0.45753997564315796 train acc 0.8154539800995025 epoch 2 batch id 401 loss 0.21193328499794006 train acc 0.820682668329177 epoch 2 batch id 601 loss 0.2509940266609192 train acc 0.8291389351081531 epoch 2 batch id 801 loss 0.4809306561946869 train acc 0.8338405118601748 epoch 2 batch id 1001 loss 0.19113385677337646 train acc 0.8403159340659341 epoch 2 batch id 1201 loss 0.4273354709148407 train acc 0.8473927976686095 epoch 2 batch id 1401 loss 0.4585113525390625 train acc 0.8525160599571735 epoch 2 batch id 1601 loss 0.23949317634105682 train acc 0.8577841973766396 epoch 2 batch id 1801 loss 0.6034642457962036 train acc 0.8620558023320377 epoch 2 train acc 0.8655 epoch 2 test acc 0.8954825 epoch 3 batch id 1 loss 0.4243091642856598 train acc 0.90625 epoch 3 batch id 201 loss 0.21953757107257843 train acc 0.9146455223880597 epoch 3 batch id 401 loss 0.07513393461704254 train acc 0.9156016209476309 epoch 3 batch id 601 loss 0.18744242191314697 train acc 0.9186772046589018 epoch 3 batch id 801 loss 0.2241429090499878 train acc 0.9189684769038702 epoch 3 batch id 1001 loss 0.10889383405447006 train acc 0.9202047952047953 epoch 3 batch id 1201 loss 0.18296684324741364 train acc 0.9220181099084097 epoch 3 batch id 1401 loss 0.14050564169883728 train acc 0.9232690935046396 epoch 3 batch id 1601 loss 0.09440383315086365 train acc 0.9252029981261711 epoch 3 batch id 1801 loss 0.5415806770324707 train acc 0.9260653803442532 epoch 3 train acc 0.9265625 epoch 3 test acc 0.925875 epoch 4 batch id 1 loss 0.14898772537708282 train acc 0.96875 epoch 4 batch id 201 loss 0.12260929495096207 train acc 0.9416977611940298 epoch 4 batch id 401 loss 0.02143574133515358 train acc 0.9424096009975063 epoch 4 batch id 601 loss 0.20356294512748718 train acc 0.9439995840266223 epoch 4 batch id 801 loss 0.12632118165493011 train acc 0.9434691011235955 epoch 4 batch id 1001 loss 0.13546746969223022 train acc 0.9446803196803197 epoch 4 batch id 1201 loss 0.19045278429985046 train acc 0.9453840549542049 epoch 4 batch id 1401 loss 0.33764785528182983 train acc 0.9457530335474661 epoch 4 batch id 1601 loss 0.025721626356244087 train acc 0.9464982823235478 epoch 4 batch id 1801 loss 0.26869356632232666 train acc 0.946609522487507 epoch 4 train acc 0.946546875 epoch 4 test acc 0.939305 epoch 5 batch id 1 loss 0.04207547754049301 train acc 1.0 epoch 5 batch id 201 loss 0.03910402953624725 train acc 0.9519589552238806 epoch 5 batch id 401 loss 0.028201986104249954 train acc 0.9544887780548629 epoch 5 batch id 601 loss 0.25792691111564636 train acc 0.9551268718801996 epoch 5 batch id 801 loss 0.040920648723840714 train acc 0.9553292759051186 epoch 5 batch id 1001 loss 0.15717390179634094 train acc 0.9556068931068931 epoch 5 batch id 1201 loss 0.005653911270201206 train acc 0.9557660283097419 epoch 5 batch id 1401 loss 0.015682296827435493 train acc 0.9557235902926481 epoch 5 batch id 1601 loss 0.015796437859535217 train acc 0.9561602123672704 epoch 5 batch id 1801 loss 0.33754774928092957 train acc 0.9558058023320377 epoch 5 train acc 0.9556875 epoch 5 test acc 0.944385 epoch 6 batch id 1 loss 0.14212766289710999 train acc 0.96875 epoch 6 batch id 201 loss 0.06867929548025131 train acc 0.9600435323383084 epoch 6 batch id 401 loss 0.0469791442155838 train acc 0.9609569825436409 epoch 6 batch id 601 loss 0.027698852121829987 train acc 0.9620424292845258 epoch 6 batch id 801 loss 0.018958164379000664 train acc 0.9618835830212235 epoch 6 batch id 1001 loss 0.15529529750347137 train acc 0.9622877122877123 epoch 6 batch id 1201 loss 0.004932724870741367 train acc 0.9628434637801832 epoch 6 batch id 1401 loss 0.008432171307504177 train acc 0.9627944325481799 epoch 6 batch id 1601 loss 0.018579157069325447 train acc 0.9629138038725796 epoch 6 batch id 1801 loss 0.23195700347423553 train acc 0.9628157967795669 epoch 6 train acc 0.96246875 epoch 6 test acc 0.9479299999999999 epoch 7 batch id 1 loss 0.005102638155221939 train acc 1.0 epoch 7 batch id 201 loss 0.0474524050951004 train acc 0.9625310945273632 epoch 7 batch id 401 loss 0.03710765391588211 train acc 0.9650872817955112 epoch 7 batch id 601 loss 0.006671147886663675 train acc 0.9666181364392679 epoch 7 batch id 801 loss 0.03770418465137482 train acc 0.9666042446941323 epoch 7 batch id 1001 loss 0.04848151654005051 train acc 0.9667832167832168 epoch 7 batch id 1201 loss 0.002549548167735338 train acc 0.9671627810158201 epoch 7 batch id 1401 loss 0.00337349995970726 train acc 0.9673001427551748 epoch 7 batch id 1601 loss 0.005779507104307413 train acc 0.9673836664584634 epoch 7 batch id 1801 loss 0.22956259548664093 train acc 0.9671536646307607 epoch 7 train acc 0.966796875 epoch 7 test acc 0.948465 epoch 8 batch id 1 loss 0.0025592187885195017 train acc 1.0 epoch 8 batch id 201 loss 0.03764337673783302 train acc 0.9678171641791045 epoch 8 batch id 401 loss 0.03216252475976944 train acc 0.9685941396508728 epoch 8 batch id 601 loss 0.004787285812199116 train acc 0.9697899334442596 epoch 8 batch id 801 loss 0.0339641273021698 train acc 0.9700764669163545 epoch 8 batch id 1001 loss 0.05479202792048454 train acc 0.9702797202797203 epoch 8 batch id 1201 loss 0.00951945036649704 train acc 0.9703372189841799 epoch 8 batch id 1401 loss 0.0018710399745032191 train acc 0.9703559957173448 epoch 8 batch id 1601 loss 0.006067350506782532 train acc 0.9704871955028107 epoch 8 batch id 1801 loss 0.19043497741222382 train acc 0.9703983897834536 epoch 8 train acc 0.969890625 epoch 8 test acc 0.9481975 epoch 9 batch id 1 loss 0.0056968447752296925 train acc 1.0 epoch 9 batch id 201 loss 0.016539184376597404 train acc 0.9676616915422885 epoch 9 batch id 401 loss 0.05257967859506607 train acc 0.9694513715710723 epoch 9 batch id 601 loss 0.002826718147844076 train acc 0.971089850249584 epoch 9 batch id 801 loss 0.02559412084519863 train acc 0.9716760299625468 epoch 9 batch id 1001 loss 0.056784145534038544 train acc 0.972027972027972 epoch 9 batch id 1201 loss 0.0012190788984298706 train acc 0.9721846378018318 epoch 9 batch id 1401 loss 0.0018965977942571044 train acc 0.9720512134189865 epoch 9 batch id 1601 loss 0.016450652852654457 train acc 0.9719901623985009 epoch 9 batch id 1801 loss 0.19731643795967102 train acc 0.9720467795669073 epoch 9 train acc 0.971859375 epoch 9 test acc 0.9469474999999999 epoch 10 batch id 1 loss 0.0060175699181854725 train acc 1.0 epoch 10 batch id 201 loss 0.026890743523836136 train acc 0.9734141791044776 epoch 10 batch id 401 loss 0.06891006231307983 train acc 0.9737375311720698 epoch 10 batch id 601 loss 0.0034921150654554367 train acc 0.9752495840266223 epoch 10 batch id 801 loss 0.021551022306084633 train acc 0.9754213483146067 epoch 10 batch id 1001 loss 0.038401391357183456 train acc 0.9757117882117882 epoch 10 batch id 1201 loss 0.0011323075741529465 train acc 0.9758534554537885 epoch 10 batch id 1401 loss 0.002252324717119336 train acc 0.9758877587437544 epoch 10 batch id 1601 loss 0.02398933656513691 train acc 0.9757183010618363 epoch 10 batch id 1801 loss 0.18408672511577606 train acc 0.9756038312048861 epoch 10 train acc 0.9751875 epoch 10 test acc 0.94668 . import pandas as pd . . test_history . . [0.8024825, 0.8954825, 0.925875, 0.939305, 0.944385, 0.9479299999999999, 0.948465, 0.9481975, 0.9469474999999999, 0.94668] . total_train = pd.DataFrame() total_train[&quot;train_history&quot;] = train_history total_train[&quot;loss_history&quot;] = loss_history total_test = pd.DataFrame() total_test[&quot;test_history&quot;] = test_history total_train.to_csv(&quot;데이터증강(n=5) train.csv&quot;) total_test.to_csv(&quot;데이터증강(n=5) test.csv&quot;) . . &#54617;&#49845;&#46108; &#47784;&#45944;&#51012; &#51060;&#50857;&#54620; test data set &#50696;&#52769; . &#50696;&#52769; &#54632;&#49688; &#44396;&#54788; . def predict(predict_sentence): data = [predict_sentence, &#39;0&#39;] dataset_another = [data] another_test = BERTDataset(dataset_another, 0, 1, tok, vocab, max_len, True, False) test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=4) model.eval() for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader): token_ids = token_ids.long().to(device) segment_ids = segment_ids.long().to(device) valid_length= valid_length label = label.long().to(device) out = model(token_ids, valid_length, segment_ids) test_eval=[] for i in out: logits=i logits = logits.detach().cpu().numpy() test_eval.append(np.argmax(logits)) return test_eval . loop&#47484; &#53685;&#54644; test data set&#51032; &#51204;&#52404; &#47928;&#49436;&#51032; &#51201;&#50857; . length = len(dataset_test) . length . 15993 . y_pred = [] y_true = [] for i in range(length) : pred = predict(dataset_test[i][0])[0] y_pred.append(pred) y_true.append(dataset_test[i][1]) . &#44208;&#44284; &#51200;&#51109; . result = pd.DataFrame() result[&quot;y_pred&quot;] = y_pred result[&quot;y_true&quot;] = y_true . result.head() . y_pred y_true . 0 2 | 2 | . 1 16 | 16 | . 2 6 | 6 | . 3 18 | 18 | . 4 12 | 12 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; result.dtypes . y_pred int64 y_true object dtype: object . result[&quot;y_true&quot;]=result[&quot;y_true&quot;].astype(int) . . result.dtypes . y_pred int64 y_true int64 dtype: object . 모델 입력시 토픽 넘버에 1을 빼줬으므로 다시 더해줌 | . result[&quot;y_true&quot;] = result[&quot;y_true&quot;]+1 result[&quot;y_pred&quot;] = result[&quot;y_pred&quot;]+1 . result.head() . . y_pred y_true . 0 3 | 3 | . 1 17 | 17 | . 2 7 | 7 | . 3 19 | 19 | . 4 13 | 13 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 1을 더한 토픽값이 잘 생성되었는지 결과 확인 | . result[&quot;y_true&quot;].unique() . array([ 3, 17, 7, 19, 13, 8, 22, 12, 4, 9, 11, 23, 1, 6, 20, 16, 5, 21, 2, 10, 18, 14, 15]) . 결과 파일 저장 (14시간 걸린 파일이나 목숨보다 소중히 하자...) | . result.to_csv(&quot;predict 결과.csv&quot;) . Confusion matrix report &#51089;&#49457; . from sklearn.metrics import classification_report import pandas as pd . result = pd.read_csv(&quot;predict 결과.csv&quot;) . y_true = result[&quot;y_true&quot;].to_list() y_pred = result[&quot;y_pred&quot;].to_list() . topic_number = list(range(1,24)) topic_number = list(map(str,topic_number)) print(topic_number) . [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;, &#39;10&#39;, &#39;11&#39;, &#39;12&#39;, &#39;13&#39;, &#39;14&#39;, &#39;15&#39;, &#39;16&#39;, &#39;17&#39;, &#39;18&#39;, &#39;19&#39;, &#39;20&#39;, &#39;21&#39;, &#39;22&#39;, &#39;23&#39;] . target_names = [] for i in range((len(topic_number))) : target_names.append(&quot;토픽 &quot; + topic_number[i]) . report = classification_report(y_true, y_pred, target_names=target_names,output_dict=True) . report_df = pd.DataFrame(report).transpose() . report_df . . precision recall f1-score support . 토픽 1 0.955319 | 0.958378 | 0.956846 | 937.000000 | . 토픽 2 0.938395 | 0.933048 | 0.935714 | 702.000000 | . 토픽 3 0.950920 | 0.943683 | 0.947288 | 657.000000 | . 토픽 4 0.944134 | 0.926535 | 0.935252 | 912.000000 | . 토픽 5 0.919877 | 0.914242 | 0.917051 | 653.000000 | . 토픽 6 0.947257 | 0.949260 | 0.948258 | 946.000000 | . 토픽 7 0.932357 | 0.946214 | 0.939234 | 1413.000000 | . 토픽 8 0.951456 | 0.974586 | 0.962882 | 905.000000 | . 토픽 9 0.960767 | 0.968366 | 0.964551 | 1138.000000 | . 토픽 10 0.965574 | 0.937898 | 0.951535 | 628.000000 | . 토픽 11 0.958435 | 0.956098 | 0.957265 | 410.000000 | . 토픽 12 0.919575 | 0.945398 | 0.932308 | 641.000000 | . 토픽 13 0.940299 | 0.945946 | 0.943114 | 666.000000 | . 토픽 14 0.810000 | 0.931034 | 0.866310 | 87.000000 | . 토픽 15 0.966587 | 0.950704 | 0.958580 | 426.000000 | . 토픽 16 0.948542 | 0.946918 | 0.947729 | 584.000000 | . 토픽 17 0.961272 | 0.939189 | 0.950103 | 740.000000 | . 토픽 18 0.930070 | 0.930070 | 0.930070 | 143.000000 | . 토픽 19 0.939219 | 0.955817 | 0.947445 | 679.000000 | . 토픽 20 0.973684 | 0.965599 | 0.969625 | 843.000000 | . 토픽 21 0.902367 | 0.915916 | 0.909091 | 333.000000 | . 토픽 22 0.963855 | 0.939794 | 0.951673 | 681.000000 | . 토픽 23 0.949883 | 0.937860 | 0.943833 | 869.000000 | . accuracy 0.946727 | 0.946727 | 0.946727 | 0.946727 | . macro avg 0.940428 | 0.944024 | 0.941989 | 15993.000000 | . weighted avg 0.946960 | 0.946727 | 0.946764 | 15993.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; report_df.to_csv(&quot;분석결과report.csv&quot;) . 분석이 끝났다. 맨 처음 데이터를 증강시키 않은 상태에서 5, 10 배 씩 증가시켜 모델을 적합하였다. | . 총 3 가지 경우에 대해 모델링을 한 시간을 종합해보면 하.... 총 30 시간은 넘은 것 같다. 중간에 colab 알람 못 듣고 자서 모델 다시 돌릴 땐 정말 창문을 부수고 싶었다. 결과가 나와서 다행이다. 참고한 블로그와 코드 링크는 아래에 첨부하겠습니당 | . &#52280;&#44256; . [1] KoBert 모델링 . [2] 데이터 증강 .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2022/03/09/KoBERT-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5(%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A6%9D%EA%B0%95-n=5).html",
            "relUrl": "/python/2022/03/09/KoBERT-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5(%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A6%9D%EA%B0%95-n=5).html",
            "date": " • Mar 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "데이터 증강",
            "content": "Introduction . 본 코드는 EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks 를 한국어로 쓸 수 있도록 wordnet 부분만 교체한 프로젝트의 코드를 인용하였다. | . 원 데이터를 5,10 배수로 증가시켰다. | . &#54632;&#49688; &#44396;&#54788; . import random import pickle import re wordnet = {} with open(&quot;wordnet.pickle&quot;, &quot;rb&quot;) as f: wordnet = pickle.load(f) # 한글만 남기고 나머지는 삭제 def get_only_hangul(line): parseText= re.compile(&#39;/ ^[ㄱ-ㅎㅏ-ㅣ가-힣]*$/&#39;).sub(&#39;&#39;,line) return parseText ######################################################################## # Synonym replacement # Replace n words in the sentence with synonyms from wordnet ######################################################################## def synonym_replacement(words, n): new_words = words.copy() random_word_list = list(set([word for word in words])) random.shuffle(random_word_list) num_replaced = 0 for random_word in random_word_list: synonyms = get_synonyms(random_word) if len(synonyms) &gt;= 1: synonym = random.choice(list(synonyms)) new_words = [synonym if word == random_word else word for word in new_words] num_replaced += 1 if num_replaced &gt;= n: break if len(new_words) != 0: sentence = &#39; &#39;.join(new_words) new_words = sentence.split(&quot; &quot;) else: new_words = &quot;&quot; return new_words def get_synonyms(word): synomyms = [] try: for syn in wordnet[word]: for s in syn: synomyms.append(s) except: pass return synomyms ######################################################################## # Random deletion # Randomly delete words from the sentence with probability p ######################################################################## def random_deletion(words, p): if len(words) == 1: return words new_words = [] for word in words: r = random.uniform(0, 1) if r &gt; p: new_words.append(word) if len(new_words) == 0: rand_int = random.randint(0, len(words)-1) return [words[rand_int]] return new_words ######################################################################## # Random swap # Randomly swap two words in the sentence n times ######################################################################## def random_swap(words, n): new_words = words.copy() for _ in range(n): new_words = swap_word(new_words) return new_words def swap_word(new_words): random_idx_1 = random.randint(0, len(new_words)-1) random_idx_2 = random_idx_1 counter = 0 while random_idx_2 == random_idx_1: random_idx_2 = random.randint(0, len(new_words)-1) counter += 1 if counter &gt; 3: return new_words new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] return new_words ######################################################################## # Random insertion # Randomly insert n words into the sentence ######################################################################## def random_insertion(words, n): new_words = words.copy() for _ in range(n): add_word(new_words) return new_words def add_word(new_words): synonyms = [] counter = 0 while len(synonyms) &lt; 1: if len(new_words) &gt;= 1: random_word = new_words[random.randint(0, len(new_words)-1)] synonyms = get_synonyms(random_word) counter += 1 else: random_word = &quot;&quot; if counter &gt;= 10: return random_synonym = synonyms[0] random_idx = random.randint(0, len(new_words)-1) new_words.insert(random_idx, random_synonym) def EDA(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=4): sentence = get_only_hangul(sentence) words = sentence.split(&#39; &#39;) words = [word for word in words if word is not &quot;&quot;] num_words = len(words) augmented_sentences = [] num_new_per_technique = int(num_aug/4) + 1 n_sr = max(1, int(alpha_sr*num_words)) n_ri = max(1, int(alpha_ri*num_words)) n_rs = max(1, int(alpha_rs*num_words)) # sr for _ in range(num_new_per_technique): a_words = synonym_replacement(words, n_sr) augmented_sentences.append(&#39; &#39;.join(a_words)) # ri for _ in range(num_new_per_technique): a_words = random_insertion(words, n_ri) augmented_sentences.append(&#39; &#39;.join(a_words)) # rs for _ in range(num_new_per_technique): a_words = random_swap(words, n_rs) augmented_sentences.append(&quot; &quot;.join(a_words)) # rd for _ in range(num_new_per_technique): a_words = random_deletion(words, p_rd) augmented_sentences.append(&quot; &quot;.join(a_words)) augmented_sentences = [get_only_hangul(sentence) for sentence in augmented_sentences] random.shuffle(augmented_sentences) if num_aug &gt;= 1: augmented_sentences = augmented_sentences[:num_aug] else: keep_prob = num_aug / len(augmented_sentences) augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) &lt; keep_prob] augmented_sentences.append(sentence) return augmented_sentences . &lt;&gt;:138: SyntaxWarning: &#34;is not&#34; with a literal. Did you mean &#34;!=&#34;? &lt;&gt;:138: SyntaxWarning: &#34;is not&#34; with a literal. Did you mean &#34;!=&#34;? C: Users rkdcj AppData Local Temp/ipykernel_17464/3250283189.py:138: SyntaxWarning: &#34;is not&#34; with a literal. Did you mean &#34;!=&#34;? words = [word for word in words if word is not &#34;&#34;] . &#44592;&#51316; &#45936;&#51060;&#53552; &#48520;&#47084;&#50724;&#44592; . import pandas as pd . data = pd.read_csv(&quot;kobert입력데이터.csv&quot;) . topic, document &#52972;&#47100; type &#48320;&#44221; . data[&quot;topic&quot;] = data[&quot;topic&quot;].astype(str) data[&quot;document&quot;] = data[&quot;document&quot;].astype(str) . length = len(data) . &#48708;&#50612;&#51080;&#45716; &#45936;&#51060;&#53552;&#54532;&#47112;&#51076;&#49373;&#49457; . total = pd.DataFrame() . &#49373;&#49457; &#54980; &#54632;&#49688;&#47484; &#46028;&#47140; &#47928;&#49436;&#48324;&#47196; &#49939;&#44592; . 각 문서를 4개씩 증가시켜 1개의 문서를 5개로 만들었다!! (이 과정을 10배수에도 적용하였음.) | . for i in range(length) : temp = pd.DataFrame() text = EDA(data[&quot;clean_txt1&quot;][i]) l = len(text) topic = [data[&quot;topic&quot;][i]]*l document = [data[&quot;document&quot;][i]]*l temp[&quot;topic&quot;] = topic temp[&quot;text&quot;] = text temp[&quot;document&quot;] = document total =pd.concat([total,temp]) . total = total.iloc[:, [2,0,1]] . total = total.reset_index().iloc[:,[1,2,3]] . &#50756;&#49457;&#46108; &#45936;&#51060;&#53552;&#47484; &#51200;&#51109;&#54616;&#51088;! . total.to_csv(&quot;kobert데이터증강(n=5).csv&quot;) . &#52280;&#44256; . [1] 데이터증강기법 논문리뷰 블로그 . [2] 데이터증강기법 코드 .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2022/03/08/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A6%9D%EA%B0%95%EA%B8%B0%EB%B2%95.html",
            "relUrl": "/python/2022/03/08/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A6%9D%EA%B0%95%EA%B8%B0%EB%B2%95.html",
            "date": " • Mar 8, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Word Embedding",
            "content": "WordEmbedding . 1. 차원의 저주(Curse of Dimensionality) : 수학적 공간 차원(=변수 개수)이 늘어나면서, 문제 계산법이 지수적으로 커지는 상황 . 만약 $x=[1,2,3,4,5], , y= [0,0,0,0,0] to (X,Y)$ 을 표현한다고 하자 . 아래와 같이 1차원 상에서 표현되는 정보를 2차원 상에서 표현하게되어 설명 공간이 $5^2 =25$가 된 것이다. . 이러한 경우를 차원의 저주라고 하며 이는 모델링 과정에서 저장 공간과 처리 시간이 불필요하게 증가됨을 의미한다. . 이러한 문제점은 위와 같은 $(X,Y)$이산형 확률분포에서 결합분포를 구할 때 발생한다. . 또한 이산형 변수들이 다양한 값$(0,1,2 dots 145748)$을 가질 경우 같은 길이의 두 문자열에서 거리를 측정하는 척도인 &quot;해밍 거리&quot;의 값이 거의 최댓값에 이르게 된다. [1] . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . . fig, axes = plt.subplots(1,2, figsize=(10,5)) ax1, ax2 =axes x = [1,2,3,4,5] y = [0,0,0,0,0] ax1.plot(x,y) ax1.set_title(&quot;1-dimensional&quot;) ax1.axis(&quot;off&quot;) ax2.plot(x,y) ax2.set_title(&quot;2-dimensional&quot;) fig.tight_layout() . . 이러한 문제점을 해결하기 위해 NLP 분야에서는 단어를 저차원에 표현하기 위한 &quot;워드 임베딩(Word Embedding)&quot;을 제안하였다. . 기존의 통계적인 방법이 단어의 출현 빈도에 집중 한다면 워드 임베딩은 서로 유사한 단어들 간 유사성을 포착하는데 집중한다. . - 가정 : 유사한 의미를 가진 단어는 유사한 문맥안에서 발견된다. . - 가정의 해석 : 유사한 의미를 가진 단어들은 유사한 단어 벡터를 가진다. . - 이점 : 이웃된 단어들의 단어 벡터들을 학습하여 단어간 유사성을 도출해낼 수 있다. . example 1 :&#39;man&#39; + &#39;royal&#39; = &#39;king&#39; . 이제 이러한 임베딩 기법 중 하나인 &quot;Word2Vec&quot; 기법을 소개한다. [2] . Word2Vec . 워드 &quot;Word2Vec&quot; 중 가장 대표적인 방법으로 &quot;CBOW&quot;, &quot;skip-gram&quot; 이 존재한다. | . CBOW . 주변 단어를 이용하여 중심단어를 예측한다. | . 주어진 문맥에서 window size $k$ 를 적용해 target word 양옆에 $k$개의 단어들을 이용하여 조건부 확률을 계산한다. (편의상 k=1 이라고 설정) | . 프라닭 이라는 단어를 예측한다고 가정한다. | . 문장1: 금요일 밤에 프라닭은 못참지 . 문장2: 불금인데 교촌치킨 에 맥주? . 단어 : [ &quot;금요일&quot;, &quot;밤&quot;, &quot;프라닭&quot;,&quot;불금&quot;, &quot;교촌치킨&quot;, &quot;맥주&quot; ] $ to Word in R^{6 times 6}$ . 문장의 개수는 $j=2$, 단어의 개수는 총 $i=6$, 축소할 임베딩 차원의 개수는 $n=3$ 으로 설정하자. | . 차원축소를 위해 생성되는 임베딩(=가중치) 행렬 $W in R^{6 times 3}$ 으로 파이토치 기준 $N(0,1)$에서 생성된다. | . 목적 1 : $Word in R^{6 times 6} to W in R^{6 times 3}$ | . 목적 2 : 단어간 의미적 유사성을 포착하기 위한 임베딩 행렬 갱신 $W^{t} to W^{t+1} $ | . 1. one-hot vector $Word in R^{6 times 6}$ 생성 . import numpy as np import pandas as pd index = [ &quot;금요일&quot;, &quot;밤&quot;, &quot;프라닭&quot;,&quot;불금&quot;, &quot;교촌치킨&quot;, &quot;맥주&quot; ] word1 = [1,0,0,0,0,0] word2 = [0,1,0,0,0,0] word3 = [0,0,1,0,0,0] word4 = [0,0,0,1,0,0] word5 = [0,0,0,0,1,0] word6 = [0,0,0,0,0,1] one_hot = pd.DataFrame([word1,word2,word3,word4,word5,word6],index=index) one_hot . . 0 1 2 3 4 5 . 금요일 1 | 0 | 0 | 0 | 0 | 0 | . 밤 0 | 1 | 0 | 0 | 0 | 0 | . 프라닭 0 | 0 | 1 | 0 | 0 | 0 | . 불금 0 | 0 | 0 | 1 | 0 | 0 | . 교촌치킨 0 | 0 | 0 | 0 | 1 | 0 | . 맥주 0 | 0 | 0 | 0 | 0 | 1 | . 2. 임베딩(가중치) 행렬 생성 $W in R^{6 times 3}$ . W = np.random.normal(loc = 0, scale=1,size=18).reshape(6,3) W = pd.DataFrame(W,index=index, columns = [&quot;W1&quot;,&quot;W2&quot;,&quot;W3&quot;]) W . . W1 W2 W3 . 금요일 -0.946677 | -0.964799 | -2.236172 | . 밤 1.481341 | 0.678401 | -1.239748 | . 프라닭 -0.855941 | 0.556102 | 0.330505 | . 불금 0.316146 | -1.791996 | -0.307091 | . 교촌치킨 1.289018 | -1.415381 | 0.418707 | . 맥주 1.106920 | 0.051748 | 0.478479 | . 3. $ widehat W_{프라닭} = frac {W_{밤} + W_{불금}} {2} = [0.89,-0.55,0.77]$ . W_1 = list((W.loc[&quot;밤&quot;] + W.loc[&quot;불금&quot;])/2) W_1 . . [0.8987433793584608, -0.5567973759616802, -0.7734197905021276] . 4. $ Z = widehat W_{프라닭} times W^T = $ [ 1.42, 1.91, -1.33, 1.52, 1.62, 0.6 ] . z = np.dot(np.array(W_1),W.T.to_numpy()) z.round(2) . . array([ 1.42, 1.91, -1.33, 1.52, 1.62, 0.6 ]) . 5. $ hat y$ 계산 . $ hat y=softmax(Z) = [0.18, , ,0.30, , ,0.01, , ,0.20, , ,0.22, , ,0.08]$ . from scipy.special import softmax y=[0,0,1,0,0,0] so = pd.DataFrame({&quot;y_hat&quot;: softmax(z), &quot;y&quot; : y},index=index) so . . y_hat y . 금요일 0.182270 | 0 | . 밤 0.299486 | 0 | . 프라닭 0.011647 | 1 | . 불금 0.202155 | 0 | . 교촌치킨 0.224158 | 0 | . 맥주 0.080284 | 0 | . &#44032;&#51473;&#52824; &#54665;&#47148; &#44081;&#49888; . 위와 같은 과정을 모든 단어에 대해 수행하여 크로스 엔트로피 함수를 적용한 $loss$를 계산한다. . 1. $loss=- sum_{i=1}^6 y_i log p_i$ . 2.$loss$를 최소화 하는 최적의 파라미터 $ theta$를 구함 $ to frac { partial loss}{ partial p} $ . 3. example $W_{밤},W_{불금}$ 업데이트($ alpha $ : learning rate) . $W_{밤}^{t+1} = W_{밤}^t + left( , alpha , times theta , right)$ . $W_{불금}^{t+1} = W_{불금}^t + left( , alpha , times theta , right)$ . Summary . 중심단어 벡터 $W_c$가 있고, 주변 단어 벡터 $W_o$가 있다고 하자. | . $t+1$ 시점에서 $t$ 시점의 결과를 반영하여 단어벡터 $W_{o}$를 갱신한다. | . $W^{t+1}_{o} =W^{t}_{o} + alpha times [l( theta_{c})]$ | . 타겟단어 예측시 사용되는 수식 [4] | . $$P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{w=1}^W exp ,({v^{ prime}_w}^Tv_{w_{I}}) } $$ . $$I , ,: , ,Input, , ,O , ,: , , Output$$ . $$W : number , , , of , , , Word$$ . Skip-gram . Skip-gram의 경우 CBOW와 달리 중심단어를 가지고 주변단어를 예측하는 과정이다. | . 따라서 CBOW의 3번째 단계 window-size내의 주변 단어들의 합을 평균 내는 과정이 생략된다. | . 이러한 부분을 빼면 CBOW와 동일하다. | . Skip-gram과 CBOW의 경우 아래와 같은 수식을 최대화 하는 것을 목표로 한다. (베르누이분포의 MLE 를 생각해보장) [4] | . 베르누이분포의 MLE와 크로스 엔트로피 손실함수의 최소값 파라미터를 구하는 것이 동치라고 생각해보자.(이 부분 다시 증명) | . $$ frac 1T sum_{t=1}^T sum_{-c leq j leq c, j neq 0} y_{t+j} log p left(w_{t+j}|w_t right), quad[4]$$ . $$T : mathrm{ number , , of , , traning , , word}, quad c : mathrm{the , , size , , of , , training , , context , , (=window)} $$ . 사실상 $y_{t+j}$ 는 예측하고자 하는 target단어가 아니면 0, 맞으면 1이므로 위식은 다시 아래와 같이 바꿔 쓸 수 있다. | . $$ frac 1T sum_{t=1}^T sum_{-c leq j leq c, j neq 0} log p left(w_{t+j}|w_t right), quad[4]$$ . Negative Sampling . $$ P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{w=1}^W exp ,({v^{ prime}_w}^Tv_{w_{I}}) }$$ . 위와 같이 Word2Vec은 출력층이 내놓는 스코어 값에 소프트맥스 함수를 적용해 확률값으로 변환한 후 이를 정답과 비교해 &quot;역전파(backpropagation)&quot; 하는 구조이다. | . 그런데 소프트맥스를 적용하려면 분모에 해당하는 값, 즉 중심(=input) 단어와 나머지 모든 단어의 내적을 한 뒤 이를 다시 $ exp$ 취해줘야한다. | . 보통 전체 단어가 10만개 이상 주어지므로 계산량이 어마어마 해진다. | . Negative Sampling은 소프트맥스 확률을 구할 때 전체 단어를 구하지 않고, 일부 단어만 뽑아서 계산을 하자는 아이디어다.[3][4] | . 사용자가 지정한 윈도우 사이즈 내에 등장하지 않는 단어(Negative saple)을 5~20개 정도 뽑고, 이를 우리가 예측하고자 하는 타겟 단어와 합쳐 전체 단어처럼 소프트맥스 확률을 구하는 것이다. | . 보통 5~20 ($k$) 개 정도 뽑는데, 이는 만약 윈도우 사이즈가 5일 경우 최대 25개의 단어를 대상으로만 소프트맥스 확률을 계산하고, 파라미터 업데이트도 25개 대상으로만 이뤄진다는 이야기이다.[3][4] | . $$ P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{i=1}^k exp ,({v^{ prime}_w}^Tv_{w_{I}}) }$$ . 특정 단어가 negative sampliing 될 확률 $P_n(w_i) $은 다음과 같다. $P_n(w_i)$는 free parameter로 선행연구에 의하면 단일 단어인 $ mathrm{unigram-distribution}$의 경우 $f(w_i)^{3/4}$일 때 뛰어난 성능을 보였다고 한다.[4] | . $$ P_n(w_i)= frac {f(w_i)^{3/4}}{ sum_{j=0}^n f(w_j)^{3/4}}$$ . $$f(w_i) = (해당 , ,단어 , ,빈도 , , / , ,전체 , , 단어 , , 수)$$ . 참고문헌 . skip-gram : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/ . [1] : Bengio, Yoshua, et al. &quot;A neural probabilistic language model.&quot; Journal of machine learning research 3.Feb (2003): 1137-1155. . [2] : Young, Tom, et al. &quot;Recent trends in deep learning based natural language processing.&quot; ieee Computational intelligenCe magazine 13.3 (2018): 55-75. . [3] : Mikolov, Tomas, et al. &quot;Efficient estimation of word representations in vector space.&quot; arXiv preprint arXiv:1301.3781 (2013). . [4] : Mikolov, Tomas, et al. &quot;Distributed representations of words and phrases and their compositionality.&quot; Advances in neural information processing systems. 2013. .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2022/01/21/Word2Vec.html",
            "relUrl": "/python/2022/01/21/Word2Vec.html",
            "date": " • Jan 21, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Chapter 2. NLP 기술 빠르게 훑어보기",
            "content": "&#47568;&#47945;&#52824;, &#53664;&#53360;, &#53440;&#51077; . &#47568;&#47945;&#52824; . 고전이나 현대의 모든 NLP 작업에서 쓰이는 text data 샘플 : metadata + text | 위 같은 샘플들이 모인 데이터 셋을 말뭉치라고 표현한다, | . | . &#53664;&#53360;&#54868; (Tokenization) . 토큰 : 문법적으로 더 이상 나눌 수 없는 언어 요소 | . 텍스트를 토큰으로 나누는 과정 | . &#53076;&#46300; 2-1 . 아래의 코드는 텍스트 처리 분야에 널리 사용되는 패키지인 spacy, NLTK 의 예시이다. | . import spacy . nlp = spacy.load(&quot;en_core_web_sm&quot;) text = &quot;Mary, don&#39;t slap the green witch&quot; print( [str(token) for token in nlp(text.lower())] ) . [&#39;mary&#39;, &#39;,&#39;, &#39;do&#39;, &#34;n&#39;t&#34;, &#39;slap&#39;, &#39;the&#39;, &#39;green&#39;, &#39;witch&#39;] . from nltk.tokenize import TweetTokenizer . tweet = u&quot;Snow White and the Seven Degrees #MakeaMoviecold@midnight:-)&quot; . tokenizer = TweetTokenizer() . spacy 와 nltk 비교 | . print( tokenizer.tokenize(tweet.lower())) . [&#39;snow&#39;, &#39;white&#39;, &#39;and&#39;, &#39;the&#39;, &#39;seven&#39;, &#39;degrees&#39;, &#39;#makeamoviecold&#39;, &#39;@midnight&#39;, &#39;:-)&#39;] . print( [str(token) for token in nlp(tweet.lower())] ) . [&#39;snow&#39;, &#39;white&#39;, &#39;and&#39;, &#39;the&#39;, &#39;seven&#39;, &#39;degrees&#39;, &#39;#&#39;, &#39;makeamoviecold@midnight:-&#39;, &#39;)&#39;] . &#53440;&#51077; . 말뭉치에 등장하는 고유한 토큰 | . 말뭉치에 있는 모든 타입의 집합이 어휘 사전 또는 어휘(lexicon)이다. | . $ divideontimes$ 특성 공학 : 언어학을 이해하고 NLP 문제 해결에 적용하는 과정 . N-gram . 텍스트에 있는 고정 길이 (n)의 연속된 토큰 시퀀스이다. | . &#53076;&#46300; 2-2 . def n_gram(text,n) : return [ text [i:i+n] for i in range(len(text)-n+1)] . cleaned = [&quot;marry&quot;, &quot;,&quot;,&quot;n&#39;t&quot;, &quot;slap&quot;, &quot;green&quot;, &quot;witch&quot;,&quot;,&quot;] . print(n_gram(cleaned,3)) . [[&#39;marry&#39;, &#39;,&#39;, &#34;n&#39;t&#34;], [&#39;,&#39;, &#34;n&#39;t&#34;, &#39;slap&#39;], [&#34;n&#39;t&#34;, &#39;slap&#39;, &#39;green&#39;], [&#39;slap&#39;, &#39;green&#39;, &#39;witch&#39;], [&#39;green&#39;, &#39;witch&#39;, &#39;,&#39;]] . 부분 단어 자체가 유용한 정보를 전달한다면 문자 $n-gram$을 생성할 수 있음 | . example :methanol의 methan 탄소 화합물, 접미사 -ol 은 알코올 종류를 나타낸다. 이 같은 경우 2-gram 으로 생각할 수 있지만 유기 화합물 이름을 구분하는 작업에서는 토큰 하나로 취급할 수 있다. . &#54364;&#51228;&#50612;&#50752; &#50612;&#44036; . &#54364;&#51228;&#50612; . 표제어 : 단어의 기본형 또는 사전에 등재된 단어 fly : flow, flew, flies, flown, flowing 의 표제어 | 토큰을 표제어로 바꾸어 벡터 표현의 차원을 줄이는 방법도 종종 도움이 된다. | . | . spacy는 사전에 정의된 WordNet 사전을 이용해 표제어를 추출한다. | . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) doc = nlp(&quot;he was running late&quot;) for token in doc : print (&quot;{} --&gt; {}&quot;.format(token, token.lemma_)) . he --&gt; -PRON- was --&gt; be running --&gt; run late --&gt; late . &#50612;&#44036;(Stemming) . 어형변화의 기초가 되는 부분 | . Porter와 Snowball어간 추출기가 유명하다. | . &#53076;&#46300; 2-3 . import nltk from nltk.stem.porter import * . stemmer = PorterStemmer() tokens = [&#39;compute&#39;, &#39;computer&#39;, &#39;computed&#39;, &#39;computing&#39;] for token in tokens: print(token + &#39; --&gt; &#39; + stemmer.stem(token)) . compute --&gt; comput computer --&gt; comput computed --&gt; comput computing --&gt; comput . &#54408;&#49324; &#53468;&#44613; . &#53076;&#46300; 2-4 . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) doc = nlp(u&quot;Mary, don&#39;t slap the green witch&quot;) for token in doc : print (&quot;{} --&gt; {}&quot;.format(token, token.pos_)) . Mary --&gt; PROPN , --&gt; PUNCT do --&gt; VERB n&#39;t --&gt; ADV slap --&gt; VERB the --&gt; DET green --&gt; ADJ witch --&gt; NOUN . &#52397;&#53356;&#45208;&#45572;&#44592; = &#48512;&#48516; &#44396;&#47928; &#48516;&#49437; . Chunking, shallow parsing | . 청크 : 하나의 의미가 있는 말 덩어리 (여러 토큰들이 모여 청크가 될 수 있다고 생각하자) | . 연속된 여러 토큰으로 구분되는 텍스트 구에 레이블을 할당하는 작업 | . 다음은 명사구(NP) 부분 구문 분석결과이다. | . &#53076;&#46300; 2-5 . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) doc = nlp(u&quot;Marry slapped the green witch.&quot;) for chunk in doc.noun_chunks : print (&quot;{} --&gt; {}&quot;.format(chunk, chunk.label_)) . Marry --&gt; NP the green witch --&gt; NP .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2021/01/04/Chapter-2.-NLP-%EA%B8%B0%EC%88%A0-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%ED%9B%91%EC%96%B4%EB%B3%B4%EA%B8%B0.html",
            "relUrl": "/python/2021/01/04/Chapter-2.-NLP-%EA%B8%B0%EC%88%A0-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%ED%9B%91%EC%96%B4%EB%B3%B4%EA%B8%B0.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Chapter 1. Introduction",
            "content": "NLP . Natrual Language Processing | . 언어학 지식에 상관없이 텍스트를 &quot;이해&quot; 하는 통계적인 방법을 사용해 실전 문제를 해결하는 일련의 기술 | . 텍스트 마이닝과의 차이 비교가 현재는 거의 무의미 하지만 아래와 같은 관심 중점에 차이가있다 | . 1 텍스트 마이닝 : 텍스트 데이터로 부터 유의미한 정보를 추출 . 2 NLP : 기계가 자연어를 이해할 수 있고 처리할 수 있도록 하는일을 의미한다. . 여기서 이해라는 것은 주로 텍스트를 계산 가능한 &quot;표현(representation)&quot; 으로 변환함으로써 이루어짐 | . 표현 : 벡터, 텐서, 그래프, 트리 같이 이산적이거나 연속적으로 조합한 구조이다. | . 이 책에서는 딥러닝과 NLP을 배운다. | . Deep learning . 개념 : 계산 그래프와 수치 최적화 기술을 사용해 데이터에서 표현을 효과적으로 학습하는 기술 | . Supervised Learning . 목적 : 주어진 데이터 셋에서 손실 함수를 최소화하는 파라미터 값을 고르는 것 | . 손실함수 : $L( hat y, ,y)$ | . &#44221;&#49324;&#54616;&#44053;&#48277; . Gradient descent : 식의 근을 찾는 일반적인 방법 | . &#51204;&#53685;&#51201;&#51064; &#44221;&#49324; &#54616;&#44053;&#48277; . = batct gradient descent | . 오차를 구할 때 전체 데이터 셋을 고려 $ to$ 한 번에 에포크에서 모든 매개변수에 업데이트를 단 한번 수행 | . 파라미터의 초깃값을 추측한 다음 목적 함수(= 손실 함수)의 값이 수용할만한 임계점(수렴 조건) 아래로 내려갈 때 까지 파라미터를 반복해서 업데이트를 한다. | . 데이터 셋이 클 경우 메모리 제약 $ to$ 계산 비용이 높아 매우 느림 | . &#54869;&#47456;&#51201; &#44221;&#49324;&#54616;&#44053;&#48277; . Stochastic gradient descent(SGD) | . 데이터 포인트를 하나 또는 일부 랜덤하게 선택하여 그래디언트를 계산한다. | . 데이터 포인트를 하나 사용하는 방법은 순수 SGD, 두 개 이상 사용하는 방법은 미니배치 SGD 라고 부른다. | . 순수 SGD : 업데이트에 잡음이 많아 수렴이 매우 느려 실전에서는 거의 사용하지 않음 | . &#50669;&#51204;&#54028; . Backpropagation | . 파라미터를 반복적으로 업데이트 하는 과정 | . 단계 = 정방향 계산 (forward pass) + 역방향 계산 (backward pass) | . 정방향 계산 : 현재 파라미터값으로 입력을 받아 평가하여 손실함수를 계산 | . 역방향 계산 : 손실의 그래이디언트를 사용하여 파라미터를 업데이트 함. | . &#50896;-&#54635; &#54364;&#54788; . &#49324;&#51060;&#53431;&#47088;&#51012; &#49324;&#50857;&#54616;&#50668; &#50896;-&#54635; &#48289;&#53552; &#46608;&#45716; &#51060;&#51652; &#54364;&#54788; &#47564;&#46308;&#44592; . from sklearn.feature_extraction.text import CountVectorizer import seaborn as sns import matplotlib.pyplot as plt . corpus = [&quot;Time flies like an arrow.&quot;,&quot;Fruit flies like a banana&quot;] . one_hot_vectorizer = CountVectorizer(binary=True) one_hot = one_hot_vectorizer.fit_transform(corpus).toarray() . vocab = one_hot_vectorizer.get_feature_names() vocab . [&#39;an&#39;, &#39;arrow&#39;, &#39;banana&#39;, &#39;flies&#39;, &#39;fruit&#39;, &#39;like&#39;, &#39;time&#39;] . sns.heatmap(one_hot, annot = True, cbar = False, xticklabels = vocab, yticklabels = [&quot;Sentence1&quot;, &quot;Setence2&quot;]) . &lt;AxesSubplot:&gt; . TF-IDF &#54364;&#54788; . $$TF(w) = Frequency , , of , ,word$$ . $$IDF(w) = log frac {N}{n_w}$$ . $$N : total , document quad n_w : number ,of , documnet , that , include ,word $$ . $TF$ : $i$번째 문서에서 $j$ 번째 단어의 출현 횟수 | . $IDF$ : 역문서 빈도, 단어 $w$가 출현한 역문서 빈도 | . 사이킷런의 TfidfVectorizer 클래스에서는 단어를 모두 가상의 문서가 있는 것처럼 로그 안의 분모와 분자에 1을 더해서 분모가 0이 되는 상활을 방지함 또한 마지막에 1을 더해 모든 문서에 포함된 단어가 있을 때 IDF 가 0이 되지 않도록함 | . | . $$IDF(w) = log left( frac {N+1}{n_w+1} right ) + 1$$ . 만약 특허와 관련된 문서 묶음이 있다고 가정해보자. 그러면 문서 대부분에 [&quot;Claim&quot;, &quot;System&quot;, &quot;method&quot;] 와 같은 단어가 여러번 반복해서 나온다. | . 이런 흔한 단어는 사실 특정 특허와 관련한 어떤 정보도 담겨있지 않는다. | . 반대로 &quot;tetrafluoroethylene&quot;(테트라플루오로에틸랜) 과 같이 희귀한 단어는 자주 나오지 않지만 특허 문서의 특징을 잘나타냄 | . $TF(w) times IDF(w)$이다. 즉 모든 문서에 등장하는 매우 흔한 단어는 TF-IDF 점수가 0이된다. | . 이처럼 아무런 정보없이 무분별하게 여러번 등장하는 단어들에 패널티를 주기위한 기법이다. | . &#49324;&#51060;&#53431;&#47088;&#51012; &#49324;&#50857;&#54644; TF-IDF &#54364;&#54788; &#47564;&#46308;&#44592; . from sklearn.feature_extraction.text import TfidfVectorizer import seaborn as sns . tfidf_vectorizer = TfidfVectorizer() tfidf =tfidf_vectorizer.fit_transform(corpus).toarray() tfidf . array([[0.49922133, 0.49922133, 0. , 0.35520009, 0. , 0.35520009, 0.49922133], [0. , 0. , 0.57615236, 0.40993715, 0.57615236, 0.40993715, 0. ]]) . sns.heatmap(tfidf,annot=True,xticklabels=vocab,cbar=False, yticklabels=[&quot;Sentence 1&quot;, &quot;Sentence 2&quot;]) . &lt;AxesSubplot:&gt; . &#54028;&#51060;&#53664;&#52824; &#44592;&#52488; . 오픈 소스로 씨아노, 카페, 텐서플로와 달리 파이토치는 테이프 기반 자동 미분 방식을 구현한다. | . 위 방식은 계산 그래프를 동적으로 정의하고 실행할 수 있다. 또한 디버깅이 아주 편리하며 복잡한 모델을 손쉽게 만들게 해줌 | . 텐서 : 다차원 데이터를 담은 수학객체 | . &#53584;&#49436; &#47564;&#46308;&#44592; . 먼저 헬퍼 함수 describe(x)를 정의하여 텐서 타입, 차원, 값 같은 텐서의 속성을 출력 | . def describe(x) : print(&quot;타입 : {}&quot;.format(x.type())) print(&quot;크기 : {}&quot;.format(x.shape)) print(&quot;값 : n{} n n n================================================ n n&quot;.format(x)) . import torch . 차원을 지정 후 텐서를 랜덤하게 초기화하는 방법 | . describe(torch.Tensor(2,3)) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0.2582, 0.4032, 0.1510], [0.8588, 0.2966, 0.9982]]) ================================================ . &#44512;&#46321;&#48516;&#54252;&#50752; &#54364;&#51456;&#51221;&#44508;&#48516;&#54252; . describe(torch.rand(2,3)) ## 균등 분포 describe(torch.randn(2,3)) ## 표준 정규 분포 . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0.6322, 0.4141, 0.1253], [0.5149, 0.2065, 0.5259]]) ================================================ 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[-0.5282, -0.1808, 1.0561], [ 0.6534, -0.0688, 1.0356]]) ================================================ . &#46041;&#51068;&#54620; &#49828;&#52860;&#46972;&#44050;&#51004;&#47196; &#52292;&#50868; &#53584;&#49436;&#47484; &#47564;&#46308;&#44592; . 내장함수로 0또는 1로 채운 텐서 생성 | . describe(torch.zeros(2,3)) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0., 0., 0.], [0., 0., 0.]]) ================================================ . x = torch.ones(2,3) describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 1., 1.], [1., 1., 1.]]) ================================================ . &#51064;-&#54540;&#47112;&#51060;&#49828; &#47700;&#49436;&#46300; . 인-플레이스 메서드 : 데이터를 직접 변경하는 것 | . 아래와 같이 fill_() 메서드와 비슷하게 랜덤 샘플링에 유용한 인-플레이스 메서드가 잇다. | . x.fill_(5) . tensor([[5., 5., 5.], [5., 5., 5.]]) . describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[5., 5., 5.], [5., 5., 5.]]) ================================================ . 새로운 객체를 생성하지 않고 바로 값을 변환하는데 객체 지향적인 관점에서 별로 좋지는 않은 것 같다. | . y = torch.Tensor(3,2) . y . tensor([[4.4155e-05, 2.1259e+20], [8.1714e+20, 6.7875e-07], [2.5668e-09, 4.1537e-08]]) . y.uniform_() . tensor([[0.7345, 0.1133], [0.0873, 0.7492], [0.3604, 0.2273]]) . y . tensor([[0.7345, 0.1133], [0.0873, 0.7492], [0.3604, 0.2273]]) . y.normal_() . tensor([[ 0.3838, -0.6120], [-1.3810, 0.8169], [-0.6775, -1.2050]]) . y . tensor([[ 0.3838, -0.6120], [-1.3810, 0.8169], [-0.6775, -1.2050]]) . &#47532;&#49828;&#53944;&#47196; &#53584;&#49436;&#47484; &#47564;&#46308;&#44256; &#52488;&#44592;&#54868; . x = torch.Tensor([[1,2,3], [4,5,6]]) . describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 2., 3.], [4., 5., 6.]]) ================================================ . 위 처럼 값을 리스트나. 넘파이 배열로 전달하여 텐서를 생성하고 초기화가 가능하다. | . 또한 언제든지 파이토치 텐서를 넘파이 배열로 바꿀 수 있음 | . 단 넘파이 배열을 사용하면 텐서 타입이, FloatTensor가 아니라 DoubleTensor가 된다. | . import numpy as np . npy = np.random.rand(2,3) . describe(torch.from_numpy(npy)) . 타입 : torch.DoubleTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0.9967, 0.8784, 0.1933], [0.6447, 0.7438, 0.1425]], dtype=torch.float64) ================================================ . 넘파이 배열과 파이토치텐서 사이를 변환하는 기능은 넘파이 포맷의 수치데이터를사용하는 래거시(legacy) 라이브러리를 사용할 때 중요하다. | . &#53440;&#51077;&#51012; &#52488;&#44592;&#54868; &#54616;&#50668; &#53584;&#49436; &#49373;&#49457; . x = torch.FloatTensor([[1,2,3], [4,5,6]]) describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 2., 3.], [4., 5., 6.]]) ================================================ . x = x.long() describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1, 2, 3], [4, 5, 6]]) ================================================ . x = torch.tensor([[1,2,3], [4,5,6]], dtype=torch.int64) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1, 2, 3], [4, 5, 6]]) ================================================ . x = x.float() describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 2., 3.], [4., 5., 6.]]) ================================================ . &#53584;&#49436; &#50672;&#49328; . &#45927;&#49480; . import torch . x = torch.randn(2,3) describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[ 0.0715, -0.2960, -0.5417], [ 0.3304, 1.5357, -1.1040]]) ================================================ . describe(torch.add(x,x)) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[ 0.1431, -0.5919, -1.0834], [ 0.6609, 3.0715, -2.2080]]) ================================================ . x = torch.arange(6) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([6]) 값 : tensor([0, 1, 2, 3, 4, 5]) ================================================ . view() 메서드는 동일한 데이터를 공유하는 새로운 텐서를 만든다. data_ptr() 매서드를 사용하면 원본 텐서와 뷰 텐서가 같은 저장 위치를 가리키고 있다는 것을 확인할 수 있다. | . x1 = x.view(2,3) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5]]) ================================================ . x.data_ptr() . 2251512245888 . x1.data_ptr() . 2251512245888 . 0 으로 지정시 colsum 반환, 1로 지정시 rowsum을 반환 | . describe(torch.sum(x,dim=0)) . 타입 : torch.LongTensor 크기 : torch.Size([3]) 값 : tensor([3, 5, 7]) ================================================ . describe(torch.sum(x,dim=1)) . 타입 : torch.LongTensor 크기 : torch.Size([2]) 값 : tensor([ 3, 12]) ================================================ . transpose함수는 두 번째와 세 번째 매개변수로 전달된 차원을 전치한 텐서를 만든다. | . x . tensor([[0, 1, 2], [3, 4, 5]]) . describe(torch.transpose(x,0,1)) . 타입 : torch.LongTensor 크기 : torch.Size([3, 2]) 값 : tensor([[0, 3], [1, 4], [2, 5]]) ================================================ . &#51064;&#45937;&#49905;, &#49836;&#46972;&#51060;&#49905;, &#50672;&#44208; . &#53584;&#49436; &#49836;&#46972;&#51060;&#49905;&#44284; &#51064;&#45937;&#49905; . import torch x = torch.arange(6).view(2,3) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5]]) ================================================ . describe(x[:1,:2]) . 타입 : torch.LongTensor 크기 : torch.Size([1, 2]) 값 : tensor([[0, 1]]) ================================================ . describe(x[0,1]) . 타입 : torch.LongTensor 크기 : torch.Size([]) 값 : 1 ================================================ . &#48373;&#51105;&#54620; &#51064;&#45937;&#49905;, &#50672;&#49549;&#51201;&#51060;&#51648; &#50506;&#51008; &#53584;&#49436; &#51064;&#45937;&#49828; &#52280;&#51312;&#54616;&#44592; . x . tensor([[0, 1, 2], [3, 4, 5]]) . 아래 코드를 해석하면 컬럼차원에서 0,2 번째 컬럼을 반환한다는 뜻이다. | . indices = torch.LongTensor([0,2]) describe(torch.index_select(x,dim=1,index = indices)) . 타입 : torch.LongTensor 크기 : torch.Size([2, 2]) 값 : tensor([[0, 2], [3, 5]]) ================================================ . indices = torch.LongTensor([0,0]) describe(torch.index_select(x,dim=0,index = indices)) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [0, 1, 2]]) ================================================ . 인덱스가 LongTensor 라는 점에 주목하자. 파이토치 함수를 사용할 때 필수 조건이다. | . x . tensor([[0, 1, 2], [3, 4, 5]]) . row_indices = torch.arange(2).long() ## 로우나 컬럼이나 같은 표현이다. col_indices = torch.LongTensor([0,1]) describe(x[row_indices,col_indices]) . 타입 : torch.LongTensor 크기 : torch.Size([2]) 값 : tensor([0, 4]) ================================================ . &#53584;&#49436; &#50672;&#44208; . import torch . x = torch.arange(6).view(2,3) . describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5]]) ================================================ . cat, dim=0 $ to$ r의 cbind | . describe(torch.cat([x,x],dim=0)) ## . 타입 : torch.LongTensor 크기 : torch.Size([4, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5], [0, 1, 2], [3, 4, 5]]) ================================================ . describe(torch.cat([x,x],dim=1)) ## . 타입 : torch.LongTensor 크기 : torch.Size([2, 6]) 값 : tensor([[0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5]]) ================================================ . describe(torch.stack([x,x])) ## . 타입 : torch.LongTensor 크기 : torch.Size([2, 2, 3]) 값 : tensor([[[0, 1, 2], [3, 4, 5]], [[0, 1, 2], [3, 4, 5]]]) ================================================ . &#53584;&#49436;&#51032; &#49440;&#54805; &#45824;&#49688; &#44228;&#49328; : &#54665;&#47148; &#44273;&#49480; . import torch . x1 = torch.arange(6.).view(2,3) describe(x1) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0., 1., 2.], [3., 4., 5.]]) ================================================ . x2 = torch.ones(3,2) x2 . tensor([[1., 1.], [1., 1.], [1., 1.]]) . x2 [:,1] +=1 . describe(x2) . 타입 : torch.FloatTensor 크기 : torch.Size([3, 2]) 값 : tensor([[1., 2.], [1., 2.], [1., 2.]]) ================================================ . 행렬 곱셈 | . torch.mm(x1,x2) . tensor([[ 3., 6.], [12., 24.]]) . 역행렬 | . torch.inverse(torch.rand(2,2)) . tensor([[ 1.1645, -2.5808], [-0.0280, 3.1795]]) . 대각합 | . torch.trace(torch.rand(2,2)) . tensor(0.8843) . &#53584;&#49436;&#50752; &#44228;&#49328;&#44536;&#47000;&#54532; . 그래이디언트 연산을 할 수 있는 텐서 만들기 | . requires_grad = True는 그레이디언트 기반 학습에 필요한 손실함수와 텐서의 그레이디언트를 기록하는 부가 연산을 활성화 시킨다. . import torch . x = torch.ones(2,2, requires_grad=True) . describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 2]) 값 : tensor([[1., 1.], [1., 1.]], requires_grad=True) ================================================ . print(x.grad is None) . True . $$ y = (x+2)(x+5) +3$$ . y = (x+2)*(x+5) + 3 describe(y) print(x.grad is None) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 2]) 값 : tensor([[21., 21.], [21., 21.]], grad_fn=&lt;AddBackward0&gt;) ================================================ True . z = y.mean() describe(z) z.backward() print(x.grad is None) . 타입 : torch.FloatTensor 크기 : torch.Size([]) 값 : 21.0 ================================================ False . 파이토치에서 계산 그래프에 있는 노드에 대한 그레이디언트를 .grad 속성으로 참조할 수 있다. 옵티마이저는 .grad 속성을 사용해서 파라미터 값을 업데이트한다. | . &#50672;&#49845;&#47928;&#51228; . 1&#48264; . 2D 텐서를 만들고 차원 0 위치에 크기가 1인 차원을 추가하시오. . Solution . a = torch.rand(3,3) a . tensor([[0.2966, 0.4459, 0.0016], [0.2637, 0.7056, 0.2209], [0.2585, 0.7810, 0.2748]]) . a.unsqueeze(0) . tensor([[[0.2966, 0.4459, 0.0016], [0.2637, 0.7056, 0.2209], [0.2585, 0.7810, 0.2748]]]) . 2&#48264; . 이전 텐서에 추가한 차원을 삭제하기 . Solution . a.squeeze(0) . tensor([[0.2966, 0.4459, 0.0016], [0.2637, 0.7056, 0.2209], [0.2585, 0.7810, 0.2748]]) . 3&#48264; . 범위가 [3,7) 이고 크기가 $5 times 3$인 램덤판 텐서를 만들기 | . Solution . 3+torch.rand(5,3)*(7-3) . tensor([[4.8969, 5.3626, 4.2073], [5.1916, 6.7691, 3.1864], [6.1773, 6.1206, 5.6650], [4.0778, 6.3007, 6.8788], [5.8507, 4.0870, 5.9019]]) . 4&#48264; . Solution . 정규분포를 사용해서 텐서를 만들기 . a= torch.rand(3,3) a.normal_() . tensor([[ 0.3830, -0.2555, -0.2903], [ 1.0973, 0.3475, -0.5337], [-0.2465, 0.9446, -0.4641]]) . 5&#48264; . 텐서 torch.Tensor([1,1,1,0,1]) 에서 0이 아닌 원소의 인덱스를 추출하세요. . Solution . a = torch.Tensor([1,1,1,0,1]) torch.nonzero(a,as_tuple=True) . (tensor([0, 1, 2, 4]),) . as_tuple : True 로 지정하지 않으면 2차원 텐서를 변환한다. | . 6&#48264; . 크기가 (3,1)인 랜덤한 텐서를 만들고 네 벌을 복사해 쌓으세요 . Solution . a = torch.rand(3,1) a . tensor([[0.1492], [0.1221], [0.5768]]) . a.expand(3,4) . tensor([[0.1492, 0.1492, 0.1492, 0.1492], [0.1221, 0.1221, 0.1221, 0.1221], [0.5768, 0.5768, 0.5768, 0.5768]]) . 7&#48264; . 2차원 행렬 개 ( a = torch.rand(3,4,5), b = torch.rand(3,5,4) ) 의 배치 행렬 곱셈을 계산하세요. . Solution . a = torch.rand(3,4,5) b = torch.rand(3,5,4) . a . tensor([[[0.8432, 0.3043, 0.3210, 0.9431, 0.2673], [0.7349, 0.5522, 0.5258, 0.0104, 0.3234], [0.4519, 0.3940, 0.0187, 0.3106, 0.2780], [0.4799, 0.3480, 0.6322, 0.1634, 0.7024]], [[0.7353, 0.2029, 0.0166, 0.0391, 0.7469], [0.0473, 0.0727, 0.1912, 0.2763, 0.8238], [0.8702, 0.6047, 0.2439, 0.3102, 0.2967], [0.5944, 0.9755, 0.8162, 0.3612, 0.4574]], [[0.7215, 0.0704, 0.7937, 0.7839, 0.7022], [0.3435, 0.1463, 0.8671, 0.2648, 0.7312], [0.7399, 0.3415, 0.9808, 0.7982, 0.8345], [0.1444, 0.1740, 0.1711, 0.0673, 0.3027]]]) . b . tensor([[[0.2951, 0.1355, 0.5337, 0.1931], [0.1613, 0.7947, 0.6819, 0.7209], [0.1560, 0.1124, 0.6236, 0.7558], [0.6739, 0.1252, 0.2276, 0.1586], [0.2627, 0.1673, 0.3396, 0.2907]], [[0.6307, 0.3764, 0.1830, 0.0020], [0.3913, 0.3344, 0.9843, 0.3801], [0.7788, 0.5737, 0.5411, 0.5289], [0.7318, 0.4864, 0.8927, 0.6358], [0.8435, 0.0820, 0.3751, 0.3977]], [[0.5797, 0.1567, 0.3288, 0.7492], [0.0626, 0.7531, 0.8028, 0.4698], [0.3778, 0.8844, 0.9128, 0.1916], [0.9723, 0.2381, 0.1718, 0.3828], [0.0870, 0.5339, 0.6326, 0.3959]]]) . torch.bmm(a,b) . tensor([[[1.0538, 0.5550, 1.1631, 0.8521], [0.4799, 0.6529, 1.2088, 1.0330], [0.4821, 0.4619, 0.6866, 0.5155], [0.5910, 0.5506, 1.1633, 1.0515]], [[1.2148, 0.4345, 0.6584, 0.4093], [1.1042, 0.3537, 0.7393, 0.6321], [1.4528, 0.8450, 1.2747, 0.6757], [2.0424, 1.2314, 2.0047, 1.2151]], [[1.5457, 1.4296, 1.5972, 1.3038], [0.8569, 1.3843, 1.5300, 0.8831], [1.6695, 1.8762, 2.0778, 1.5386], [0.2510, 0.4826, 0.5464, 0.3683]]]) .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2021/01/03/Chapter-1.Introduction.html",
            "relUrl": "/python/2021/01/03/Chapter-1.Introduction.html",
            "date": " • Jan 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "github . github . soundcloud . C.I.C .",
          "url": "https://gangcheol.github.io/nlp-with-pytroch/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gangcheol.github.io/nlp-with-pytroch/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}