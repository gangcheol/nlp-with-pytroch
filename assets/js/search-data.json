{
  
    
        "post0": {
            "title": "Title",
            "content": "from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . cd /content/drive/MyDrive/Colab Notebooks . /content/drive/MyDrive/Colab Notebooks . import torch torch.cuda.is_available() . True . import numpy as np import sys sys.path.append(&#39;..&#39;) from utils import train def main(): data = np.load(&#39;data.npy&#39;) unigram_distribution = np.load(&#39;unigram_distribution.npy&#39;) word_vectors = np.load(&#39;word_vectors.npy&#39;) doc_weights_init = np.load(&#39;doc_weights_init.npy&#39;) # transform to logits doc_weights_init = np.log(doc_weights_init + 1e-4) # make distribution softer temperature = 7.0 doc_weights_init /= temperature # if you want to train the model like in the original paper # set doc_weights_init=None train( data, unigram_distribution, word_vectors, doc_weights_init=None, n_topics=5, batch_size=1024*7, n_epochs=123, lambda_const=500.0, num_sampled=15, topics_weight_decay=1e-2, topics_lr=1e-3, doc_weights_lr=1e-3, word_vecs_lr=1e-3, save_every=20, grad_clip=5.0 ) . main() . number of documents: 559 number of windows: 33521 number of topics: 5 vocabulary size: 736 word embedding dim: 10 . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) /content/drive/MyDrive/Colab Notebooks/utils/lda2vec_loss.py:47: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_. init.normal(self.doc_weights.weight, std=DOC_WEIGHTS_INIT) . number of batches: 5 epoch 1 . 0%| | 0/5 [00:00&lt;?, ?it/s]/content/drive/MyDrive/Colab Notebooks/utils/lda2vec_loss.py:196: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. doc_probs = F.softmax(doc_weights) /content/drive/MyDrive/Colab Notebooks/utils/lda2vec_loss.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument. dirichlet_loss = (w*F.log_softmax(doc_weights).sum(1)).mean() 100%|██████████| 5/5 [00:00&lt;00:00, 8.19it/s] . 151.14 -3226.54 epoch 2 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.15it/s] . 148.29 -3226.76 epoch 3 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.51it/s] . 146.19 -3227.02 epoch 4 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.14it/s] . 143.82 -3227.30 epoch 5 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.00it/s] . 141.97 -3227.61 epoch 6 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.70it/s] . 139.94 -3227.94 epoch 7 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.00it/s] . 137.96 -3228.30 epoch 8 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.20it/s] . 136.14 -3228.68 epoch 9 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.57it/s] . 134.50 -3229.09 epoch 10 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.86it/s] . 132.65 -3229.53 epoch 11 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.33it/s] . 131.34 -3230.00 epoch 12 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.20it/s] . 129.84 -3230.49 epoch 13 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.89it/s] . 128.47 -3231.01 epoch 14 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.89it/s] . 127.08 -3231.56 epoch 15 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.06it/s] . 126.03 -3232.15 epoch 16 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.03it/s] . 124.82 -3232.77 epoch 17 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.95it/s] . 123.77 -3233.43 epoch 18 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.88it/s] . 122.75 -3234.12 epoch 19 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.87it/s] . 121.78 -3234.84 epoch 20 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.69it/s] . 120.79 -3235.61 saving! epoch 21 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.00it/s] . 120.05 -3236.41 epoch 22 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.87it/s] . 119.06 -3237.26 epoch 23 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.17it/s] . 118.25 -3238.15 epoch 24 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.13it/s] . 117.47 -3239.08 epoch 25 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.52it/s] . 116.64 -3240.04 epoch 26 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.82it/s] . 115.88 -3241.06 epoch 27 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.98it/s] . 115.28 -3242.12 epoch 28 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.26it/s] . 114.62 -3243.23 epoch 29 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.34it/s] . 114.02 -3244.39 epoch 30 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.92it/s] . 113.29 -3245.59 epoch 31 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.08it/s] . 112.70 -3246.85 epoch 32 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.14it/s] . 112.00 -3248.16 epoch 33 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.71it/s] . 111.60 -3249.51 epoch 34 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.19it/s] . 110.96 -3250.92 epoch 35 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.79it/s] . 110.42 -3252.38 epoch 36 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.19it/s] . 109.82 -3253.90 epoch 37 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.22it/s] . 109.29 -3255.47 epoch 38 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.04it/s] . 108.85 -3257.09 epoch 39 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.62it/s] . 108.34 -3258.78 epoch 40 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.11it/s] . 107.76 -3260.51 saving! epoch 41 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.72it/s] . 107.31 -3262.31 epoch 42 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.79it/s] . 106.80 -3264.17 epoch 43 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.08it/s] . 106.45 -3266.09 epoch 44 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.77it/s] . 105.82 -3268.06 epoch 45 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.01it/s] . 105.31 -3270.11 epoch 46 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.96it/s] . 104.72 -3272.21 epoch 47 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.88it/s] . 104.44 -3274.38 epoch 48 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.87it/s] . 103.87 -3276.62 epoch 49 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.79it/s] . 103.47 -3278.91 epoch 50 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.85it/s] . 102.95 -3281.28 epoch 51 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.48it/s] . 102.59 -3283.71 epoch 52 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.98it/s] . 101.99 -3286.21 epoch 53 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.85it/s] . 101.55 -3288.79 epoch 54 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.04it/s] . 100.98 -3291.43 epoch 55 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.80it/s] . 100.64 -3294.13 epoch 56 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.70it/s] . 100.14 -3296.90 epoch 57 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.70it/s] . 99.78 -3299.75 epoch 58 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.27it/s] . 99.17 -3302.67 epoch 59 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.88it/s] . 98.81 -3305.66 epoch 60 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.02it/s] . 98.29 -3308.72 saving! epoch 61 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.69it/s] . 97.77 -3311.86 epoch 62 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.65it/s] . 97.29 -3315.08 epoch 63 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.94it/s] . 96.81 -3318.37 epoch 64 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.12it/s] . 96.36 -3321.75 epoch 65 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.98it/s] . 95.71 -3325.19 epoch 66 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.92it/s] . 95.29 -3328.71 epoch 67 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.52it/s] . 94.78 -3332.31 epoch 68 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.96it/s] . 94.24 -3335.99 epoch 69 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.85it/s] . 93.79 -3339.75 epoch 70 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.78it/s] . 93.18 -3343.59 epoch 71 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.71it/s] . 92.68 -3347.50 epoch 72 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.83it/s] . 92.23 -3351.50 epoch 73 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.84it/s] . 91.57 -3355.58 epoch 74 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.94it/s] . 91.01 -3359.74 epoch 75 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.08it/s] . 90.51 -3364.00 epoch 76 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.00it/s] . 89.98 -3368.31 epoch 77 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.93it/s] . 89.55 -3372.71 epoch 78 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.17it/s] . 88.99 -3377.21 epoch 79 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.81it/s] . 88.60 -3381.79 epoch 80 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.03it/s] . 87.95 -3386.45 saving! epoch 81 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.28it/s] . 87.53 -3391.20 epoch 82 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.71it/s] . 86.81 -3396.03 epoch 83 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.93it/s] . 86.26 -3400.95 epoch 84 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.88it/s] . 85.81 -3405.95 epoch 85 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.84it/s] . 85.30 -3411.03 epoch 86 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.03it/s] . 84.79 -3416.20 epoch 87 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.12it/s] . 84.10 -3421.46 epoch 88 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.71it/s] . 83.52 -3426.81 epoch 89 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.70it/s] . 83.04 -3432.24 epoch 90 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.70it/s] . 82.39 -3437.77 epoch 91 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.53it/s] . 81.92 -3443.37 epoch 92 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.10it/s] . 81.37 -3449.06 epoch 93 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.82it/s] . 80.89 -3454.85 epoch 94 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.25it/s] . 80.36 -3460.71 epoch 95 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.02it/s] . 79.86 -3466.67 epoch 96 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.40it/s] . 79.34 -3472.71 epoch 97 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.55it/s] . 78.82 -3478.84 epoch 98 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.92it/s] . 78.14 -3485.06 epoch 99 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.85it/s] . 77.48 -3491.36 epoch 100 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.38it/s] . 77.15 -3497.77 saving! epoch 101 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.71it/s] . 76.59 -3504.24 epoch 102 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.92it/s] . 76.11 -3510.81 epoch 103 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.64it/s] . 75.58 -3517.46 epoch 104 . 100%|██████████| 5/5 [00:00&lt;00:00, 10.16it/s] . 74.92 -3524.20 epoch 105 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.73it/s] . 74.69 -3531.03 epoch 106 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.57it/s] . 74.14 -3537.95 epoch 107 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.12it/s] . 73.43 -3544.97 epoch 108 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.15it/s] . 73.09 -3552.04 epoch 109 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.32it/s] . 72.43 -3559.23 epoch 110 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.36it/s] . 72.02 -3566.48 epoch 111 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.68it/s] . 71.44 -3573.82 epoch 112 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.03it/s] . 70.99 -3581.26 epoch 113 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.73it/s] . 70.44 -3588.78 epoch 114 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.29it/s] . 70.02 -3596.39 epoch 115 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.39it/s] . 69.70 -3604.09 epoch 116 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.52it/s] . 69.08 -3611.86 epoch 117 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.77it/s] . 68.72 -3619.72 epoch 118 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.34it/s] . 68.30 -3627.67 epoch 119 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.83it/s] . 67.81 -3635.72 epoch 120 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.28it/s] . 67.18 -3643.82 saving! epoch 121 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.52it/s] . 66.69 -3652.02 epoch 122 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.34it/s] . 66.40 -3660.31 epoch 123 . 100%|██████████| 5/5 [00:00&lt;00:00, 9.68it/s] . 65.96 -3668.67 . .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/2022/01/19/modeltrain.html",
            "relUrl": "/2022/01/19/modeltrain.html",
            "date": " • Jan 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "드라이브 연결",
            "content": "from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . !pwd . /content . cd /content/drive/MyDrive/Colab Notebooks . /content/drive/MyDrive/Colab Notebooks . %load_ext autoreload %autoreload 2 . MIN_COUNTS = 20 ## 최소 단어 빈도 수 MAX_COUNTS = 1800 ## 최대 단어 빈도 수 MIN_LENGTH = 15 ## 문서가 최소한 가져야 하는 단어 개수 HALF_WINDOW_SIZE = 5 # 2*HALF_WINDOW_SIZE &lt; MIN_LENGTH . from sklearn.datasets import fetch_20newsgroups import numpy as np from tqdm import tqdm import spacy from gensim import corpora, models import random import sys sys.path.append(&#39;..&#39;) from utils import preprocess, get_windows . nlp = spacy.load(&#39;en&#39;) . dataset = fetch_20newsgroups(subset=&#39;all&#39;, remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)) docs = dataset[&#39;data&#39;] . docs = [(i, doc) for i, doc in enumerate(docs)] . &#49368;&#54540; 1000&#44060; &#52628;&#52636; . docs= random.sample(docs,k=1000) . np.save(&#39;docs.npy&#39;, docs) . docs[] . list . type(dataset) . sklearn.utils.Bunch . encoded_docs, decoder, word_counts = preprocess( docs, nlp, MIN_LENGTH, MIN_COUNTS, MAX_COUNTS ) . 100%|██████████| 1000/1000 [00:42&lt;00:00, 23.57it/s] . number of removed short documents: 214 total number of tokens: 72916 number of tokens to be removed: 37125 number of additionally removed short documents: 227 total number of tokens: 33521 minimum word count number: 14 this number can be less than MIN_COUNTS because of document removal . . # create a way of restoring initial ids: doc_decoder = {i: doc_id for i, (doc_id, doc) in enumerate(encoded_docs)} . data = [] # new ids are created here for index, (_, doc) in tqdm(enumerate(encoded_docs)): windows = get_windows(doc, HALF_WINDOW_SIZE) # index represents id of a document, # windows is a list of (word, window around this word), # where word is in the document data += [[index, w[0]] + w[1] for w in windows] data = np.array(data, dtype=&#39;int64&#39;) . 559it [00:00, 12470.23it/s] . # id of a document, id of a word in this document, a window around this word # 1 + 1 + 10 data.shape[1] . 12 . data.shape[0] . 33521 . word_counts = np.array(word_counts) unigram_distribution = word_counts/sum(word_counts) . %%time vocab_size = len(decoder) embedding_dim = 10 # train a skip-gram word2vec model texts = [[str(j) for j in doc] for i, doc in encoded_docs] model = models.Word2Vec(texts, size=embedding_dim, window=5, workers=4, sg=1, negative=15, iter=70) model.init_sims(replace=True) word_vectors = np.zeros((vocab_size, embedding_dim)).astype(&#39;float32&#39;) for i in decoder: word_vectors[i] = model.wv[str(i)] . CPU times: user 40.5 s, sys: 124 ms, total: 40.7 s Wall time: 21.7 s . vocab_size . 736 . word_vectors.shape . (736, 10) . texts = [[decoder[j] for j in doc] for i, doc in encoded_docs] dictionary = corpora.Dictionary(texts) corpus = [dictionary.doc2bow(text) for text in texts] . %%time n_topics = 5 lda = models.LdaModel(corpus, alpha=0.9, id2word=dictionary, num_topics=n_topics) corpus_lda = lda[corpus] . CPU times: user 975 ms, sys: 29.5 ms, total: 1 s Wall time: 985 ms . for i, topics in lda.show_topics(n_topics, formatted=False): print(&#39;topic&#39;, i, &#39;:&#39;, &#39; &#39;.join([t for t, _ in topics])) . topic 0 : use file like include need know good people mail year topic 1 : know people think use drive like year find time system topic 2 : system like time problem use work think know good run topic 3 : know time people use right think like way good need topic 4 : like know use problem file system good think find thing . doc_weights_init = np.zeros((len(corpus_lda), n_topics)) for i in tqdm(range(len(corpus_lda))): topics = corpus_lda[i] for j, prob in topics: doc_weights_init[i, j] = prob . 100%|██████████| 559/559 [00:00&lt;00:00, 1118.44it/s] . np.save(&#39;data.npy&#39;, data) np.save(&#39;word_vectors.npy&#39;, word_vectors) np.save(&#39;unigram_distribution.npy&#39;, unigram_distribution) np.save(&#39;decoder.npy&#39;, decoder) np.save(&#39;doc_decoder.npy&#39;, doc_decoder) np.save(&#39;doc_weights_init.npy&#39;, doc_weights_init) . len(decoder) . 736 .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/2022/01/19/get_windows.html",
            "relUrl": "/2022/01/19/get_windows.html",
            "date": " • Jan 19, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Load data",
            "content": "from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import numpy as np from sklearn.datasets import fetch_20newsgroups import torch import matplotlib.pyplot as plt %matplotlib inline from MulticoreTSNE import MulticoreTSNE as TSNE . cd /content/drive/MyDrive/Colab Notebooks . /content/drive/MyDrive/Colab Notebooks . def softmax(x): # x has shape [batch_size, n_classes] e = np.exp(x) n = np.sum(e, 1, keepdims=True) return e/n . docs = np.load(&#39;docs.npy&#39;)[()] . decoder = np.load(&#39;decoder.npy&#39;,allow_pickle=True)[()] # for restoring document ids, &quot;id used while training -&gt; initial id&quot; doc_decoder = np.load(&#39;doc_decoder.npy&#39;,allow_pickle=True)[()] . dataset = fetch_20newsgroups(subset=&#39;all&#39;, remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)) . targets = dataset[&#39;target&#39;] target_names = dataset[&#39;target_names&#39;] targets = np.array([targets[doc_decoder[i]] for i in range(len(doc_decoder))]) . Load the trained model . state = torch.load(&#39;model_state.pytorch&#39;, map_location=lambda storage, loc: storage) n_topics = 5 doc_weights = state[&#39;doc_weights.weight&#39;].cpu().clone().numpy() topic_vectors = state[&#39;topics.topic_vectors&#39;].cpu().clone().numpy() resulted_word_vectors = state[&#39;neg.embedding.weight&#39;].cpu().clone().numpy() # 각 문서에서 토픽 확률을 저장한다. topic_dist = softmax(doc_weights) # 문서벡터 저장 doc_vecs = np.matmul(topic_dist, topic_vectors) . &#44033; &#53664;&#54589;&#50640;&#49436; &#46321;&#51109;&#54616;&#45716; &#45800;&#50612;&#46308;&#51012; &#48372;&#50668;&#51468; . similarity = np.matmul(topic_vectors, resulted_word_vectors.T) most_similar = similarity.argsort(axis=1)[:, -10:] for j in range(n_topics): topic_words = &#39; &#39;.join([decoder[i] for i in reversed(most_similar[j])]) print(&#39;topic&#39;, j + 1, &#39;:&#39;, topic_words) . topic 1 : want card know good like need drive ram disk sell topic 2 : know like think thing problem car good find get card topic 3 : double good know look run line like home time DES topic 4 : know good time like people think use year right come topic 5 : disk use know drive key file like need mode type . Show learned document embeddings . tsne = TSNE(perplexity=200, n_jobs=4) X = tsne.fit_transform(doc_vecs.astype(&#39;float64&#39;)) . def plot(X): # X has shape [n_documents, 2] plt.figure(figsize=(16, 9), dpi=120); cmap = plt.cm.tab20 number_of_targets = 20 for i in range(number_of_targets): label = target_names[i] size = 15.0 linewidths = 0.5 edgecolors = &#39;k&#39; color = cmap(i) if &#39;comp&#39; in label: marker = &#39;x&#39; elif &#39;sport&#39; in label: marker = &#39;s&#39; edgecolors = &#39;b&#39; elif &#39;politics&#39; in label: marker = &#39;o&#39; edgecolors = &#39;g&#39; elif &#39;religion&#39; in label: marker = &#39;P&#39; size = 17.0 elif &#39;sci&#39; in label: marker = &#39;o&#39; size = 14.0 edgecolors = &#39;k&#39; linewidths = 1.0 elif &#39;atheism&#39; in label: marker = &#39;P&#39; size = 18.0 edgecolors = &#39;r&#39; linewidths = 0.5 else: marker = &#39;v&#39; edgecolors = &#39;m&#39; plt.scatter( X[targets == i, 0], X[targets == i, 1], s=size, c=color, marker=marker, linewidths=linewidths, edgecolors=edgecolors, label=label ); leg = plt.legend() leg.get_frame().set_alpha(0.3) . plot(X) # learned document vectors # different colors and markers represent # ground truth labels of each document # open this image in new tab to see it better . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. . Show initial document weights (vanilla lda) . doc_weights_init = np.load(&#39;doc_weights_init.npy&#39;) . tsne = TSNE(perplexity=200, n_jobs=4) Y = tsne.fit_transform(doc_weights_init.astype(&#39;float64&#39;)) . # I run normal LDA and used output distributions over topics # of each document plot(Y) # distribution over the topics for each document (output of LDA) # different colors and markers represent # ground truth labels of each document # open this image in new tab to see it better . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. . Explore learned topic distributions . tsne = TSNE(perplexity=200, n_jobs=4) Z = tsne.fit_transform(topic_dist.astype(&#39;float64&#39;)) . plot(Z) # learned distribution over the topics for each document # these are topic assignments as on the plot above # but these ones are after the training of lda2vec # different colors and markers represent # ground truth labels of each document # open this image in new tab to see it better . *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* &amp; *y*. Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points. . dist = topic_dist.reshape(-1) plt.hist(dist[dist &gt; 0.01], bins=40); . dist.shape . (2795,) . topic_dist.shape . (559, 5) . plt.hist(topic_dist[:, 4], bins=40); . plt.scatter(topic_dist[:, 2], topic_dist[:, 4]); . corr = np.corrcoef(topic_dist.transpose(1, 0)) plt.imshow(corr); plt.colorbar(); . Show a document and its topics . i = 0 # document id print(&#39;DOCUMENT:&#39;) print([doc for j, doc in docs if int(j) == doc_decoder[i]][0], &#39; n&#39;) print(&#39;DISTRIBUTION OVER TOPICS:&#39;) s = &#39;&#39; for j, p in enumerate(topic_dist[i], 1): s += &#39;{0}:{1:.3f} &#39;.format(j, p) if j%6 == 0: s += &#39; n n&#39; print(s) print(&#39; n nTOP TOPICS:&#39;) for j in reversed(topic_dist[i].argsort()[-3:]): topic_words = &#39; &#39;.join([decoder[i] for i in reversed(most_similar[j])]) print(&#39;topic&#39;, j + 1, &#39;:&#39;, topic_words) . DOCUMENT: -- That means that there cannot be any atheists since there is NO WAY that you can prove that there is no god. Atheists are people who BELIEVE that there is no god, most not only believe, but also are damn sure that there isn&#39;t a god (like me). The Cursor, aka Nick Humphries, u2nmh@csc.liv.ac.uk, at your service. &#34;What&#39;s the use of computers? They&#39;ll never play | &#34;Why pay money to see chess, draw art or make music.&#34; - Jean Genet. | bad films? Stay home &#34;Intelligence isn&#39;t to make no mistakes, but how | and see bad TV for to make them look good.&#34; - Bertolt Brecht. | free.&#34; - Sam Goldwyn. DISTRIBUTION OVER TOPICS: 1:0.142 2:0.139 3:0.341 4:0.270 5:0.108 TOP TOPICS: topic 3 : double good know look run line like home time DES topic 4 : know good time like people think use year right come topic 1 : want card know good like need drive ram disk sell .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/2022/01/19/explore_trained_model.html",
            "relUrl": "/2022/01/19/explore_trained_model.html",
            "date": " • Jan 19, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Word Embedding",
            "content": "WordEmbedding . 1. 차원의 저주(Curse of Dimensionality) : 수학적 공간 차원(=변수 개수)이 늘어나면서, 문제 계산법이 지수적으로 커지는 상황 . 만약 $x=[1,2,3,4,5], , y= [0,0,0,0,0] to (X,Y)$ 을 표현한다고 하자 . 아래와 같이 1차원 상에서 표현되는 정보를 2차원 상에서 표현하게되어 설명 공간이 $5^2 =25$가 된 것이다. . 이러한 경우를 차원의 저주라고 하며 이는 모델링 과정에서 저장 공간과 처리 시간이 불필요하게 증가됨을 의미한다. . 이러한 문제점은 위와 같은 $(X,Y)$이산형 확률분포에서 결합분포를 구할 때 발생한다. . 또한 이산형 변수들이 다양한 값$(0,1,2 dots 145748)$을 가질 경우 같은 길이의 두 문자열에서 거리를 측정하는 척도인 &quot;해밍 거리&quot;의 값이 거의 최댓값에 이르게 된다. [1] . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . . fig, axes = plt.subplots(1,2, figsize=(10,5)) ax1, ax2 =axes x = [1,2,3,4,5] y = [0,0,0,0,0] ax1.plot(x,y) ax1.set_title(&quot;1-dimensional&quot;) ax1.axis(&quot;off&quot;) ax2.plot(x,y) ax2.set_title(&quot;2-dimensional&quot;) fig.tight_layout() . . 이러한 문제점을 해결하기 위해 NLP 분야에서는 단어를 저차원에 표현하기 위한 &quot;워드 임베딩(Word Embedding)&quot;을 제안하였다. . 기존의 통계적인 방법이 단어의 출현 빈도에 집중 한다면 워드 임베딩은 서로 유사한 단어들 간 유사성을 포착하는데 집중한다. . - 가정 : 유사한 의미를 가진 단어는 유사한 문맥안에서 발견된다. . - 가정의 해석 : 유사한 의미를 가진 단어들은 유사한 단어 벡터를 가진다. . - 이점 : 이웃된 단어들의 단어 벡터들을 학습하여 단어간 유사성을 도출해낼 수 있다. . example 1 :&#39;man&#39; + &#39;royal&#39; = &#39;king&#39; . 이제 이러한 임베딩 기법 중 하나인 &quot;Word2Vec&quot; 기법을 소개한다. [2] . Word2Vec . 워드 &quot;Word2Vec&quot; 중 가장 대표적인 방법으로 &quot;CBOW&quot;, &quot;skip-gram&quot; 이 존재한다. | . CBOW . 주변 단어를 이용하여 중심단어를 예측한다. | . 주어진 문맥에서 window size $k$ 를 적용해 target word 양옆에 $k$개의 단어들을 이용하여 조건부 확률을 계산한다. (편의상 k=1 이라고 설정) | . 프라닭 이라는 단어를 예측한다고 가정한다. | . 문장1: 금요일 밤에 프라닭은 못참지 . 문장2: 불금인데 교촌치킨 에 맥주? . 단어 : [ &quot;금요일&quot;, &quot;밤&quot;, &quot;프라닭&quot;,&quot;불금&quot;, &quot;교촌치킨&quot;, &quot;맥주&quot; ] $ to Word in R^{6 times 6}$ . 문장의 개수는 $j=2$, 단어의 개수는 총 $i=6$, 축소할 임베딩 차원의 개수는 $n=3$ 으로 설정하자. | . 차원축소를 위해 생성되는 임베딩(=가중치) 행렬 $W in R^{6 times 3}$ 으로 파이토치 기준 $N(0,1)$에서 생성된다. | . 목적 1 : $Word in R^{6 times 6} to W in R^{6 times 3}$ | . 목적 2 : 단어간 의미적 유사성을 포착하기 위한 임베딩 행렬 갱신 $W^{t} to W^{t+1} $ | . 1. one-hot vector $Word in R^{6 times 6}$ 생성 . import numpy as np import pandas as pd index = [ &quot;금요일&quot;, &quot;밤&quot;, &quot;프라닭&quot;,&quot;불금&quot;, &quot;교촌치킨&quot;, &quot;맥주&quot; ] word1 = [1,0,0,0,0,0] word2 = [0,1,0,0,0,0] word3 = [0,0,1,0,0,0] word4 = [0,0,0,1,0,0] word5 = [0,0,0,0,1,0] word6 = [0,0,0,0,0,1] one_hot = pd.DataFrame([word1,word2,word3,word4,word5,word6],index=index) one_hot . . 0 1 2 3 4 5 . 금요일 1 | 0 | 0 | 0 | 0 | 0 | . 밤 0 | 1 | 0 | 0 | 0 | 0 | . 프라닭 0 | 0 | 1 | 0 | 0 | 0 | . 불금 0 | 0 | 0 | 1 | 0 | 0 | . 교촌치킨 0 | 0 | 0 | 0 | 1 | 0 | . 맥주 0 | 0 | 0 | 0 | 0 | 1 | . 2. 임베딩(가중치) 행렬 생성 $W in R^{6 times 3}$ . W = np.random.normal(loc = 0, scale=1,size=18).reshape(6,3) W = pd.DataFrame(W,index=index, columns = [&quot;W1&quot;,&quot;W2&quot;,&quot;W3&quot;]) W . . W1 W2 W3 . 금요일 -0.946677 | -0.964799 | -2.236172 | . 밤 1.481341 | 0.678401 | -1.239748 | . 프라닭 -0.855941 | 0.556102 | 0.330505 | . 불금 0.316146 | -1.791996 | -0.307091 | . 교촌치킨 1.289018 | -1.415381 | 0.418707 | . 맥주 1.106920 | 0.051748 | 0.478479 | . 3. $ widehat W_{프라닭} = frac {W_{밤} + W_{불금}} {2} = [0.89,-0.55,0.77]$ . W_1 = list((W.loc[&quot;밤&quot;] + W.loc[&quot;불금&quot;])/2) W_1 . . [0.8987433793584608, -0.5567973759616802, -0.7734197905021276] . 4. $ Z = widehat W_{프라닭} times W^T = $ [ 1.42, 1.91, -1.33, 1.52, 1.62, 0.6 ] . z = np.dot(np.array(W_1),W.T.to_numpy()) z.round(2) . . array([ 1.42, 1.91, -1.33, 1.52, 1.62, 0.6 ]) . 5. $ hat y$ 계산 . $ hat y=softmax(Z) = [0.18, , ,0.30, , ,0.01, , ,0.20, , ,0.22, , ,0.08]$ . from scipy.special import softmax y=[0,0,1,0,0,0] so = pd.DataFrame({&quot;y_hat&quot;: softmax(z), &quot;y&quot; : y},index=index) so . . y_hat y . 금요일 0.182270 | 0 | . 밤 0.299486 | 0 | . 프라닭 0.011647 | 1 | . 불금 0.202155 | 0 | . 교촌치킨 0.224158 | 0 | . 맥주 0.080284 | 0 | . &#44032;&#51473;&#52824; &#54665;&#47148; &#44081;&#49888; . 위와 같은 과정을 모든 단어에 대해 수행하여 크로스 엔트로피 함수를 적용한 $loss$를 계산한다. . 1. $loss=- sum_{i=1}^6 y_i log p_i$ . 2.$loss$를 최소화 하는 최적의 파라미터 $ theta$를 구함 $ to frac { partial loss}{ partial p} $ . 3. example $W_{밤},W_{불금}$ 업데이트($ alpha $ : learning rate) . $W_{밤}^{t+1} = W_{밤}^t + left( , alpha , times theta , right)$ . $W_{불금}^{t+1} = W_{불금}^t + left( , alpha , times theta , right)$ . Summary . 중심단어 벡터 $W_c$가 있고, 주변 단어 벡터 $W_o$가 있다고 하자. | . $t+1$ 시점에서 $t$ 시점의 결과를 반영하여 단어벡터 $W_{o}$를 갱신한다. | . $W^{t+1}_{o} =W^{t}_{o} + alpha times [l( theta_{c})]$ | . 타겟단어 예측시 사용되는 수식 [4] | . $$P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{w=1}^W exp ,({v^{ prime}_w}^Tv_{w_{I}}) } $$ . $$I , ,: , ,Input, , ,O , ,: , , Output$$ . $$W : number , , , of , , , Word$$ . Skip-gram . Skip-gram의 경우 CBOW와 달리 중심단어를 가지고 주변단어를 예측하는 과정이다. | . 따라서 CBOW의 3번째 단계 window-size내의 주변 단어들의 합을 평균 내는 과정이 생략된다. | . 이러한 부분을 빼면 CBOW와 동일하다. | . Skip-gram과 CBOW의 경우 아래와 같은 수식을 최대화 하는 것을 목표로 한다. (베르누이분포의 MLE 를 생각해보장) [4] | . 베르누이분포의 MLE와 크로스 엔트로피 손실함수의 최소값 파라미터를 구하는 것이 동치라고 생각해보자.(이 부분 다시 증명) | . $$ frac 1T sum_{t=1}^T sum_{-c leq j leq c, j neq 0} log p left(w_{t+j}|w_t right)$$ . $T :$ number of traning word | . $c$ : the size of training context (=window) | . Negative Sampling . $$ P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{w=1}^W exp ,({v^{ prime}_w}^Tv_{w_{I}}) }$$ . 위와 같이 Word2Vec은 출력층이 내놓는 스코어 값에 소프트맥스 함수를 적용해 확률값으로 변환한 후 이를 정답과 비교해 &quot;역전파(backpropagation)&quot; 하는 구조이다. | . 그런데 소프트맥스를 적용하려면 분모에 해당하는 값, 즉 중심(=input) 단어와 나머지 모든 단어의 내적을 한 뒤 이를 다시 $ exp$ 취해줘야한다. | . 보통 전체 단어가 10만개 이상 주어지므로 계산량이 어마어마 해진다. | . Negative Sampling은 소프트맥스 확률을 구할 때 전체 단어를 구하지 않고, 일부 단어만 뽑아서 계산을 하자는 아이디어다.[3][4] | . 사용자가 지정한 윈도우 사이즈 내에 등장하지 않는 단어(Negative saple)을 5~20개 정도 뽑고, 이를 우리가 예측하고자 하는 타겟 단어와 합쳐 전체 단어처럼 소프트맥스 확률을 구하는 것이다. | . 보통 5~20 ($k$) 개 정도 뽑는데, 이는 만약 윈도우 사이즈가 5일 경우 최대 25개의 단어를 대상으로만 소프트맥스 확률을 계산하고, 파라미터 업데이트도 25개 대상으로만 이뤄진다는 이야기이다.[3][4] | . $$ P(w_O|w_I) = frac{ exp ,({v^{ prime}_{w_O}}^Tv_{w_{I}})} { sum_{i=1}^k exp ,({v^{ prime}_w}^Tv_{w_{I}}) }$$ . 특정 단어가 negative sampliing 될 확률 $P_n(w_i) $은 다음과 같다.(논문에선 정의한 수식은 첫번째인데....더 찾아보자) | . $$P_n(w_i)= frac {U(w_i)^{3/4}}{Z}$$ . 이건 구글링한 수식 | . $$ P_n(w_i)= frac {f(w_i)^{3/4}}{ sum_{j=0}^n f(w_j)^{3/4}}$$ . $$f(w_i) = (해당 , ,단어 , ,빈도 , , / , ,전체 , , 단어 , , 수)$$ . 참고문헌 . skip-gram : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/ . [1] : Bengio, Yoshua, et al. &quot;A neural probabilistic language model.&quot; Journal of machine learning research 3.Feb (2003): 1137-1155. . [2] : Young, Tom, et al. &quot;Recent trends in deep learning based natural language processing.&quot; ieee Computational intelligenCe magazine 13.3 (2018): 55-75. . [3] : Mikolov, Tomas, et al. &quot;Efficient estimation of word representations in vector space.&quot; arXiv preprint arXiv:1301.3781 (2013). . [4] : Mikolov, Tomas, et al. &quot;Distributed representations of words and phrases and their compositionality.&quot; Advances in neural information processing systems. 2013. .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2021/01/06/Word2Vec.html",
            "relUrl": "/python/2021/01/06/Word2Vec.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Chapter 2. NLP 기술 빠르게 훑어보기",
            "content": "&#47568;&#47945;&#52824;, &#53664;&#53360;, &#53440;&#51077; . &#47568;&#47945;&#52824; . 고전이나 현대의 모든 NLP 작업에서 쓰이는 text data 샘플 : metadata + text | 위 같은 샘플들이 모인 데이터 셋을 말뭉치라고 표현한다, | . | . &#53664;&#53360;&#54868; (Tokenization) . 토큰 : 문법적으로 더 이상 나눌 수 없는 언어 요소 | . 텍스트를 토큰으로 나누는 과정 | . &#53076;&#46300; 2-1 . 아래의 코드는 텍스트 처리 분야에 널리 사용되는 패키지인 spacy, NLTK 의 예시이다. | . import spacy . nlp = spacy.load(&quot;en_core_web_sm&quot;) text = &quot;Mary, don&#39;t slap the green witch&quot; print( [str(token) for token in nlp(text.lower())] ) . [&#39;mary&#39;, &#39;,&#39;, &#39;do&#39;, &#34;n&#39;t&#34;, &#39;slap&#39;, &#39;the&#39;, &#39;green&#39;, &#39;witch&#39;] . from nltk.tokenize import TweetTokenizer . tweet = u&quot;Snow White and the Seven Degrees #MakeaMoviecold@midnight:-)&quot; . tokenizer = TweetTokenizer() . spacy 와 nltk 비교 | . print( tokenizer.tokenize(tweet.lower())) . [&#39;snow&#39;, &#39;white&#39;, &#39;and&#39;, &#39;the&#39;, &#39;seven&#39;, &#39;degrees&#39;, &#39;#makeamoviecold&#39;, &#39;@midnight&#39;, &#39;:-)&#39;] . print( [str(token) for token in nlp(tweet.lower())] ) . [&#39;snow&#39;, &#39;white&#39;, &#39;and&#39;, &#39;the&#39;, &#39;seven&#39;, &#39;degrees&#39;, &#39;#&#39;, &#39;makeamoviecold@midnight:-&#39;, &#39;)&#39;] . &#53440;&#51077; . 말뭉치에 등장하는 고유한 토큰 | . 말뭉치에 있는 모든 타입의 집합이 어휘 사전 또는 어휘(lexicon)이다. | . $ divideontimes$ 특성 공학 : 언어학을 이해하고 NLP 문제 해결에 적용하는 과정 . N-gram . 텍스트에 있는 고정 길이 (n)의 연속된 토큰 시퀀스이다. | . &#53076;&#46300; 2-2 . def n_gram(text,n) : return [ text [i:i+n] for i in range(len(text)-n+1)] . cleaned = [&quot;marry&quot;, &quot;,&quot;,&quot;n&#39;t&quot;, &quot;slap&quot;, &quot;green&quot;, &quot;witch&quot;,&quot;,&quot;] . print(n_gram(cleaned,3)) . [[&#39;marry&#39;, &#39;,&#39;, &#34;n&#39;t&#34;], [&#39;,&#39;, &#34;n&#39;t&#34;, &#39;slap&#39;], [&#34;n&#39;t&#34;, &#39;slap&#39;, &#39;green&#39;], [&#39;slap&#39;, &#39;green&#39;, &#39;witch&#39;], [&#39;green&#39;, &#39;witch&#39;, &#39;,&#39;]] . 부분 단어 자체가 유용한 정보를 전달한다면 문자 $n-gram$을 생성할 수 있음 | . example :methanol의 methan 탄소 화합물, 접미사 -ol 은 알코올 종류를 나타낸다. 이 같은 경우 2-gram 으로 생각할 수 있지만 유기 화합물 이름을 구분하는 작업에서는 토큰 하나로 취급할 수 있다. . &#54364;&#51228;&#50612;&#50752; &#50612;&#44036; . &#54364;&#51228;&#50612; . 표제어 : 단어의 기본형 또는 사전에 등재된 단어 fly : flow, flew, flies, flown, flowing 의 표제어 | 토큰을 표제어로 바꾸어 벡터 표현의 차원을 줄이는 방법도 종종 도움이 된다. | . | . spacy는 사전에 정의된 WordNet 사전을 이용해 표제어를 추출한다. | . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) doc = nlp(&quot;he was running late&quot;) for token in doc : print (&quot;{} --&gt; {}&quot;.format(token, token.lemma_)) . he --&gt; -PRON- was --&gt; be running --&gt; run late --&gt; late . &#50612;&#44036;(Stemming) . 어형변화의 기초가 되는 부분 | . Porter와 Snowball어간 추출기가 유명하다. | . &#53076;&#46300; 2-3 . import nltk from nltk.stem.porter import * . stemmer = PorterStemmer() tokens = [&#39;compute&#39;, &#39;computer&#39;, &#39;computed&#39;, &#39;computing&#39;] for token in tokens: print(token + &#39; --&gt; &#39; + stemmer.stem(token)) . compute --&gt; comput computer --&gt; comput computed --&gt; comput computing --&gt; comput . &#54408;&#49324; &#53468;&#44613; . &#53076;&#46300; 2-4 . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) doc = nlp(u&quot;Mary, don&#39;t slap the green witch&quot;) for token in doc : print (&quot;{} --&gt; {}&quot;.format(token, token.pos_)) . Mary --&gt; PROPN , --&gt; PUNCT do --&gt; VERB n&#39;t --&gt; ADV slap --&gt; VERB the --&gt; DET green --&gt; ADJ witch --&gt; NOUN . &#52397;&#53356;&#45208;&#45572;&#44592; = &#48512;&#48516; &#44396;&#47928; &#48516;&#49437; . Chunking, shallow parsing | . 청크 : 하나의 의미가 있는 말 덩어리 (여러 토큰들이 모여 청크가 될 수 있다고 생각하자) | . 연속된 여러 토큰으로 구분되는 텍스트 구에 레이블을 할당하는 작업 | . 다음은 명사구(NP) 부분 구문 분석결과이다. | . &#53076;&#46300; 2-5 . import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) doc = nlp(u&quot;Marry slapped the green witch.&quot;) for chunk in doc.noun_chunks : print (&quot;{} --&gt; {}&quot;.format(chunk, chunk.label_)) . Marry --&gt; NP the green witch --&gt; NP .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2021/01/04/Chapter-2.-NLP-%EA%B8%B0%EC%88%A0-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%ED%9B%91%EC%96%B4%EB%B3%B4%EA%B8%B0.html",
            "relUrl": "/python/2021/01/04/Chapter-2.-NLP-%EA%B8%B0%EC%88%A0-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%ED%9B%91%EC%96%B4%EB%B3%B4%EA%B8%B0.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Chapter 1. Introduction",
            "content": "NLP . Natrual Language Processing | . 언어학 지식에 상관없이 텍스트를 &quot;이해&quot; 하는 통계적인 방법을 사용해 실전 문제를 해결하는 일련의 기술 | . 텍스트 마이닝과의 차이 비교가 현재는 거의 무의미 하지만 아래와 같은 관심 중점에 차이가있다 | . 1 텍스트 마이닝 : 텍스트 데이터로 부터 유의미한 정보를 추출 . 2 NLP : 기계가 자연어를 이해할 수 있고 처리할 수 있도록 하는일을 의미한다. . 여기서 이해라는 것은 주로 텍스트를 계산 가능한 &quot;표현(representation)&quot; 으로 변환함으로써 이루어짐 | . 표현 : 벡터, 텐서, 그래프, 트리 같이 이산적이거나 연속적으로 조합한 구조이다. | . 이 책에서는 딥러닝과 NLP을 배운다. | . Deep learning . 개념 : 계산 그래프와 수치 최적화 기술을 사용해 데이터에서 표현을 효과적으로 학습하는 기술 | . Supervised Learning . 목적 : 주어진 데이터 셋에서 손실 함수를 최소화하는 파라미터 값을 고르는 것 | . 손실함수 : $L( hat y, ,y)$ | . &#44221;&#49324;&#54616;&#44053;&#48277; . Gradient descent : 식의 근을 찾는 일반적인 방법 | . &#51204;&#53685;&#51201;&#51064; &#44221;&#49324; &#54616;&#44053;&#48277; . = batct gradient descent | . 오차를 구할 때 전체 데이터 셋을 고려 $ to$ 한 번에 에포크에서 모든 매개변수에 업데이트를 단 한번 수행 | . 파라미터의 초깃값을 추측한 다음 목적 함수(= 손실 함수)의 값이 수용할만한 임계점(수렴 조건) 아래로 내려갈 때 까지 파라미터를 반복해서 업데이트를 한다. | . 데이터 셋이 클 경우 메모리 제약 $ to$ 계산 비용이 높아 매우 느림 | . &#54869;&#47456;&#51201; &#44221;&#49324;&#54616;&#44053;&#48277; . Stochastic gradient descent(SGD) | . 데이터 포인트를 하나 또는 일부 랜덤하게 선택하여 그래디언트를 계산한다. | . 데이터 포인트를 하나 사용하는 방법은 순수 SGD, 두 개 이상 사용하는 방법은 미니배치 SGD 라고 부른다. | . 순수 SGD : 업데이트에 잡음이 많아 수렴이 매우 느려 실전에서는 거의 사용하지 않음 | . &#50669;&#51204;&#54028; . Backpropagation | . 파라미터를 반복적으로 업데이트 하는 과정 | . 단계 = 정방향 계산 (forward pass) + 역방향 계산 (backward pass) | . 정방향 계산 : 현재 파라미터값으로 입력을 받아 평가하여 손실함수를 계산 | . 역방향 계산 : 손실의 그래이디언트를 사용하여 파라미터를 업데이트 함. | . &#50896;-&#54635; &#54364;&#54788; . &#49324;&#51060;&#53431;&#47088;&#51012; &#49324;&#50857;&#54616;&#50668; &#50896;-&#54635; &#48289;&#53552; &#46608;&#45716; &#51060;&#51652; &#54364;&#54788; &#47564;&#46308;&#44592; . from sklearn.feature_extraction.text import CountVectorizer import seaborn as sns import matplotlib.pyplot as plt . corpus = [&quot;Time flies like an arrow.&quot;,&quot;Fruit flies like a banana&quot;] . one_hot_vectorizer = CountVectorizer(binary=True) one_hot = one_hot_vectorizer.fit_transform(corpus).toarray() . vocab = one_hot_vectorizer.get_feature_names() vocab . [&#39;an&#39;, &#39;arrow&#39;, &#39;banana&#39;, &#39;flies&#39;, &#39;fruit&#39;, &#39;like&#39;, &#39;time&#39;] . sns.heatmap(one_hot, annot = True, cbar = False, xticklabels = vocab, yticklabels = [&quot;Sentence1&quot;, &quot;Setence2&quot;]) . &lt;AxesSubplot:&gt; . TF-IDF &#54364;&#54788; . $$TF(w) = Frequency , , of , ,word$$ . $$IDF(w) = log frac {N}{n_w}$$ . $$N : total , document quad n_w : number ,of , documnet , that , include ,word $$ . $TF$ : $i$번째 문서에서 $j$ 번째 단어의 출현 횟수 | . $IDF$ : 역문서 빈도, 단어 $w$가 출현한 역문서 빈도 | . 사이킷런의 TfidfVectorizer 클래스에서는 단어를 모두 가상의 문서가 있는 것처럼 로그 안의 분모와 분자에 1을 더해서 분모가 0이 되는 상활을 방지함 또한 마지막에 1을 더해 모든 문서에 포함된 단어가 있을 때 IDF 가 0이 되지 않도록함 | . | . $$IDF(w) = log left( frac {N+1}{n_w+1} right ) + 1$$ . 만약 특허와 관련된 문서 묶음이 있다고 가정해보자. 그러면 문서 대부분에 [&quot;Claim&quot;, &quot;System&quot;, &quot;method&quot;] 와 같은 단어가 여러번 반복해서 나온다. | . 이런 흔한 단어는 사실 특정 특허와 관련한 어떤 정보도 담겨있지 않는다. | . 반대로 &quot;tetrafluoroethylene&quot;(테트라플루오로에틸랜) 과 같이 희귀한 단어는 자주 나오지 않지만 특허 문서의 특징을 잘나타냄 | . $TF(w) times IDF(w)$이다. 즉 모든 문서에 등장하는 매우 흔한 단어는 TF-IDF 점수가 0이된다. | . 이처럼 아무런 정보없이 무분별하게 여러번 등장하는 단어들에 패널티를 주기위한 기법이다. | . &#49324;&#51060;&#53431;&#47088;&#51012; &#49324;&#50857;&#54644; TF-IDF &#54364;&#54788; &#47564;&#46308;&#44592; . from sklearn.feature_extraction.text import TfidfVectorizer import seaborn as sns . tfidf_vectorizer = TfidfVectorizer() tfidf =tfidf_vectorizer.fit_transform(corpus).toarray() tfidf . array([[0.49922133, 0.49922133, 0. , 0.35520009, 0. , 0.35520009, 0.49922133], [0. , 0. , 0.57615236, 0.40993715, 0.57615236, 0.40993715, 0. ]]) . sns.heatmap(tfidf,annot=True,xticklabels=vocab,cbar=False, yticklabels=[&quot;Sentence 1&quot;, &quot;Sentence 2&quot;]) . &lt;AxesSubplot:&gt; . &#54028;&#51060;&#53664;&#52824; &#44592;&#52488; . 오픈 소스로 씨아노, 카페, 텐서플로와 달리 파이토치는 테이프 기반 자동 미분 방식을 구현한다. | . 위 방식은 계산 그래프를 동적으로 정의하고 실행할 수 있다. 또한 디버깅이 아주 편리하며 복잡한 모델을 손쉽게 만들게 해줌 | . 텐서 : 다차원 데이터를 담은 수학객체 | . &#53584;&#49436; &#47564;&#46308;&#44592; . 먼저 헬퍼 함수 describe(x)를 정의하여 텐서 타입, 차원, 값 같은 텐서의 속성을 출력 | . def describe(x) : print(&quot;타입 : {}&quot;.format(x.type())) print(&quot;크기 : {}&quot;.format(x.shape)) print(&quot;값 : n{} n n n================================================ n n&quot;.format(x)) . import torch . 차원을 지정 후 텐서를 랜덤하게 초기화하는 방법 | . describe(torch.Tensor(2,3)) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0.2582, 0.4032, 0.1510], [0.8588, 0.2966, 0.9982]]) ================================================ . &#44512;&#46321;&#48516;&#54252;&#50752; &#54364;&#51456;&#51221;&#44508;&#48516;&#54252; . describe(torch.rand(2,3)) ## 균등 분포 describe(torch.randn(2,3)) ## 표준 정규 분포 . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0.6322, 0.4141, 0.1253], [0.5149, 0.2065, 0.5259]]) ================================================ 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[-0.5282, -0.1808, 1.0561], [ 0.6534, -0.0688, 1.0356]]) ================================================ . &#46041;&#51068;&#54620; &#49828;&#52860;&#46972;&#44050;&#51004;&#47196; &#52292;&#50868; &#53584;&#49436;&#47484; &#47564;&#46308;&#44592; . 내장함수로 0또는 1로 채운 텐서 생성 | . describe(torch.zeros(2,3)) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0., 0., 0.], [0., 0., 0.]]) ================================================ . x = torch.ones(2,3) describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 1., 1.], [1., 1., 1.]]) ================================================ . &#51064;-&#54540;&#47112;&#51060;&#49828; &#47700;&#49436;&#46300; . 인-플레이스 메서드 : 데이터를 직접 변경하는 것 | . 아래와 같이 fill_() 메서드와 비슷하게 랜덤 샘플링에 유용한 인-플레이스 메서드가 잇다. | . x.fill_(5) . tensor([[5., 5., 5.], [5., 5., 5.]]) . describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[5., 5., 5.], [5., 5., 5.]]) ================================================ . 새로운 객체를 생성하지 않고 바로 값을 변환하는데 객체 지향적인 관점에서 별로 좋지는 않은 것 같다. | . y = torch.Tensor(3,2) . y . tensor([[4.4155e-05, 2.1259e+20], [8.1714e+20, 6.7875e-07], [2.5668e-09, 4.1537e-08]]) . y.uniform_() . tensor([[0.7345, 0.1133], [0.0873, 0.7492], [0.3604, 0.2273]]) . y . tensor([[0.7345, 0.1133], [0.0873, 0.7492], [0.3604, 0.2273]]) . y.normal_() . tensor([[ 0.3838, -0.6120], [-1.3810, 0.8169], [-0.6775, -1.2050]]) . y . tensor([[ 0.3838, -0.6120], [-1.3810, 0.8169], [-0.6775, -1.2050]]) . &#47532;&#49828;&#53944;&#47196; &#53584;&#49436;&#47484; &#47564;&#46308;&#44256; &#52488;&#44592;&#54868; . x = torch.Tensor([[1,2,3], [4,5,6]]) . describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 2., 3.], [4., 5., 6.]]) ================================================ . 위 처럼 값을 리스트나. 넘파이 배열로 전달하여 텐서를 생성하고 초기화가 가능하다. | . 또한 언제든지 파이토치 텐서를 넘파이 배열로 바꿀 수 있음 | . 단 넘파이 배열을 사용하면 텐서 타입이, FloatTensor가 아니라 DoubleTensor가 된다. | . import numpy as np . npy = np.random.rand(2,3) . describe(torch.from_numpy(npy)) . 타입 : torch.DoubleTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0.9967, 0.8784, 0.1933], [0.6447, 0.7438, 0.1425]], dtype=torch.float64) ================================================ . 넘파이 배열과 파이토치텐서 사이를 변환하는 기능은 넘파이 포맷의 수치데이터를사용하는 래거시(legacy) 라이브러리를 사용할 때 중요하다. | . &#53440;&#51077;&#51012; &#52488;&#44592;&#54868; &#54616;&#50668; &#53584;&#49436; &#49373;&#49457; . x = torch.FloatTensor([[1,2,3], [4,5,6]]) describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 2., 3.], [4., 5., 6.]]) ================================================ . x = x.long() describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1, 2, 3], [4, 5, 6]]) ================================================ . x = torch.tensor([[1,2,3], [4,5,6]], dtype=torch.int64) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1, 2, 3], [4, 5, 6]]) ================================================ . x = x.float() describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[1., 2., 3.], [4., 5., 6.]]) ================================================ . &#53584;&#49436; &#50672;&#49328; . &#45927;&#49480; . import torch . x = torch.randn(2,3) describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[ 0.0715, -0.2960, -0.5417], [ 0.3304, 1.5357, -1.1040]]) ================================================ . describe(torch.add(x,x)) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[ 0.1431, -0.5919, -1.0834], [ 0.6609, 3.0715, -2.2080]]) ================================================ . x = torch.arange(6) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([6]) 값 : tensor([0, 1, 2, 3, 4, 5]) ================================================ . view() 메서드는 동일한 데이터를 공유하는 새로운 텐서를 만든다. data_ptr() 매서드를 사용하면 원본 텐서와 뷰 텐서가 같은 저장 위치를 가리키고 있다는 것을 확인할 수 있다. | . x1 = x.view(2,3) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5]]) ================================================ . x.data_ptr() . 2251512245888 . x1.data_ptr() . 2251512245888 . 0 으로 지정시 colsum 반환, 1로 지정시 rowsum을 반환 | . describe(torch.sum(x,dim=0)) . 타입 : torch.LongTensor 크기 : torch.Size([3]) 값 : tensor([3, 5, 7]) ================================================ . describe(torch.sum(x,dim=1)) . 타입 : torch.LongTensor 크기 : torch.Size([2]) 값 : tensor([ 3, 12]) ================================================ . transpose함수는 두 번째와 세 번째 매개변수로 전달된 차원을 전치한 텐서를 만든다. | . x . tensor([[0, 1, 2], [3, 4, 5]]) . describe(torch.transpose(x,0,1)) . 타입 : torch.LongTensor 크기 : torch.Size([3, 2]) 값 : tensor([[0, 3], [1, 4], [2, 5]]) ================================================ . &#51064;&#45937;&#49905;, &#49836;&#46972;&#51060;&#49905;, &#50672;&#44208; . &#53584;&#49436; &#49836;&#46972;&#51060;&#49905;&#44284; &#51064;&#45937;&#49905; . import torch x = torch.arange(6).view(2,3) describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5]]) ================================================ . describe(x[:1,:2]) . 타입 : torch.LongTensor 크기 : torch.Size([1, 2]) 값 : tensor([[0, 1]]) ================================================ . describe(x[0,1]) . 타입 : torch.LongTensor 크기 : torch.Size([]) 값 : 1 ================================================ . &#48373;&#51105;&#54620; &#51064;&#45937;&#49905;, &#50672;&#49549;&#51201;&#51060;&#51648; &#50506;&#51008; &#53584;&#49436; &#51064;&#45937;&#49828; &#52280;&#51312;&#54616;&#44592; . x . tensor([[0, 1, 2], [3, 4, 5]]) . 아래 코드를 해석하면 컬럼차원에서 0,2 번째 컬럼을 반환한다는 뜻이다. | . indices = torch.LongTensor([0,2]) describe(torch.index_select(x,dim=1,index = indices)) . 타입 : torch.LongTensor 크기 : torch.Size([2, 2]) 값 : tensor([[0, 2], [3, 5]]) ================================================ . indices = torch.LongTensor([0,0]) describe(torch.index_select(x,dim=0,index = indices)) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [0, 1, 2]]) ================================================ . 인덱스가 LongTensor 라는 점에 주목하자. 파이토치 함수를 사용할 때 필수 조건이다. | . x . tensor([[0, 1, 2], [3, 4, 5]]) . row_indices = torch.arange(2).long() ## 로우나 컬럼이나 같은 표현이다. col_indices = torch.LongTensor([0,1]) describe(x[row_indices,col_indices]) . 타입 : torch.LongTensor 크기 : torch.Size([2]) 값 : tensor([0, 4]) ================================================ . &#53584;&#49436; &#50672;&#44208; . import torch . x = torch.arange(6).view(2,3) . describe(x) . 타입 : torch.LongTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5]]) ================================================ . cat, dim=0 $ to$ r의 cbind | . describe(torch.cat([x,x],dim=0)) ## . 타입 : torch.LongTensor 크기 : torch.Size([4, 3]) 값 : tensor([[0, 1, 2], [3, 4, 5], [0, 1, 2], [3, 4, 5]]) ================================================ . describe(torch.cat([x,x],dim=1)) ## . 타입 : torch.LongTensor 크기 : torch.Size([2, 6]) 값 : tensor([[0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5]]) ================================================ . describe(torch.stack([x,x])) ## . 타입 : torch.LongTensor 크기 : torch.Size([2, 2, 3]) 값 : tensor([[[0, 1, 2], [3, 4, 5]], [[0, 1, 2], [3, 4, 5]]]) ================================================ . &#53584;&#49436;&#51032; &#49440;&#54805; &#45824;&#49688; &#44228;&#49328; : &#54665;&#47148; &#44273;&#49480; . import torch . x1 = torch.arange(6.).view(2,3) describe(x1) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 3]) 값 : tensor([[0., 1., 2.], [3., 4., 5.]]) ================================================ . x2 = torch.ones(3,2) x2 . tensor([[1., 1.], [1., 1.], [1., 1.]]) . x2 [:,1] +=1 . describe(x2) . 타입 : torch.FloatTensor 크기 : torch.Size([3, 2]) 값 : tensor([[1., 2.], [1., 2.], [1., 2.]]) ================================================ . 행렬 곱셈 | . torch.mm(x1,x2) . tensor([[ 3., 6.], [12., 24.]]) . 역행렬 | . torch.inverse(torch.rand(2,2)) . tensor([[ 1.1645, -2.5808], [-0.0280, 3.1795]]) . 대각합 | . torch.trace(torch.rand(2,2)) . tensor(0.8843) . &#53584;&#49436;&#50752; &#44228;&#49328;&#44536;&#47000;&#54532; . 그래이디언트 연산을 할 수 있는 텐서 만들기 | . requires_grad = True는 그레이디언트 기반 학습에 필요한 손실함수와 텐서의 그레이디언트를 기록하는 부가 연산을 활성화 시킨다. . import torch . x = torch.ones(2,2, requires_grad=True) . describe(x) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 2]) 값 : tensor([[1., 1.], [1., 1.]], requires_grad=True) ================================================ . print(x.grad is None) . True . $$ y = (x+2)(x+5) +3$$ . y = (x+2)*(x+5) + 3 describe(y) print(x.grad is None) . 타입 : torch.FloatTensor 크기 : torch.Size([2, 2]) 값 : tensor([[21., 21.], [21., 21.]], grad_fn=&lt;AddBackward0&gt;) ================================================ True . z = y.mean() describe(z) z.backward() print(x.grad is None) . 타입 : torch.FloatTensor 크기 : torch.Size([]) 값 : 21.0 ================================================ False . 파이토치에서 계산 그래프에 있는 노드에 대한 그레이디언트를 .grad 속성으로 참조할 수 있다. 옵티마이저는 .grad 속성을 사용해서 파라미터 값을 업데이트한다. | . &#50672;&#49845;&#47928;&#51228; . 1&#48264; . 2D 텐서를 만들고 차원 0 위치에 크기가 1인 차원을 추가하시오. . Solution . a = torch.rand(3,3) a . tensor([[0.2966, 0.4459, 0.0016], [0.2637, 0.7056, 0.2209], [0.2585, 0.7810, 0.2748]]) . a.unsqueeze(0) . tensor([[[0.2966, 0.4459, 0.0016], [0.2637, 0.7056, 0.2209], [0.2585, 0.7810, 0.2748]]]) . 2&#48264; . 이전 텐서에 추가한 차원을 삭제하기 . Solution . a.squeeze(0) . tensor([[0.2966, 0.4459, 0.0016], [0.2637, 0.7056, 0.2209], [0.2585, 0.7810, 0.2748]]) . 3&#48264; . 범위가 [3,7) 이고 크기가 $5 times 3$인 램덤판 텐서를 만들기 | . Solution . 3+torch.rand(5,3)*(7-3) . tensor([[4.8969, 5.3626, 4.2073], [5.1916, 6.7691, 3.1864], [6.1773, 6.1206, 5.6650], [4.0778, 6.3007, 6.8788], [5.8507, 4.0870, 5.9019]]) . 4&#48264; . Solution . 정규분포를 사용해서 텐서를 만들기 . a= torch.rand(3,3) a.normal_() . tensor([[ 0.3830, -0.2555, -0.2903], [ 1.0973, 0.3475, -0.5337], [-0.2465, 0.9446, -0.4641]]) . 5&#48264; . 텐서 torch.Tensor([1,1,1,0,1]) 에서 0이 아닌 원소의 인덱스를 추출하세요. . Solution . a = torch.Tensor([1,1,1,0,1]) torch.nonzero(a,as_tuple=True) . (tensor([0, 1, 2, 4]),) . as_tuple : True 로 지정하지 않으면 2차원 텐서를 변환한다. | . 6&#48264; . 크기가 (3,1)인 랜덤한 텐서를 만들고 네 벌을 복사해 쌓으세요 . Solution . a = torch.rand(3,1) a . tensor([[0.1492], [0.1221], [0.5768]]) . a.expand(3,4) . tensor([[0.1492, 0.1492, 0.1492, 0.1492], [0.1221, 0.1221, 0.1221, 0.1221], [0.5768, 0.5768, 0.5768, 0.5768]]) . 7&#48264; . 2차원 행렬 개 ( a = torch.rand(3,4,5), b = torch.rand(3,5,4) ) 의 배치 행렬 곱셈을 계산하세요. . Solution . a = torch.rand(3,4,5) b = torch.rand(3,5,4) . a . tensor([[[0.8432, 0.3043, 0.3210, 0.9431, 0.2673], [0.7349, 0.5522, 0.5258, 0.0104, 0.3234], [0.4519, 0.3940, 0.0187, 0.3106, 0.2780], [0.4799, 0.3480, 0.6322, 0.1634, 0.7024]], [[0.7353, 0.2029, 0.0166, 0.0391, 0.7469], [0.0473, 0.0727, 0.1912, 0.2763, 0.8238], [0.8702, 0.6047, 0.2439, 0.3102, 0.2967], [0.5944, 0.9755, 0.8162, 0.3612, 0.4574]], [[0.7215, 0.0704, 0.7937, 0.7839, 0.7022], [0.3435, 0.1463, 0.8671, 0.2648, 0.7312], [0.7399, 0.3415, 0.9808, 0.7982, 0.8345], [0.1444, 0.1740, 0.1711, 0.0673, 0.3027]]]) . b . tensor([[[0.2951, 0.1355, 0.5337, 0.1931], [0.1613, 0.7947, 0.6819, 0.7209], [0.1560, 0.1124, 0.6236, 0.7558], [0.6739, 0.1252, 0.2276, 0.1586], [0.2627, 0.1673, 0.3396, 0.2907]], [[0.6307, 0.3764, 0.1830, 0.0020], [0.3913, 0.3344, 0.9843, 0.3801], [0.7788, 0.5737, 0.5411, 0.5289], [0.7318, 0.4864, 0.8927, 0.6358], [0.8435, 0.0820, 0.3751, 0.3977]], [[0.5797, 0.1567, 0.3288, 0.7492], [0.0626, 0.7531, 0.8028, 0.4698], [0.3778, 0.8844, 0.9128, 0.1916], [0.9723, 0.2381, 0.1718, 0.3828], [0.0870, 0.5339, 0.6326, 0.3959]]]) . torch.bmm(a,b) . tensor([[[1.0538, 0.5550, 1.1631, 0.8521], [0.4799, 0.6529, 1.2088, 1.0330], [0.4821, 0.4619, 0.6866, 0.5155], [0.5910, 0.5506, 1.1633, 1.0515]], [[1.2148, 0.4345, 0.6584, 0.4093], [1.1042, 0.3537, 0.7393, 0.6321], [1.4528, 0.8450, 1.2747, 0.6757], [2.0424, 1.2314, 2.0047, 1.2151]], [[1.5457, 1.4296, 1.5972, 1.3038], [0.8569, 1.3843, 1.5300, 0.8831], [1.6695, 1.8762, 2.0778, 1.5386], [0.2510, 0.4826, 0.5464, 0.3683]]]) .",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/python/2021/01/03/Chapter-1.Introduction.html",
            "relUrl": "/python/2021/01/03/Chapter-1.Introduction.html",
            "date": " • Jan 3, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gangcheol.github.io/nlp-with-pytroch/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "github . github . soundcloud . C.I.C .",
          "url": "https://gangcheol.github.io/nlp-with-pytroch/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gangcheol.github.io/nlp-with-pytroch/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}